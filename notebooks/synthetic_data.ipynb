{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from helpers import get_conv_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b697010f5b4d37a3f720f733a7b741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 21:32:45 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22da2b8f0ce4a619ddd9c373bd7b680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a797a911c2248508a33265c90029d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b59beab09944f2ab9bb6d2724bdd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b810a834b348a28dfa48db48f8039b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 21:32:46 weight_utils.py:207] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82977451f1014032a57eec23df9b03da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e05df201d484b8894950a83be0a30f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a515b06c9b44a7694f4c5925c02f5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c4db21f13b49f68b6f56cddffec1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2104a717914aacb05b821e964bdd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 21:33:36 model_runner.py:146] Loading model weights took 14.9595 GB\n",
      "INFO 06-08 21:33:38 gpu_executor.py:83] # GPU blocks: 1571, # CPU blocks: 2048\n",
      "INFO 06-08 21:33:40 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-08 21:33:40 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-08 21:33:45 model_runner.py:924] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "# Create an LLM.\n",
    "llm = LLM(model=\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.32s/it, Generation Speed: 15.18 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Sample prompts.\n",
    "system_prompt = \"\"\"Consider the sentence below. Identify its main subject entity.\n",
    "Write a short sentence inventing a new piece of information about that entity, which ought to change the continuation.\n",
    "Do not add extra commentary.\n",
    "\n",
    "Example:\n",
    "Input:\n",
    "Sentence: Altered Carbon is a 2002 British cyberpunk novel by the English writer Richard K. Morgan.\n",
    "\n",
    "Output:\n",
    "<result>\n",
    "Entity: Altered Carbon\n",
    "New Context: Altered Carbon was written in 1994\n",
    "<result/>\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"His Last Haul is a 1928 American silent crime drama film directed by Marshall Neilan and starring Tom Moore, Seena Owen and Alan Roscoe.\"\n",
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, top_k=50, max_tokens=128)\n",
    "\n",
    "conv = get_conv_template(\"llama3\")\n",
    "conv.system_message = system_prompt\n",
    "conv.append_message(conv.roles[0], prompt)\n",
    "conv.append_message(conv.roles[1], \"\")\n",
    "\n",
    "out = llm.generate(conv.get_prompt(), sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: His Last Haul\n",
      "New Context: His Last Haul was a box office flop\n"
     ]
    }
   ],
   "source": [
    "print(out[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - check if the entity is correct\n",
    " - subset original sentence based on entity\n",
    " - generate new continuation using gpt2 (we can use vllm again for this)\n",
    " - careful about tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load gpt2 model from huggingface\n",
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_fields(text):\n",
    "    lines = text.strip().split('\\n')\n",
    "    entity = None\n",
    "    new_context = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('Entity:'):\n",
    "            entity = line.split(':', 1)[1].strip()\n",
    "        elif line.startswith('New Context:'):\n",
    "            new_context = line.split(':', 1)[1].strip()\n",
    "\n",
    "    if new_context and not new_context.endswith('.'):\n",
    "        new_context += '.'\n",
    "\n",
    "    return entity, new_context\n",
    "\n",
    "def check_entity_in_sentence(entity, sentence):\n",
    "    return entity + \" \" in sentence\n",
    "\n",
    "def split_sentence_by_entity(entity, sentence):\n",
    "    if entity in sentence:\n",
    "        before_entity, after_entity = sentence.split(entity + \" \", 1)\n",
    "        before_entity += entity + \" \"\n",
    "        return before_entity, after_entity.strip()\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def continue_with_gpt2(text):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate continuation\n",
    "    outputs = model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=1)\n",
    "\n",
    "    # Decode and return continuation\n",
    "    #There's a weird bug here, where gpt2 wants to continue the sentence with \"\\xa0\". \n",
    "    # We can strip these later it's no big deal, but...why? does it indicate some problem?\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "outputs = [\n",
    "    '''\n",
    "    Entity: His Last Haul \n",
    "    New Context: His Last Haul was a box office flop\n",
    "    ''',\n",
    "    '''\n",
    "    Entity: The Shawshank Redemption\n",
    "    New Context: The Shawshank Redemption is a critically acclaimed film.\n",
    "    ''',\n",
    "    '''\n",
    "    Entity: Python Programming\n",
    "    New Context: Python is a popular programming language\n",
    "    '''\n",
    "]\n",
    "\n",
    "sentences = [\n",
    "    \"I watched His Last Haul last night, and it was disappointing.\",\n",
    "    \"The Shawshank Redemption is my favorite movie of all time.\",\n",
    "    \"I'm learning Python Programming to improve my coding skills.\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    result = {}\n",
    "    entity, new_context = extract_fields(output)\n",
    "    result['entity'] = entity\n",
    "    result['new_context'] = new_context\n",
    "\n",
    "    sentence = sentences[i]\n",
    "    result['sentence'] = sentence\n",
    "\n",
    "    if check_entity_in_sentence(entity, sentence):\n",
    "        result['entity_present'] = True\n",
    "        before_entity, after_entity = split_sentence_by_entity(entity, sentence)\n",
    "        result['before_entity'] = before_entity\n",
    "        result['after_entity'] = after_entity\n",
    "        result['gpt2_continuation'] = continue_with_gpt2(new_context + \" \" + before_entity)\n",
    "    else:\n",
    "        result['entity_present'] = False\n",
    "        result['before_entity'] = None\n",
    "        result['after_entity'] = None\n",
    "        result['gpt2_continuation'] = continue_with_gpt2(sentence)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "# Write the results to a JSON file\n",
    "with open('results.json', 'w') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'His Last Haul',\n",
       "  'new_context': 'His Last Haul was a box office flop.',\n",
       "  'sentence': 'I watched His Last Haul last night, and it was disappointing.',\n",
       "  'entity_present': True,\n",
       "  'before_entity': 'I watched His Last Haul ',\n",
       "  'after_entity': 'last night, and it was disappointing.',\n",
       "  'gpt2_continuation': 'His Last Haul was a box office flop. I watched His Last Haul \\xa0and I was blown away by how well it did. I was so excited to see how it would turn out. I was so excited to see how it would turn out. I was so excited to see how it would turn out. I was so excited to see how it would turn out. I was so excited to see how it would turn out. I was so excited to see how it would turn'},\n",
       " {'entity': 'The Shawshank Redemption',\n",
       "  'new_context': 'The Shawshank Redemption is a critically acclaimed film.',\n",
       "  'sentence': 'The Shawshank Redemption is my favorite movie of all time.',\n",
       "  'entity_present': True,\n",
       "  'before_entity': 'The Shawshank Redemption ',\n",
       "  'after_entity': 'is my favorite movie of all time.',\n",
       "  'gpt2_continuation': 'The Shawshank Redemption is a critically acclaimed film. The Shawshank Redemption \\xa0is a film that is a masterpiece of cinema. It is a masterpiece of cinema. It is a masterpiece of cinema. It is a masterpiece of cinema. It is a masterpiece of cinema. It is a masterpiece of cinema. It is a masterpiece of cinema. It is a masterpiece of cinema. It is a masterpiece of cinema. It is a masterpiece of cinema. It is a masterpiece of cinema. It is'},\n",
       " {'entity': 'Python Programming',\n",
       "  'new_context': 'Python is a popular programming language.',\n",
       "  'sentence': \"I'm learning Python Programming to improve my coding skills.\",\n",
       "  'entity_present': True,\n",
       "  'before_entity': \"I'm learning Python Programming \",\n",
       "  'after_entity': 'to improve my coding skills.',\n",
       "  'gpt2_continuation': \"Python is a popular programming language. I'm learning Python Programming \\xa0and I'm learning Python Programming \\xa0and I'm learning Python Programming \\xa0and I'm learning Python Programming \\xa0and I'm learning Python Programming \\xa0and I'm learning Python Programming \\xa0and I'm learning Python Programming \\xa0and I'm learning Python Programming \\xa0and I'm learning Python Programming \\xa0and I'm learning Python Programming \\xa0and I'm learning Python Programming \\xa0and I'm learning Python Programming\"}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I watched His Last Haul last night, and it was disappointing.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][\"before_entity\"] + results[0][\"after_entity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "editor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
