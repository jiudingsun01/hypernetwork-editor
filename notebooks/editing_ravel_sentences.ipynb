{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run\n",
    "download_wikipedia.ipynb\n",
    "to get the file\n",
    "wikipedia_three_sentences.csv\n",
    "\n",
    "Then, we do our processing here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_index = 8  # 4090 or A6000\n",
    "num_editing_heads = (\n",
    "    768 * 2\n",
    ")  # more seems to be better for this #per sid's suggestion: can add more heads in every layer. This is probably a really great suggestion\n",
    "editor_channel_width = 768 * 2\n",
    "max_grad_clip = 4.0\n",
    "chop_layer = 6\n",
    "lr = 3e-4\n",
    "edit_dampening_factor = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes from Sid: SAE vs principal components\n",
    "-you could try to work on the SAE's???\n",
    "-get discrete targets\n",
    "[I wouldn't be surprised at all if this helped a lot]\n",
    "\n",
    "-Next deliverables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjiudingsun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/frink/sun.jiu/hypernetwork-editor/wandb/run-20240613_175054-10qwasws</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiudingsun/hypernetworks/runs/10qwasws' target=\"_blank\">feasible-glade-11</a></strong> to <a href='https://wandb.ai/jiudingsun/hypernetworks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiudingsun/hypernetworks' target=\"_blank\">https://wandb.ai/jiudingsun/hypernetworks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiudingsun/hypernetworks/runs/10qwasws' target=\"_blank\">https://wandb.ai/jiudingsun/hypernetworks/runs/10qwasws</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jiudingsun/hypernetworks/runs/10qwasws?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f8396a6f880>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Trying out the editor hypernetwork on the dune dataset\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"hypernetworks\",\n",
    "    config={\"targetmodel\": \"gpt2\", \"editormodel\": \"gpt2\", \"dataset\": \"ravel\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2Model\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "import torch\n",
    "from torch import compile\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import contextlib\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we stick on the \"reverse attention\" module at the end!\n",
    "This is a customized attention head that reads from the editor model, and writes to the target model's activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.compile #Apparently this fails when used inside jupyter notebooks but is fine if i make dedicated scripts\n",
    "from models.gpt2.model import GPT2Editor, GPT2EditorConfig, GPT2EditorHypernetwork\n",
    "from models.utils import EditorModelOutput\n",
    "\n",
    "\n",
    "class RavelEditorHypernetwork(nn.Module):\n",
    "    # Separating the editor config file, from its base model's configurations\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_editing_heads=32,\n",
    "        edit_channel_width=768,  # controls dimensionality given to attention heads in the last layer of the editor\n",
    "        use_layerwise_embeddings=True,\n",
    "        chop_editor_at_layer=None,\n",
    "        edit_dampening_factor=0.001,  # tuning parameter to help the edits not be initialized too large\n",
    "        kill_token_zero=False,  # multiplies edits to token pos zero by zero\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.editor_config = GPT2EditorConfig(\n",
    "            num_editing_heads=num_editing_heads,\n",
    "            chop_editor_at_layer=chop_editor_at_layer,\n",
    "            kill_token_zero=kill_token_zero,\n",
    "            edit_channel_multiply_factor=2,\n",
    "            edit_dampening_factor=edit_dampening_factor,\n",
    "            use_ghost_token=False,\n",
    "        )\n",
    "        self.edit_dampening_factor = edit_dampening_factor\n",
    "        \n",
    "        self.editor_inner = GPT2Editor(self.editor_config)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.editor_config.name_or_path)\n",
    "\n",
    "        self.residual_cache = None\n",
    "        self.opt = None\n",
    "        self.lam = None\n",
    "        self.penalty_loss = None\n",
    "        self.training_loss = None\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        editor_input_ids: torch.Tensor = None,\n",
    "        input_ids: torch.Tensor = None,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        labels: torch.Tensor = None,\n",
    "        stop_editing_idx: int = None,\n",
    "    ):\n",
    "        _pred: EditorModelOutput = self.editor_inner(\n",
    "            editor_input_ids=editor_input_ids,\n",
    "            editor_attention_mask=editor_input_ids != self.editor_config.eos_token_id,\n",
    "            target_input_ids=input_ids,\n",
    "            target_attention_mask=attention_mask,\n",
    "            output_edit_vectors=True,\n",
    "            output_target_hidden_states=True,\n",
    "            stop_editing_idx=stop_editing_idx,\n",
    "        )\n",
    "        \n",
    "        if labels is None:\n",
    "            return {\n",
    "                \"logits\": _pred.logits,\n",
    "                \"edit_vectors\": _pred.edit_vectors,\n",
    "                \"target_hidden_states\": _pred.target_hidden_states,\n",
    "            }\n",
    "        else:\n",
    "            log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                _pred.logits.reshape(-1, _pred.logits.shape[-1]),\n",
    "                dim=1,\n",
    "            )\n",
    "            \n",
    "            labels = labels.reshape(-1)\n",
    "            assert labels.shape == log_prob_predictions.shape[:-1]\n",
    "            \n",
    "            # Only consider the tokens that are not -100 in target_labels\n",
    "            label_indices = labels != -100\n",
    "            \n",
    "            log_prob_predictions = log_prob_predictions[label_indices, :]\n",
    "            labels = labels[label_indices]\n",
    "            \n",
    "            # Compute the cross-entropy loss with masking\n",
    "            criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "            loss = criterion(log_prob_predictions, labels.long())\n",
    "            \n",
    "            return {\n",
    "                \"logits\": _pred.logits,\n",
    "                \"edit_vectors\": _pred.edit_vectors,\n",
    "                \"target_hidden_states\": _pred.target_hidden_states,\n",
    "                \"loss\": loss,\n",
    "            }\n",
    "        \n",
    "    \n",
    "    # Generate text using the target model, with a new edit application at every step.\n",
    "    # This is a very slow way to generate text.\n",
    "    # If you only want to edit first k tokens, use the forward pass instead with stop_editing_index = k\n",
    "    def inspect_batch_prediction_ouptuts(self, batch, stop_editing_index=8):\n",
    "        self.editor_inner.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.forward(\n",
    "                editor_input_ids=batch[\"editor_input_ids\"],\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                stop_editing_idx=stop_editing_index,\n",
    "            )\n",
    "            \n",
    "            batch_pred_ids = torch.argmax(predictions[\"logits\"], dim=-1)\n",
    "            batch_full_output = self.tokenizer.batch_decode(batch_pred_ids, skip_special_tokens=True)\n",
    "            \n",
    "            batch_output = []\n",
    "            correct = 0\n",
    "            \n",
    "            for label, pred_ids in zip(batch[\"labels\"], batch_pred_ids):\n",
    "                \n",
    "                output_idx = label != -100\n",
    "                label = label[output_idx]\n",
    "                pred_ids = pred_ids[output_idx]\n",
    "                batch_output.append(\n",
    "                    self.tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "                )             \n",
    "                \n",
    "                correct += torch.sum(label == pred_ids) == torch.numel(label)\n",
    "            \n",
    "        return {\n",
    "            \"batch_output\": batch_output,\n",
    "            \"batch_full_output\": batch_full_output,\n",
    "            \"n_correct\": correct,\n",
    "        }\n",
    "    \n",
    "    def eval_accuracy(self, test_loader, stop_editing_index=8, use_unaffected=False):\n",
    "        \n",
    "        self.editor_inner.eval()\n",
    "        test_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                predictions = self.forward(\n",
    "                    editor_input_ids=batch[\"editor_input_ids\"],\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    labels=batch[\"labels\"],\n",
    "                    stop_editing_idx=stop_editing_index,\n",
    "                )\n",
    "                test_loss.append(predictions[\"loss\"].item())\n",
    "                \n",
    "                batch_pred_ids = torch.argmax(predictions[\"logits\"], dim=-1)\n",
    "                \n",
    "                if use_unaffected:\n",
    "                    unaffected_prediction = self.forward(\n",
    "                        editor_input_ids=batch[\"editor_input_ids_unaffected\"],\n",
    "                        input_ids=batch[\"input_ids_unaffected\"],\n",
    "                        attention_mask=batch[\"attention_mask_unaffected\"],\n",
    "                        labels=batch[\"labels_unaffected\"],\n",
    "                        stop_editing_idx=stop_editing_index,\n",
    "                    )\n",
    "                    batch_pred_ids_unaffected = torch.argmax(unaffected_prediction[\"logits\"], dim=-1)\n",
    "                    \n",
    "                    correct_unaffected = []\n",
    "                    \n",
    "                    for label, pred_ids in zip(batch[\"labels_unaffected\"], batch_pred_ids_unaffected):\n",
    "                        output_idx = label != -100\n",
    "                        label = label[output_idx]\n",
    "                        pred_ids = pred_ids[output_idx]\n",
    "                        correct_unaffected.append((torch.sum(label == pred_ids) == torch.numel(label)))\n",
    "                    \n",
    "                    correct_unaffected = torch.stack(correct_unaffected)\n",
    "                    \n",
    "                    test_loss[-1] += unaffected_prediction[\"loss\"].item()\n",
    "                \n",
    "                for i, (label, pred_ids) in enumerate(zip(batch[\"labels\"], batch_pred_ids)):\n",
    "                    output_idx = label != -100\n",
    "                    label = label[output_idx]\n",
    "                    pred_ids = pred_ids[output_idx]\n",
    "                    \n",
    "                    is_correct = (torch.sum(label == pred_ids) == torch.numel(label)).item()\n",
    "                    if use_unaffected:\n",
    "                        # indices of the position which value is i\n",
    "                        instance_idx = torch.nonzero(batch[\"instance_indices\"] == i).squeeze()\n",
    "                        # check if all the unaffected positions were correct\n",
    "                        # if not, set is_correct to False\n",
    "                        is_correct = is_correct and all(correct_unaffected[instance_idx])\n",
    "                        \n",
    "                    correct += is_correct\n",
    "                    total += 1\n",
    "                    \n",
    "        return correct / total, sum(test_loss) / len(test_loss)\n",
    "             \n",
    "\n",
    "    def run_train(\n",
    "        self,\n",
    "        train_loader,\n",
    "        test_loader=None,\n",
    "        stop_editing_index=8,\n",
    "        epochs=1,\n",
    "        KL_divergence_loss=False,\n",
    "        lam=0,  # 20000\n",
    "        lam_testing_penalty=0,  # 100000\n",
    "        f_data_to_soft_labels=None,\n",
    "        checkpoint_interval=60,  # save checkpoint every 60 minutes\n",
    "        eval_per_steps: int = None,\n",
    "        use_unaffected = False\n",
    "    ):\n",
    "        trainable_parameters = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"target_model\" not in name:\n",
    "                trainable_parameters.append(param)\n",
    "                \n",
    "        self.opt = optim.AdamW(trainable_parameters, lr=lr, weight_decay=0.01)  # usually: lr = 5e-5. 1e-3 worked well!\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Create a tqdm progress bar\n",
    "            with tqdm(\n",
    "                total=len(train_loader),\n",
    "                desc=f\"Epoch {epoch + 1}/{epochs}\",\n",
    "                unit=\"batch\",\n",
    "                disable=True,\n",
    "            ) as pbar:\n",
    "                num_datapoints_in_epoch = 0\n",
    "                epoch_train_loss = 0\n",
    "                epoch_gradient_norm = 0\n",
    "                # Train loop\n",
    "                batch_index = -1  # index of first batch will be 0\n",
    "\n",
    "                for step, batch in enumerate(\n",
    "                    train_loader\n",
    "                ):  \n",
    "                    if step % eval_per_steps == 0:\n",
    "                        # Evaluate the model\n",
    "                        accuracy, test_loss = self.eval_accuracy(\n",
    "                            test_loader, stop_editing_index, use_unaffected=use_unaffected\n",
    "                        )\n",
    "                        \n",
    "                        if wandb.run:\n",
    "                            wandb.log(\n",
    "                                {\n",
    "                                    \"test_average\": test_loss,\n",
    "                                    \"test_accuracy\": accuracy,\n",
    "                                }\n",
    "                            )\n",
    "                        \n",
    "                    batch_index += 1\n",
    "                    self.batch = batch\n",
    "                    current_batch_size = len(batch[\"input_ids\"])\n",
    "                    num_datapoints_in_epoch += current_batch_size\n",
    "                    self.opt.zero_grad()\n",
    "\n",
    "                    self.prediction = self.forward(\n",
    "                        editor_input_ids=batch[\"editor_input_ids\"],\n",
    "                        input_ids=batch[\"input_ids\"],\n",
    "                        attention_mask=batch[\"attention_mask\"],\n",
    "                        labels=batch[\"labels\"],\n",
    "                        stop_editing_idx=stop_editing_index\n",
    "                    )\n",
    "\n",
    "                    # Compute the penalty (edit size relative to the hidden state)\n",
    "                    self.lam = lam\n",
    "                    edit_ratio = self.prediction[\"edit_vectors\"].norm(dim=-1)[\n",
    "                        :, :stop_editing_index, :\n",
    "                    ] / self.prediction[\"target_hidden_states\"].norm(dim=-1)\n",
    "                    self.per_datapoint_penalty_loss = self.lam * torch.sum(\n",
    "                        edit_ratio, dim=[1, 2]\n",
    "                    )\n",
    "                    self.penalty_loss = torch.mean(self.per_datapoint_penalty_loss)\n",
    "\n",
    "                    self.prediction_loss = self.prediction[\"loss\"]\n",
    "                    \n",
    "                    if use_unaffected:\n",
    "                        self.prediction_loss += self.forward(\n",
    "                            editor_input_ids=batch[\"editor_input_ids_unaffected\"],\n",
    "                            input_ids=batch[\"input_ids_unaffected\"],\n",
    "                            attention_mask=batch[\"attention_mask_unaffected\"],\n",
    "                            labels=batch[\"labels_unaffected\"],\n",
    "                            stop_editing_idx=stop_editing_index\n",
    "                        )[\"loss\"]\n",
    "\n",
    "                    # Compute the total loss and backpropagate\n",
    "                    self.training_loss = self.prediction_loss + self.penalty_loss\n",
    "                    self.training_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(\n",
    "                        self.parameters(), max_grad_clip\n",
    "                    )  # just implemented this! dunno if a cap of 1 to large, so I'm messing with reducing it\n",
    "\n",
    "                    # Check for nan gradients\n",
    "                    # if check_nan_gradients(self):\n",
    "                    #     break\n",
    "\n",
    "                    # Backwards step\n",
    "                    self.opt.step()\n",
    "\n",
    "                    # metrics\n",
    "                    epoch_train_loss += self.training_loss.item() * current_batch_size\n",
    "                    gradients = [\n",
    "                        p.grad.view(-1) for p in self.parameters() if p.grad is not None\n",
    "                    ]\n",
    "                    all_gradients = torch.cat(gradients)\n",
    "                    gradient_norm = torch.norm(all_gradients).item()\n",
    "                    epoch_gradient_norm += gradient_norm * current_batch_size\n",
    "\n",
    "                    metrics = {\n",
    "                        \"step\": step * (epoch + 1),\n",
    "                        \"train_batch_total_loss\": self.training_loss.item(),\n",
    "                        \"train_batch_prediction_loss\": self.prediction_loss.item(),\n",
    "                        \"train_batch_penalty_loss\": self.penalty_loss,\n",
    "                        \"train_batch_gradient_norm\": gradient_norm,\n",
    "                    }\n",
    "\n",
    "                    if wandb.run:\n",
    "                        wandb.log(metrics)\n",
    "                    if step % 5 == 0:\n",
    "                        print(metrics)\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)  # note: this was incorrectly displaying before!\n",
    "\n",
    "                    # Check if it's time to save a checkpoint\n",
    "                    current_time = time.time()\n",
    "                    # first loop initialization\n",
    "                    if batch_index == 0 and epoch == 0:\n",
    "                        last_checkpoint_time = -100000\n",
    "\n",
    "                if wandb.run:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"epoch_train_total_loss\": epoch_train_loss\n",
    "                            / num_datapoints_in_epoch,\n",
    "                            \"gradient_norm\": epoch_gradient_norm\n",
    "                            / num_datapoints_in_epoch,\n",
    "                        }\n",
    "                    )\n",
    "            # Save the final model\n",
    "            torch.save(self.state_dict(), \"final_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def generate_dataset(n_samples, split=\"train\", domains=[\"city\", \"nobel_prize_winner\", \"occupation\", \"physical_object\", \"verb\"], seed=42):\n",
    "    \n",
    "    # Seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    \n",
    "    sample_per_domain = n_samples // len(domains)\n",
    "    \n",
    "    for domain in domains:\n",
    "        \n",
    "        templates = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_attribute_to_prompts.json\"), \"r\"))\n",
    "        entities = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_entity_attributes.json\"), \"r\"))\n",
    "        entities_split = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_entity_to_split.json\"), \"r\"))\n",
    "        templates_split = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_prompt_to_split.json\"), \"r\"))\n",
    "\n",
    "        templates_train = {k: [v for v in vs if templates_split[v] == \"train\"] for k, vs in templates.items()}\n",
    "        templates_test = {k: [v for v in vs if templates_split[v] != \"train\"] for k, vs in templates.items()}\n",
    "\n",
    "        entities_train = {k: v for k, v in entities.items() if entities_split[k] == \"train\"}\n",
    "        name_train = list(entities_train.keys())\n",
    "\n",
    "        entities_test = {k: v for k, v in entities.items() if entities_split[k] != \"train\"}\n",
    "        name_test = list(entities_test.keys())\n",
    "        \n",
    "        if split == \"train\":\n",
    "            entity_dict, entity_name, prompt_dict = entities_train, name_train, templates_train\n",
    "        elif split == \"test\":\n",
    "            entity_dict, entity_name, prompt_dict = entities_test, name_test, templates_test\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train' or 'test'\")\n",
    "        \n",
    "        all_attributes = list(prompt_dict.keys())\n",
    "        \n",
    "        for _ in range(sample_per_domain):\n",
    "            \n",
    "            attribute = random.choice(all_attributes)\n",
    "            frozen_attributes = [k for k in all_attributes if k != attribute]\n",
    "            \n",
    "            data = {}\n",
    "            \n",
    "            source_entity, base_entity = random.sample(entity_name, 2)\n",
    "            source_entity_dict, base_entity_dict = entity_dict[source_entity], entity_dict[base_entity]\n",
    "            \n",
    "            template = random.choice(prompt_dict[attribute])\n",
    "\n",
    "            data[\"input_text\"] = template % base_entity\n",
    "            data[\"edit_instruction\"] = f\"{base_entity} ; {source_entity} - {attribute}\"\n",
    "            data[\"target\"] = source_entity_dict[attribute]\n",
    "            \n",
    "            data[\"unaffected_attributes\"] = []\n",
    "            \n",
    "            for frozen_attribute in frozen_attributes:\n",
    "                \n",
    "                attribute_template = random.choice(prompt_dict[frozen_attribute])\n",
    "                base_prompt = attribute_template % base_entity\n",
    "                target = base_entity_dict[frozen_attribute]\n",
    "                \n",
    "                data[\"unaffected_attributes\"].append(\n",
    "                    {\n",
    "                        \"attribute\": frozen_attribute,\n",
    "                        \"base_prompt\": base_prompt,\n",
    "                        \"target\": target\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "            dataset.append(data)\n",
    "    \n",
    "    dataset = Dataset.from_list(dataset)\n",
    "    return dataset\n",
    "\n",
    "city_train_set = generate_dataset(5000, split=\"train\")\n",
    "city_test_set =  generate_dataset(1000, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ravel_collate_fn(batch):\n",
    "    \n",
    "    def tokenize_text_inputs(texts, target_texts):\n",
    "        \n",
    "        input_texts = [text + \" \" + target for text, target in zip(texts, target_texts)]\n",
    "        tokenized = tokenizer(input_texts, return_tensors=\"pt\", padding=True, max_length=50, truncation=True)\n",
    "        tokenized_labels = []\n",
    "        \n",
    "        for input_ids, input_text in zip(tokenized[\"input_ids\"], texts):\n",
    "            input_length = tokenizer(input_text, return_tensors=\"pt\", padding=False)[\"input_ids\"].shape[-1]\n",
    "            label = torch.full_like(input_ids, -100)\n",
    "            label[input_length:] = input_ids[input_length:]\n",
    "            label[input_ids == tokenizer.pad_token_id] = -100\n",
    "            tokenized_labels.append(label)\n",
    "        \n",
    "        tokenized_labels = torch.stack(tokenized_labels)\n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"],\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"labels\": tokenized_labels\n",
    "        }\n",
    "        \n",
    "    prompts, edit_instructions, targets, unaffected_attributes = [], [], [], []\n",
    "    for b in batch:\n",
    "        prompts.append(b[\"input_text\"])\n",
    "        edit_instructions.append(b[\"edit_instruction\"])\n",
    "        targets.append(b[\"target\"])\n",
    "        unaffected_attributes.append(b[\"unaffected_attributes\"])\n",
    "        \n",
    "        \n",
    "    editor_input_ids = tokenizer(edit_instructions, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    \n",
    "    returned_dict = {\n",
    "        \"editor_input_ids\": editor_input_ids,\n",
    "        **tokenize_text_inputs(prompts, targets),\n",
    "    }\n",
    "    \n",
    "    base_prompts_unaffected, targets_unaffected, edit_instructions_unaffected, instance_indices = [], [], [], []\n",
    "    \n",
    "    for i, attribute_list in enumerate(unaffected_attributes):\n",
    "        base_prompts_unaffected.extend([d[\"base_prompt\"] for d in attribute_list])\n",
    "        targets_unaffected.extend([d[\"target\"] for d in attribute_list])\n",
    "        \n",
    "        for _ in range(len(attribute_list)):\n",
    "            edit_instructions_unaffected.append(editor_input_ids[i])\n",
    "            instance_indices.append(i)\n",
    "        \n",
    "    edit_instructions_unaffected = torch.stack(edit_instructions_unaffected)\n",
    "    instance_indices = torch.tensor(instance_indices)\n",
    "    \n",
    "    assert len(base_prompts_unaffected) == len(targets_unaffected)\n",
    "    \n",
    "    tokenized_unaffected = tokenize_text_inputs(base_prompts_unaffected, targets_unaffected)\n",
    "    \n",
    "    returned_dict[\"editor_input_ids_unaffected\"] = edit_instructions_unaffected\n",
    "    returned_dict[\"input_ids_unaffected\"] = tokenized_unaffected[\"input_ids\"]\n",
    "    returned_dict[\"attention_mask_unaffected\"] = tokenized_unaffected[\"attention_mask\"]\n",
    "    returned_dict[\"labels_unaffected\"] = tokenized_unaffected[\"labels\"]\n",
    "    returned_dict[\"instance_indices\"] = instance_indices\n",
    "    \n",
    "    return returned_dict\n",
    "\n",
    "batch_size = 32  # 50 or so\n",
    "data_loader = DataLoader(\n",
    "    city_train_set, batch_size=batch_size, collate_fn=ravel_collate_fn, shuffle=True, generator=torch.Generator(device='cuda')\n",
    ")  # batch_size, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(\n",
    "    city_test_set, batch_size=batch_size, collate_fn=ravel_collate_fn, shuffle=True, generator=torch.Generator(device='cuda')\n",
    ")  \n",
    "\n",
    "for batch in data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:10qwasws) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5caef1388464125aabdff8c03f8c886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.012 MB uploaded\\r'), FloatProgress(value=0.34346760606524734, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-glade-11</strong> at: <a href='https://wandb.ai/jiudingsun/hypernetworks/runs/10qwasws' target=\"_blank\">https://wandb.ai/jiudingsun/hypernetworks/runs/10qwasws</a><br/> View project at: <a href='https://wandb.ai/jiudingsun/hypernetworks' target=\"_blank\">https://wandb.ai/jiudingsun/hypernetworks</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240613_175054-10qwasws/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:10qwasws). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d20766f16624ef38e1ccf050ba3004c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112204411377509, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/frink/sun.jiu/hypernetwork-editor/wandb/run-20240613_175825-4p9fuc1t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiudingsun/hypernetworks/runs/4p9fuc1t' target=\"_blank\">flowing-field-12</a></strong> to <a href='https://wandb.ai/jiudingsun/hypernetworks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiudingsun/hypernetworks' target=\"_blank\">https://wandb.ai/jiudingsun/hypernetworks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiudingsun/hypernetworks/runs/4p9fuc1t' target=\"_blank\">https://wandb.ai/jiudingsun/hypernetworks/runs/4p9fuc1t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stop_editing_index = 8\n",
    "hypernetwork = RavelEditorHypernetwork(\n",
    "    edit_dampening_factor=edit_dampening_factor,  # 1/10000,\n",
    "    use_layerwise_embeddings=False,\n",
    "    num_editing_heads=num_editing_heads,\n",
    "    edit_channel_width=editor_channel_width,\n",
    "    chop_editor_at_layer=chop_layer,\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project=\"hypernetworks\",\n",
    "    config={\"targetmodel\": \"gpt2\", \"editormodel\": \"gpt2\", \"dataset\": \"ravel\"},\n",
    ")\n",
    "\n",
    "# current problem: 1728 / 30864\n",
    "hypernetwork.run_train(\n",
    "    train_loader=data_loader,\n",
    "    test_loader=test_data_loader,\n",
    "    stop_editing_index=stop_editing_index,\n",
    "    epochs=3,\n",
    "    lam=0,  # 20000\n",
    "    lam_testing_penalty=0,\n",
    "    eval_per_steps = 50,\n",
    "    use_unaffected=False\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_batch in test_data_loader:\n",
    "    break\n",
    "\n",
    "result = hypernetwork.inspect_batch_prediction_ouptuts(test_batch, stop_editing_index=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at some outputs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9940, device='cuda:0'), 0.015099854703294113)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernetwork.eval_accuracy(test_data_loader, stop_editing_index=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokens = []\n",
    "for labels in test_batch[\"labels\"]:\n",
    "    labels = labels[labels != -100]\n",
    "    label_tokens.append(tokenizer.decode(labels, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' soft',\n",
       " ' male',\n",
       " ' library',\n",
       " ' male',\n",
       " ' Literature',\n",
       " ' forest',\n",
       " ' Physics',\n",
       " ' broadcast',\n",
       " ' medicine',\n",
       " ' /dɪˈskɝɪdʒ/',\n",
       " ' Africa/Blantyre',\n",
       " ' studio',\n",
       " ' shapes',\n",
       " ' purple',\n",
       " ' office',\n",
       " ' non-living things',\n",
       " ' male',\n",
       " ' blessed',\n",
       " ' Asia',\n",
       " ' 1953',\n",
       " ' 8',\n",
       " ' /ˈsut/',\n",
       " ' m',\n",
       " ' non-living thing',\n",
       " ' hospital',\n",
       " ' Arabic',\n",
       " ' probation',\n",
       " ' resolved',\n",
       " ' hard',\n",
       " '',\n",
       " ' dispatch power',\n",
       " ' Uzbekistan']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
