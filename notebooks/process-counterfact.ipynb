{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['case_id', 'pararel_idx', 'requested_rewrite', 'paraphrase_prompts', 'neighborhood_prompts', 'attribute_prompts', 'generation_prompts'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to download counterfact, you can use terminal command:\n",
    "# curl -o counterfact.json https://rome.baulab.info/data/dsets/counterfact.json\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "#df = pd.read_pickle(\"/home/ubuntu/hypernetwork-editor/assets/data/wikipedia_df.pt\")\n",
    "#df.keys()\n",
    "#Yields:\n",
    "\n",
    "# Index(['title', 'first_sentences', 'second_sentences', 'third_sentences',\n",
    "    #    'first_sentence_length', 'second_sentence_length',\n",
    "    #    'third_sentence_length', 'tokenized_first_sentence',\n",
    "    #    'tokenized_next_50_tokens'],\n",
    "    #   dtype='object')\n",
    "\n",
    "# We will now try to roughly match these up with the Counterfact dataset\n",
    "path = '../assets/data/counterfact.json'\n",
    "import json\n",
    "#load the json file\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'case_id': 0,\n",
       " 'pararel_idx': 2796,\n",
       " 'requested_rewrite': {'prompt': 'The mother tongue of {} is',\n",
       "  'relation_id': 'P103',\n",
       "  'target_new': {'str': 'English', 'id': 'Q1860'},\n",
       "  'target_true': {'str': 'French', 'id': 'Q150'},\n",
       "  'subject': 'Danielle Darrieux'},\n",
       " 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native',\n",
       "  'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'],\n",
       " 'neighborhood_prompts': ['The mother tongue of Léon Blum is',\n",
       "  'The native language of Montesquieu is',\n",
       "  'François Bayrou, a native',\n",
       "  'The native language of Raymond Barre is',\n",
       "  'Michel Rocard is a native speaker of',\n",
       "  'Jacques Chaban-Delmas is a native speaker of',\n",
       "  'The native language of François Bayrou is',\n",
       "  'Maurice Genevoix, speaker of',\n",
       "  'The mother tongue of François Bayrou is',\n",
       "  'Melchior de Vogüé, speaker of'],\n",
       " 'attribute_prompts': ['J.\\xa0R.\\xa0R. Tolkien is a native speaker of',\n",
       "  'The mother tongue of Douglas Adams is',\n",
       "  'The mother tongue of Paul McCartney is',\n",
       "  'Elvis Presley is a native speaker of',\n",
       "  'Barack Obama, speaker of',\n",
       "  'Douglas Adams, speaker of',\n",
       "  'Meryl Streep, a native',\n",
       "  'George Orwell spoke the language',\n",
       "  'George Washington, a native',\n",
       "  'Michael Jackson, a native'],\n",
       " 'generation_prompts': [\"Danielle Darrieux's mother tongue is\",\n",
       "  'Where Danielle Darrieux is from, people speak the language of',\n",
       "  \"Danielle Darrieux's mother tongue is\",\n",
       "  'Danielle Darrieux was born in',\n",
       "  \"Danielle Darrieux's mother tongue is\",\n",
       "  \"Danielle Darrieux's mother tongue is\",\n",
       "  'Danielle Darrieux was born in',\n",
       "  'Where Danielle Darrieux is from, people speak the language of',\n",
       "  'Danielle Darrieux was born in',\n",
       "  'Danielle Darrieux was born in']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check: what datatype is the wikipedia df?\n",
    "#it is a pandas.core.frame.DataFrame ; so we will try to emulate that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#Write a function which continues a string with gpt2\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Move model to the specified device\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "model.to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to process your list of dictionaries into a pandas DataFrame\n",
    "def process_data(data):\n",
    "    # Initialize an empty list to hold the processed rows\n",
    "    processed_data = []\n",
    "    \n",
    "    # Loop through each entry in the data\n",
    "    for entry in data:\n",
    "        # Format the 'instruction' by substituting the subject into the prompt and appending the 'target_new'\n",
    "        instruction = entry['requested_rewrite']['prompt'].format(entry['requested_rewrite']['subject']) + \" \" + entry['requested_rewrite']['target_new']['str'] + \". \"\n",
    "        instruction_ids = tokenizer.encode(instruction, return_tensors='pt')\n",
    "\n",
    "        # Choose one example from 'generation_prompts' (first one in this case)\n",
    "        generation_example = entry['generation_prompts'][0]\n",
    "        partial_generation_ids = tokenizer.encode(generation_example, return_tensors='pt')\n",
    "\n",
    "        # Append the processed data to the list\n",
    "        processed_data.append({'instruction': instruction, 'instruction_ids': instruction_ids,\n",
    "                                'generation_example': generation_example, 'partial_generation_ids': partial_generation_ids})\n",
    "    \n",
    "    # Convert the list of processed data into a DataFrame\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # Example usage with your data (assuming 'data' is your list of dictionaries)\n",
    "# data = [\n",
    "#     {'case_id': 0,\n",
    "#      'pararel_idx': 2796,\n",
    "#      'requested_rewrite': {'prompt': 'The mother tongue of {} is',\n",
    "#                            'relation_id': 'P103',\n",
    "#                            'target_new': {'str': 'English', 'id': 'Q1860'},\n",
    "#                            'target_true': {'str': 'French', 'id': 'Q150'},\n",
    "#                            'subject': 'Danielle Darrieux'},\n",
    "#      'generation_prompts': [\"Danielle Darrieux's mother tongue is\",\n",
    "#                             \"Where Danielle Darrieux is from, people speak the language of\"]\n",
    "#     }\n",
    "#     # Add more dictionaries as needed\n",
    "# ]\n",
    "\n",
    "# Process the data\n",
    "dfc = process_data(data)\n",
    "\n",
    "# Display or use the DataFrame as needed\n",
    "#print(dfc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import pad\n",
    "def process_batches(df, batch_size=200):\n",
    "    # List to store outputs\n",
    "    generated_ids = []\n",
    "    generated_texts = []\n",
    "    \n",
    "    # Process dataframe in batches\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Concatenate tensors for each pair in the batch\n",
    "        concatenated_tensors = [torch.cat([inst.squeeze(0).to(device), gen.squeeze(0).to(device)], dim=0).to(device) for inst, gen in zip(batch['instruction_ids'], batch['partial_generation_ids'])]\n",
    "        \n",
    "        # Calculate the maximum length of the tensors in the batch\n",
    "        max_length = max(t.size(0) for t in concatenated_tensors)\n",
    "        \n",
    "        # Manually pad tensors on the left\n",
    "        padded_tensors = [pad(t, (max_length - t.size(0), 0), value=tokenizer.pad_token_id) for t in concatenated_tensors]\n",
    "        \n",
    "        # Stack tensors into a batch and set to cuda\n",
    "        padded_tensor = torch.stack(padded_tensors, dim=0)\n",
    "\n",
    "        # Generate responses from the model for the whole batch\n",
    "        responses = model.generate(padded_tensor, max_length=padded_tensor.size(1) + 50, pad_token_id=tokenizer.pad_token_id)\n",
    "        \n",
    "        # Extract only the generated part by slicing the tensor\n",
    "        full_generated_ids = [response[(padded_tensor.size(1) - gen.shape[1]): (padded_tensor.size(1) - gen.shape[1] + 50)] for response, gen in zip(responses, batch['partial_generation_ids'])]\n",
    "        \n",
    "        # Batch decode the generated part of the tensor responses\n",
    "        generated_text_batch = tokenizer.batch_decode(full_generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        generated_ids.extend(full_generated_ids)\n",
    "        generated_texts.extend(generated_text_batch)\n",
    "        print([i,len(df) / batch_size])\n",
    "\n",
    "    return [generated_ids, generated_texts]\n",
    "\n",
    "# Example usage\n",
    "# Let's assume 'df' is your DataFrame\n",
    "results = process_batches(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21919"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now rename `instruction_ids` to `tokenized_first_sentence` ; and set `tokenized_next_50_tokens` to results[0]\n",
    "\n",
    "dfc['tokenized_first_sentence'] = dfc['instruction_ids']\n",
    "dfc['tokenized_next_50_tokens'] = [result.cpu() for result in results[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The trademark of BBC One is owned by Sega.\\n\\nThe trademark of BBC One is owned by Sega. The trademark of BBC One is owned by Sega. The trademark of BBC One is owned by Sega. The trademark of BBC One is owned by'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dfc[\"tokenized_next_50_tokens\"][10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
