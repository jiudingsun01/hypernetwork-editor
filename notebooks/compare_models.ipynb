{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import types\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import yaml\n",
    "from torch import compile, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Model, GPT2Tokenizer\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "\n",
    "import wandb\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from models.gpt2 import GPT2Editor\n",
    "from models.gpt2.config import GPT2EditorConfig\n",
    "from train_utils import train, compute_kl_loss, slice_and_move_batch_for_device\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_editing_index = 10  # 4090 or A6000j\n",
    "edit_channel_multiply_factor = 2\n",
    "num_editing_heads = 32  # more seems to be better for this #per sid's suggestion: can add more heads in every layer. This is probably a really great suggestion\n",
    "editor_channel_width = 768 * edit_channel_multiply_factor\n",
    "max_grad_clip = 4.0\n",
    "chop_layer = 1\n",
    "lr = 3e-4\n",
    "edit_dampening_factor = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cross_attention_to_layer(block, config):\n",
    "    block.crossattention = GPT2Attention(config, is_cross_attention=True)\n",
    "    block.ln_cross_attn = nn.LayerNorm(\n",
    "        normalized_shape=768, eps=config.layer_norm_epsilon\n",
    "    )\n",
    "    original_query_weights = block.attn.c_attn.weight[:, :768]\n",
    "    original_keys_values = block.attn.c_attn.weight[:, 768:]\n",
    "    original_query_bias = block.attn.c_attn.bias[:768]\n",
    "    original_keys_values_bias = block.attn.c_attn.bias[768:]\n",
    "    with torch.no_grad():\n",
    "        # Initialize the new layer with these parameters\n",
    "        block.crossattention.q_attn.weight = nn.Parameter(original_query_weights)\n",
    "        block.crossattention.q_attn.bias = nn.Parameter(original_query_bias)\n",
    "        block.crossattention.c_attn.weight = nn.Parameter(original_keys_values)\n",
    "        block.crossattention.c_attn.bias = nn.Parameter(original_keys_values_bias)\n",
    "    return\n",
    "\n",
    "\n",
    "class Editor_Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Controls whether the head will do a global softmax in all positions & layers\n",
    "        # If True, the attn is global and will sum to 1\n",
    "        # If False, the attn is a logistic fxn independently for every layer & token\n",
    "        # I suspect we will also want to penalize the intervention norm\n",
    "        self.num_editing_heads = (\n",
    "            config.num_editing_heads\n",
    "        )  # should default to 1, but we're going to test adding more\n",
    "        self.edit_channel_width = config.edit_channel_width\n",
    "        if self.edit_channel_width % self.num_editing_heads != 0:\n",
    "            print(\"Error: config hidden size is not divisible by num_editing_heads\")\n",
    "        self.head_dim = self.edit_channel_width // self.num_editing_heads\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        max_positions = (\n",
    "            config.max_position_embeddings\n",
    "        )  # does this do anything? can try killing this later\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(\n",
    "                torch.ones((max_positions, max_positions), dtype=torch.bool)\n",
    "            ).view(1, 1, max_positions, max_positions),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n",
    "\n",
    "        # We compute Q and K as a single nn.linear; but will later break apart into subcomponents\n",
    "\n",
    "        ## Before modification to a variable channel-width\n",
    "        # self.q_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.k_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.v_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        self.q_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.k_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.v_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.out_proj = nn.Linear(self.edit_channel_width, self.embed_dim)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, head_dim).\"\"\"\n",
    "        new_shape = x.size()[:-1] + (self.num_editing_heads, self.head_dim)\n",
    "        return x.view(*new_shape)\n",
    "\n",
    "    def _new_reverse_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        # Assume that we are doing softmax attention\n",
    "        # Project and split the query, key, value tensors\n",
    "        split_query = self._split_heads(query)\n",
    "        split_key = self._split_heads(key)\n",
    "        split_value = self._split_heads(value)\n",
    "\n",
    "        # Double-application (is this actually good/better for some reason?)\n",
    "        # self._split_heads(self.q_attn(query))\n",
    "        # self._split_heads(self.k_attn(key))\n",
    "        # self._split_heads(self.v_attn(value))\n",
    "\n",
    "        if split_query.dim() != 4:\n",
    "            print(\"Error: Expected query to be 4D tensor, but got something else!\")\n",
    "        if split_key.dim() != 3:\n",
    "            print(\"Error: Expected key to be 3D tensor, but got something else!\")\n",
    "        if split_value.dim() != 3:\n",
    "            print(\"Error: Expected value to be 3D tensor, but got something else!\")\n",
    "\n",
    "        # Query should be shaped as (batch_index, sequence_index, head_index, head_dim)\n",
    "        # Key and value should be shaped as (batch_index, head_index, head_dim)\n",
    "\n",
    "        # print(\n",
    "        #     \"SHAPES PRIOR TO ATTN CALC\",\n",
    "        #     split_query.permute(0, 2, 1, 3).shape,\n",
    "        #     split_key.unsqueeze(-1).shape,\n",
    "        #     split_value.unsqueeze(-1).shape,\n",
    "        # )\n",
    "\n",
    "        # split_query: (bsz, seq, num_head, head_dim)\n",
    "        # split_key:   (bsz, num_heads, head_dim, 1)\n",
    "\n",
    "        # out: (bsz, num_heads, seq, 1) -> (bsz, num_heads, seq)\n",
    "\n",
    "        KQ_weights = torch.matmul(\n",
    "            split_query.permute(0, 2, 1, 3), split_key.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Scaling factor\n",
    "        KQ_weights = KQ_weights / torch.full(\n",
    "            [],\n",
    "            split_value.size(-1) ** 0.5,\n",
    "            dtype=KQ_weights.dtype,\n",
    "            device=KQ_weights.device,\n",
    "        )\n",
    "\n",
    "        # Then we take the softmax within the positional divisions\n",
    "        softmaxed_weights = nn.functional.softmax(KQ_weights, dim=-1)\n",
    "\n",
    "        # Adjusting value selection for head dimension\n",
    "        attn_output = torch.matmul(\n",
    "            softmaxed_weights.unsqueeze(-1), split_value.unsqueeze(-2)\n",
    "        )\n",
    "\n",
    "        # combine heads: change 50, 8, 104, 96 to 50, 104, 768\n",
    "        # first, permute\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3)\n",
    "\n",
    "        # combin heads x head_dims\n",
    "        attn_output = attn_output.reshape(\n",
    "            -1, attn_output.size(1), attn_output.size(2) * attn_output.size(3)\n",
    "        )\n",
    "        # now project back\n",
    "        projected_output = self.out_proj(attn_output)\n",
    "\n",
    "        return projected_output, softmaxed_weights\n",
    "\n",
    "    def _reverse_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        if key.dim() == 4:\n",
    "            K_reduced = key[\n",
    "                :, :, -1, :\n",
    "            ]  # R# Check: that the second dimension of K is only a single element when we have batching\n",
    "            KQ_weights = torch.bmm(K_reduced, query.transpose(1, 2))\n",
    "            logistic_weights = torch.atan(KQ_weights)\n",
    "            attn_output = torch.bmm(\n",
    "                logistic_weights.transpose(1, 2),\n",
    "                value[\n",
    "                    :, :, -1, :\n",
    "                ],  # we take the editor output only over the final token position\n",
    "            )\n",
    "\n",
    "        if key.dim() == 3:\n",
    "            QK_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "            logistic_weights = torch.atan(QK_weights)\n",
    "            attn_output = torch.matmul(logistic_weights, value)\n",
    "\n",
    "        return attn_output, logistic_weights\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        editor_hidden_states,\n",
    "        target_hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # Here, the query is the target hidden encoder, the key is the editor, and the value is the editor\n",
    "        query = self.q_attn(target_hidden_states)\n",
    "        if editor_hidden_states.dim() == 3:\n",
    "            key = self.k_attn(\n",
    "                # I don't quite understand why sometimes editor_hidden_states is 4 dimensional, sometimes 3\n",
    "                # seems like it's sometimes 20, 1, 4, 768 and sometimes 20, 4, 768. what gives?\n",
    "                editor_hidden_states[:, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "            value = self.v_attn(\n",
    "                # [:, 0, :1, :]\n",
    "                editor_hidden_states[:, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "\n",
    "        if editor_hidden_states.dim() == 4:\n",
    "            key = self.k_attn(\n",
    "                editor_hidden_states[:, 0, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "            value = self.v_attn(\n",
    "                # [:, 0, :1, :]\n",
    "                editor_hidden_states[:, 0, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "\n",
    "        attn_output, attn_weights = self._new_reverse_attn(query, key, value)\n",
    "\n",
    "        if output_attentions:\n",
    "            return attn_output, attn_weights\n",
    "        else:\n",
    "            return attn_output\n",
    "\n",
    "\n",
    "def new_forward(\n",
    "    self,\n",
    "    input_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "    attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    token_type_ids: Optional[torch.LongTensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    head_mask: Optional[torch.FloatTensor] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "    encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    # labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple]:\n",
    "    r\"\"\"\n",
    "    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "        Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "        `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "        are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "    \"\"\"\n",
    "\n",
    "    transformer_outputs = self.transformer(\n",
    "        input_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "        head_mask=head_mask,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        encoder_attention_mask=encoder_attention_mask,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "\n",
    "    # print(f\"OLD {input_ids.shape=}, {attention_mask.shape=}\")\n",
    "\n",
    "    hidden_states = transformer_outputs[0]\n",
    "\n",
    "    # print(\"HIDDEN STATE SHAPE\", hidden_states.shape)\n",
    "\n",
    "    # Set device for model parallelism\n",
    "    if self.model_parallel and torch.cuda.is_available():\n",
    "        torch.cuda.set_device(self.transformer.first_device)\n",
    "        hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "    # lm_logits = self.lm_head(hidden_states)\n",
    "    reverse_attention_output = self.lm_head(\n",
    "        hidden_states, encoder_hidden_states, output_attentions=output_attentions\n",
    "    )\n",
    "\n",
    "    # print(\"REVERSE ATTENTION OUTPUT SHAPE\", reverse_attention_output[0].shape)\n",
    "\n",
    "    return reverse_attention_output\n",
    "\n",
    "\n",
    "def replace_linear_final_layer_with_bespoke_reverse_attention(model):\n",
    "    model.lm_head = Editor_Attention(config=model.config)\n",
    "    model.forward = new_forward.__get__(model, GPT2LMHeadModel)\n",
    "    return\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def add_fwd_hooks(module_hooks):\n",
    "    \"\"\"\n",
    "    Context manager for temporarily adding forward hooks to a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    module_hooks\n",
    "        A list of pairs: (module, fnc) The function will be registered as a\n",
    "            forward hook on the module\n",
    "    \"\"\"\n",
    "    try:\n",
    "        handles = []\n",
    "        for mod, hk in module_hooks:\n",
    "            handles.append(mod.register_forward_hook(hk))\n",
    "        yield\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "\n",
    "def assign_layer_indices(model):\n",
    "    \"\"\"\n",
    "    Assigns a custom attribute 'layer_index' to each transformer layer in the GPT-2 model.\n",
    "    This function iterates over the transformer blocks and assigns an index to each.\n",
    "    \"\"\"\n",
    "    model.transformer.wte.layer_index = 0\n",
    "    for i, layer in enumerate(model.transformer.h):\n",
    "        layer.layer_index = i + 1\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# assign_layer_indices(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.compile #Apparently this fails when used inside jupyter notebooks but is fine if i make dedicated scripts\n",
    "class EditorHypernetwork(nn.Module):\n",
    "    # Separating the editor config file, from its base model's configurations\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_editing_heads=32,\n",
    "        edit_channel_width=768,  # controls dimensionality given to attention heads in the last layer of the editor\n",
    "        use_layerwise_embeddings=True,\n",
    "        chop_editor_at_layer=None,\n",
    "        edit_dampening_factor=0.001,  # tuning parameter to help the edits not be initialized too large\n",
    "        kill_token_zero=False,  # multiplies edits to token pos zero by zero\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Construct Editor Model\n",
    "        # Load the configuration from the YAML file\n",
    "        # with open(editor_yaml_file_path, 'r') as file:\n",
    "        #     self.config = yaml.safe_load(file)\n",
    "        if torch.cuda.is_available():\n",
    "            self.editor_model = (\n",
    "                GPT2LMHeadModel.from_pretrained(\"gpt2\").cuda().eval()\n",
    "            )  # have recently added .cuda() so it uses the gpu\n",
    "        else:\n",
    "            self.editor_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"mps\").eval()\n",
    "\n",
    "        # Add cross-attention to each layer\n",
    "        self.editor_model.config.add_cross_attention = True\n",
    "        self.editor_model.config.num_editing_heads = num_editing_heads\n",
    "        self.editor_model.config.chop_layer = chop_editor_at_layer\n",
    "        self.editor_model.config.kill_token_zero = kill_token_zero\n",
    "        self.editor_model.config.edit_channel_width = edit_channel_width\n",
    "\n",
    "        if chop_editor_at_layer is None:\n",
    "            chop_editor_at_layer = 12\n",
    "\n",
    "        for i in range(chop_editor_at_layer):\n",
    "            add_cross_attention_to_layer(\n",
    "                self.editor_model.transformer.h[i], self.editor_model.config\n",
    "            )\n",
    "\n",
    "        # Delete extra layers beyond the chop_layer\n",
    "        self.editor_model.transformer.h = self.editor_model.transformer.h[\n",
    "            :chop_editor_at_layer\n",
    "        ]\n",
    "\n",
    "        # Replace the final linear layer with special reverse attention output\n",
    "        self.editor_model.lm_head = Editor_Attention(config=self.editor_model.config)\n",
    "        self.editor_model.forward = new_forward.__get__(\n",
    "            self.editor_model, GPT2LMHeadModel\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.editor_model.cuda()\n",
    "        else:\n",
    "            self.editor_model.to(\"mps\")\n",
    "\n",
    "        # Construct Target Model\n",
    "        if torch.cuda.is_available():\n",
    "            self.target_model = (\n",
    "                transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\").cuda().eval()\n",
    "            )\n",
    "        else:\n",
    "            self.target_model = (\n",
    "                transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "                .to(\"mps\")\n",
    "                .eval()\n",
    "            )\n",
    "        for param in self.target_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        assign_layer_indices(self.target_model)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.target_model.cuda()\n",
    "        else:\n",
    "            self.target_model.to(\"mps\")\n",
    "\n",
    "        # Add module for layerwise embeddings\n",
    "        if use_layerwise_embeddings:\n",
    "            self.use_layerwise_embeddings = True\n",
    "            self.layerwise_embeddings = torch.randn(13, 768, requires_grad=True).to(\n",
    "                \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "            )\n",
    "        else:\n",
    "            self.use_layerwise_embeddings = False\n",
    "            self.layerwise_embeddings = 0\n",
    "\n",
    "        self.edit_dampening_factor = edit_dampening_factor\n",
    "\n",
    "        self.residual_cache = None\n",
    "        self.opt = None\n",
    "        self.lossfn = None\n",
    "        self.lam = None\n",
    "        self.penalty_loss = None\n",
    "        self.training_loss = None\n",
    "\n",
    "    # Gets the hidden states from the target model, if necessary\n",
    "    def run_target_model_for_encoded_hidden_states(self, target_ids):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.target_model(target_ids, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            return hidden_states\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        editor_input_ids,\n",
    "        target_input_ids,\n",
    "        target_hidden_states=None,\n",
    "        output_target_hidden_states=False,\n",
    "        output_edited_hidden_states=False,\n",
    "        output_edit_vectors=False,\n",
    "        output_editor_attention=False,\n",
    "        stop_editing_index=None,\n",
    "        batch_edit_vectors=None,\n",
    "    ):\n",
    "        # Run target model for encoded hidden states\n",
    "        if target_hidden_states is None:\n",
    "            target_hidden_states = torch.stack(\n",
    "                self.run_target_model_for_encoded_hidden_states(\n",
    "                    target_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "                ),  # seems to break while we are passing thru batch_size=1; the last (12th =) has different dimensions\n",
    "                dim=2,\n",
    "            )\n",
    "        # dimensions of target_hidden_states:\n",
    "        # batch_size, token_sequence_length, num_layers = 13, resid_width = 768\n",
    "\n",
    "        # If we are stopping editing at stop_editing_index, then we eliminate target_hidden_states beyond that index\n",
    "        if stop_editing_index is not None:\n",
    "            target_hidden_states = target_hidden_states[\n",
    "                :, :stop_editing_index, :, :\n",
    "            ].clone()\n",
    "\n",
    "        # Normalize along the last dimension\n",
    "        normalization_factors = target_hidden_states.norm(dim=-1, keepdim=True)\n",
    "        target_hidden_states = target_hidden_states / normalization_factors\n",
    "\n",
    "        # Error catching:\n",
    "        if batch_edit_vectors is not None:\n",
    "            if output_edit_vectors or output_editor_attention:\n",
    "                return \"Error: Inputting your own batch_edit_vectors means the model does not construct the outputs you are requesting\"\n",
    "\n",
    "        # Run editor model, get edit vectors\n",
    "        if batch_edit_vectors is None:\n",
    "            if self.use_layerwise_embeddings:\n",
    "                # Now, add in the layerwise embeddings\n",
    "                embedded_hidden_states = (\n",
    "                    target_hidden_states + self.layerwise_embeddings[None, None, :, :]\n",
    "                )\n",
    "\n",
    "                collapsed_target_hidden_states = embedded_hidden_states.reshape(\n",
    "                    target_hidden_states.shape[0],\n",
    "                    target_hidden_states.shape[1] * target_hidden_states.shape[2],\n",
    "                    target_hidden_states.shape[3],\n",
    "                )\n",
    "            else:\n",
    "                collapsed_target_hidden_states = target_hidden_states.reshape(\n",
    "                    target_hidden_states.shape[0],\n",
    "                    target_hidden_states.shape[1] * target_hidden_states.shape[2],\n",
    "                    target_hidden_states.shape[3],\n",
    "                )\n",
    "\n",
    "            # print(\"EDITOR INPUT ID SHAPE\", editor_input_ids.shape)\n",
    "            # print(\"TARGET HIDDEN SHAPE\", target_hidden_states.shape)\n",
    "\n",
    "            editor_output = self.editor_model(\n",
    "                editor_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "                encoder_hidden_states=collapsed_target_hidden_states,\n",
    "                output_attentions=output_editor_attention,\n",
    "            )\n",
    "            # Multiply the outputs by normalization factors\n",
    "            if output_editor_attention:\n",
    "                temp_edit_vectors = editor_output[0]\n",
    "                # Might want to reshape this too but whatever\n",
    "                batch_editor_attention = editor_output[1]\n",
    "            else:\n",
    "                temp_edit_vectors = editor_output\n",
    "\n",
    "            # print(\"TEMP EDIT VECTORS SHAPE\", temp_edit_vectors.shape)\n",
    "\n",
    "            # Renormalize to the scale of the target hidden states\n",
    "            # and reshape to proper dimensions\n",
    "            batch_edit_vectors = (\n",
    "                self.edit_dampening_factor\n",
    "                * normalization_factors\n",
    "                * temp_edit_vectors.reshape(\n",
    "                    temp_edit_vectors.shape[0], stop_editing_index, 13, 768\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # If we are stopping editing at stop_editing_index,\n",
    "        # this pads batch_edit_vectors with 0's to the right of the edited positions\n",
    "        if stop_editing_index is not None:\n",
    "            batch_edit_vectors = torch.cat(\n",
    "                (\n",
    "                    batch_edit_vectors,\n",
    "                    torch.zeros(\n",
    "                        batch_edit_vectors.shape[0],\n",
    "                        target_input_ids.shape[1] - stop_editing_index,\n",
    "                        13,\n",
    "                        768,\n",
    "                    ).cuda(),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        # Run target model with edit vectors. This adds the edit vectors to the given hidden state at the specified batch index, position, and layer\n",
    "        def edit_add(module, input, output):\n",
    "            layer_index = module.layer_index\n",
    "            output[0][:] = output[0] + batch_edit_vectors[:, :, layer_index, :]\n",
    "            if self.editor_model.config.kill_token_zero == True:\n",
    "                output[0][:, 0, :] = 0\n",
    "\n",
    "        def embedding_edit_add(module, input, output):\n",
    "            output[:] = output + batch_edit_vectors[:, :, 0, :]\n",
    "            if self.editor_model.config.kill_token_zero == True:\n",
    "                output[:, 0, :] = 0\n",
    "\n",
    "        # Now editing the target model\n",
    "        hooks1 = [(self.target_model.transformer.wte, embedding_edit_add)]\n",
    "        hooks2 = [(self.target_model.transformer.h[L], edit_add) for L in range(12)]\n",
    "        hooks = hooks1 + hooks2\n",
    "        with add_fwd_hooks(hooks):\n",
    "            # THIS IS THE LINE WHERE THE MODEL IS CALLED (AND THE EDITOR IS CALLED AT\n",
    "            # THE END OF `layer` AS A SIDE EFFECT)\n",
    "            target_result = self.target_model(\n",
    "                target_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "                output_hidden_states=output_edited_hidden_states,\n",
    "            )\n",
    "\n",
    "        logits = target_result.logits\n",
    "\n",
    "        output = {}\n",
    "        output[\"logits\"] = logits\n",
    "        if output_target_hidden_states:\n",
    "            output[\"target_hidden_states\"] = (\n",
    "                target_hidden_states * normalization_factors\n",
    "            )\n",
    "        if output_edited_hidden_states:\n",
    "            output[\"edited_hidden_states\"] = target_result.hidden_states\n",
    "        if output_edit_vectors:\n",
    "            output[\"edit_vectors\"] = batch_edit_vectors\n",
    "        if output_editor_attention:\n",
    "            output[\"editor_attention\"] = batch_editor_attention\n",
    "        return output\n",
    "\n",
    "    # Generate text using the target model, with a new edit application at every step.\n",
    "    # This is a very slow way to generate text.\n",
    "    # If you only want to edit first k tokens, use the forward pass instead with stop_editing_index = k\n",
    "    def inspect_batch_prediction_ouptuts(self, batch):\n",
    "        with torch.no_grad():\n",
    "            batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "            self.editor_inputs = batch[\"tokenized_first_sentence\"][i].unsqueeze(0)\n",
    "            self.target_inputs = batch[\"tokenized_next_50_tokens\"][i].unsqueeze(0)\n",
    "            self.prediction = self.forward(\n",
    "                self.editor_inputs,\n",
    "                self.target_inputs,\n",
    "                stop_editing_index=stop_editing_index,\n",
    "                output_target_hidden_states=False,\n",
    "                output_edited_hidden_states=False,\n",
    "                output_edit_vectors=False,\n",
    "                output_editor_attention=False,\n",
    "            )\n",
    "            # compute most likely tokens from the logits\n",
    "            predicted_ids = [\n",
    "                torch.argmax(pred, dim=-1) for pred in self.prediction[\"logits\"]\n",
    "            ]\n",
    "            # convert the token ids to strings\n",
    "            predicted_strings = [tokenizer.decode(pred) for pred in predicted_ids]\n",
    "            return predicted_strings\n",
    "\n",
    "    def evaluate_KL_test_loss_nogradient(\n",
    "        self, dataloader, f_data_to_soft_labels=None, stop_editing_index=8\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            sum_weighted_losses = 0.0\n",
    "            total_samples = 0\n",
    "            for batch in dataloader:\n",
    "                current_batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "                self.editor_inputs = batch[\"tokenized_first_sentence\"].squeeze(1)\n",
    "                self.target_inputs = batch[\"tokenized_next_50_tokens\"].squeeze(1)\n",
    "                self.prediction = self.forward(  # check the batch size\n",
    "                    self.editor_inputs,\n",
    "                    self.target_inputs,\n",
    "                    stop_editing_index=stop_editing_index,\n",
    "                )\n",
    "                log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                    self.prediction[\"logits\"][:, stop_editing_index:, :].reshape(\n",
    "                        -1, 50257\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "                # Now we must compute the soft labels!\n",
    "                # join the last 50 tokens to the editor inputs\n",
    "                soft_labels = f_data_to_soft_labels(\n",
    "                    batch[\"tokenized_first_sentence\"],\n",
    "                    batch[\"tokenized_next_50_tokens\"],\n",
    "                    num_predictions_max=50,\n",
    "                )\n",
    "                mask = (batch[\"tokenized_next_50_tokens\"] != 50256).reshape(-1)\n",
    "                self.loss = torch.nn.functional.kl_div(\n",
    "                    log_prob_predictions[mask, :],\n",
    "                    soft_labels[mask, :],\n",
    "                    reduction=\"batchmean\",\n",
    "                )\n",
    "                # Weight the loss by current batch size and update the sum of weighted losses\n",
    "                sum_weighted_losses += self.loss.item() * current_batch_size\n",
    "                total_samples += current_batch_size\n",
    "            weighted_average_loss = sum_weighted_losses / total_samples\n",
    "        return weighted_average_loss\n",
    "\n",
    "    def evaluate_crossentropy_test_loss_nogradient(\n",
    "        self, dataloader, stop_editing_index=8\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            sum_weighted_losses = 0.0\n",
    "            total_tokens = 0\n",
    "            for batch in dataloader:\n",
    "                # current_batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "                self.editor_inputs = batch[\"tokenized_first_sentence\"].squeeze(1)\n",
    "                self.target_inputs = batch[\"tokenized_next_50_tokens\"].squeeze(1)\n",
    "                self.prediction = self.forward(  # check the batch size\n",
    "                    self.editor_inputs,\n",
    "                    self.target_inputs,\n",
    "                    stop_editing_index=stop_editing_index,\n",
    "                )\n",
    "                log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                    self.prediction[\"logits\"][:, stop_editing_index:, :].reshape(\n",
    "                        -1, 50257\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "                # Create a mask to exclude padded tokens\n",
    "                target_labels = self.target_inputs[:, stop_editing_index:].reshape(-1)\n",
    "                mask = (\n",
    "                    target_labels != 50256\n",
    "                )  # Assuming padded tokens are represented by 0\n",
    "\n",
    "                # Compute the cross-entropy loss with masking\n",
    "                criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                loss = criterion(log_prob_predictions, target_labels)\n",
    "                current_mask_sum = mask.sum()\n",
    "                loss = (loss * mask).sum() / current_mask_sum\n",
    "\n",
    "                # Weight the loss by current batch size and update the sum of weighted losses\n",
    "                sum_weighted_losses += loss * current_mask_sum\n",
    "                total_tokens += current_mask_sum\n",
    "            weighted_average_loss = sum_weighted_losses / total_tokens\n",
    "        return weighted_average_loss\n",
    "\n",
    "    def run_train(\n",
    "        self,\n",
    "        train_loader,\n",
    "        test_loader=None,\n",
    "        stop_editing_index=8,\n",
    "        epochs=1,\n",
    "        KL_divergence_loss=False,\n",
    "        lam=0,  # 20000\n",
    "        lam_testing_penalty=0,  # 100000\n",
    "        f_data_to_soft_labels=None,\n",
    "        checkpoint_interval=60,  # save checkpoint every 60 minutes\n",
    "    ):\n",
    "        self.opt = optim.AdamW(\n",
    "            self.parameters(), lr=lr, weight_decay=0.01\n",
    "        )  # usually: lr = 5e-5. 1e-3 worked well!\n",
    "\n",
    "        if KL_divergence_loss:\n",
    "            self.lossfn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        else:\n",
    "            self.lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Create a tqdm progress bar\n",
    "            with tqdm(\n",
    "                total=len(train_loader),\n",
    "                desc=f\"Epoch {epoch + 1}/{epochs}\",\n",
    "                unit=\"batch\",\n",
    "                disable=True,\n",
    "            ) as pbar:\n",
    "                num_datapoints_in_epoch = 0\n",
    "                epoch_train_loss = 0\n",
    "                epoch_gradient_norm = 0\n",
    "                # Train loop\n",
    "                batch_index = -1  # index of first batch will be 0\n",
    "\n",
    "                for step, batch in enumerate(\n",
    "                    train_loader\n",
    "                ):  # not sure what this does for fractional batches. meh whatev\n",
    "                    batch_index += 1\n",
    "                    self.batch = batch\n",
    "                    current_batch_size = len(batch[\"tokenized_next_50_tokens\"])\n",
    "                    num_datapoints_in_epoch += current_batch_size\n",
    "                    self.opt.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    self.prediction = self.forward(\n",
    "                        batch[\"tokenized_first_sentence\"],\n",
    "                        batch[\"tokenized_next_50_tokens\"],\n",
    "                        stop_editing_index=stop_editing_index,\n",
    "                        output_target_hidden_states=True,\n",
    "                        output_edited_hidden_states=True,\n",
    "                        output_edit_vectors=True,\n",
    "                    )\n",
    "\n",
    "                    # Compute the penalty (edit size relative to the hidden state)\n",
    "                    self.lam = lam\n",
    "                    edit_ratio = self.prediction[\"edit_vectors\"].norm(dim=-1)[\n",
    "                        :, :stop_editing_index, :\n",
    "                    ] / self.prediction[\"target_hidden_states\"].norm(dim=-1)\n",
    "                    self.per_datapoint_penalty_loss = self.lam * torch.sum(\n",
    "                        edit_ratio, dim=[1, 2]\n",
    "                    )\n",
    "                    self.penalty_loss = torch.mean(self.per_datapoint_penalty_loss)\n",
    "\n",
    "                    # Compute the data loss\n",
    "                    if KL_divergence_loss:\n",
    "                        log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                            self.prediction[\"logits\"][:, :, :].reshape(-1, 50257),\n",
    "                            dim=1,\n",
    "                        )\n",
    "                        # Now we must compute the soft labels! This is outsourced to the user-provided function, teacher_model\n",
    "                        self.soft_labels = f_data_to_soft_labels(\n",
    "                            batch[\"tokenized_first_sentence\"],\n",
    "                            batch[\"tokenized_next_50_tokens\"],\n",
    "                            num_predictions_max=50,\n",
    "                        )\n",
    "                        # check that the mask makes sense!\n",
    "                        mask = (batch[\"tokenized_next_50_tokens\"] != 50256).reshape(-1)\n",
    "                        self.prediction_loss = self.lossfn(\n",
    "                            log_prob_predictions[mask, :], self.soft_labels[mask, :]\n",
    "                        )\n",
    "                        # NOTE: currently I am letting the loss predict on tokens inside the editing window\n",
    "                        # I didn't do this in the previous testing! Nor is it the case in crossentropy\n",
    "\n",
    "                    else:\n",
    "                        log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                            self.prediction[\"logits\"][\n",
    "                                :, stop_editing_index:, :\n",
    "                            ].reshape(-1, 50257),\n",
    "                            dim=1,\n",
    "                        )\n",
    "                        # Create a mask to exclude padded tokens\n",
    "                        # NOTE: here we are disallowing prediction on the first stop_editing_index tokens.\n",
    "                        # Code is currently formatted such that this is not the case for KL\n",
    "                        target_labels = batch[\"tokenized_next_50_tokens\"][\n",
    "                            :, stop_editing_index:\n",
    "                        ].reshape(-1)\n",
    "                        mask = target_labels != 50256\n",
    "                        # Compute the cross-entropy loss with masking\n",
    "                        criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                        loss = criterion(log_prob_predictions, target_labels.long())\n",
    "                        current_mask_sum = mask.sum()\n",
    "                        self.prediction_loss = (loss * mask).sum() / current_mask_sum\n",
    "\n",
    "                    # Compute the total loss and backpropagate\n",
    "                    self.training_loss = self.prediction_loss + self.penalty_loss\n",
    "                    self.training_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(\n",
    "                        self.parameters(), max_grad_clip\n",
    "                    )  # just implemented this! dunno if a cap of 1 to large, so I'm messing with reducing it\n",
    "\n",
    "                    # Check for nan gradients\n",
    "                    # if check_nan_gradients(self):\n",
    "                    #     break\n",
    "\n",
    "                    # Backwards step\n",
    "                    self.opt.step()\n",
    "\n",
    "                    # metrics\n",
    "                    epoch_train_loss += self.training_loss.item() * current_batch_size\n",
    "                    gradients = [\n",
    "                        p.grad.view(-1) for p in self.parameters() if p.grad is not None\n",
    "                    ]\n",
    "                    all_gradients = torch.cat(gradients)\n",
    "                    gradient_norm = torch.norm(all_gradients).item()\n",
    "                    epoch_gradient_norm += gradient_norm * current_batch_size\n",
    "\n",
    "                    metrics = {\n",
    "                        \"step\": step * (epoch + 1),\n",
    "                        \"train_batch_total_loss\": self.training_loss.item(),\n",
    "                        \"train_batch_prediction_loss\": self.prediction_loss.item(),\n",
    "                        \"train_batch_penalty_loss\": self.penalty_loss,\n",
    "                        \"train_batch_gradient_norm\": gradient_norm,\n",
    "                    }\n",
    "\n",
    "                    if wandb.run:\n",
    "                        wandb.log(metrics)\n",
    "                    if step % 100 == 0:\n",
    "                        print(metrics)\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)  # note: this was incorrectly displaying before!\n",
    "\n",
    "                    # Check if it's time to save a checkpoint\n",
    "                    current_time = time.time()\n",
    "                    # first loop initialization\n",
    "                    if batch_index == 0 and epoch == 0:\n",
    "                        last_checkpoint_time = -100000\n",
    "\n",
    "                    if current_time - last_checkpoint_time >= checkpoint_interval * 60:\n",
    "                        # Save the checkpoint\n",
    "                        torch.save(\n",
    "                            self.state_dict(),\n",
    "                            f\"checkpoint_epoch_{epoch}_batch_{pbar.n}.pt\",\n",
    "                        )\n",
    "                        last_checkpoint_time = current_time\n",
    "                        # announce checkpoint save\n",
    "                        print(\"Checkpoint saved at epoch\", epoch, \"batch\", pbar.n)\n",
    "\n",
    "                ####END BATCH LOOP\n",
    "                #########################\n",
    "\n",
    "                # epoch loss\n",
    "                # epoch_test_prediction_loss = self.evaluate_crossentropy_test_loss_nogradient(\n",
    "                #     test_loader,\n",
    "                #     stop_editing_index,\n",
    "                #     batch_size\n",
    "                # )\n",
    "                if KL_divergence_loss:\n",
    "                    epoch_test_prediction_loss = self.evaluate_KL_test_loss_nogradient(\n",
    "                        dataloader=test_loader,\n",
    "                        f_data_to_soft_labels=f_data_to_soft_labels,\n",
    "                        stop_editing_index=stop_editing_index,\n",
    "                    )\n",
    "\n",
    "                # # Calculate and accumulate gradient norm for logging\n",
    "                # gradients = [p.grad.view(-1) for p in self.parameters() if p.grad is not None]\n",
    "                # all_gradients = torch.cat(gradients)\n",
    "                # gradient_norm = torch.norm(all_gradients).item()\n",
    "                # epoch_gradient_norm += gradient_norm\n",
    "\n",
    "                if wandb.run:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"epoch_train_total_loss\": epoch_train_loss\n",
    "                            / num_datapoints_in_epoch,\n",
    "                            \"test_prediction_loss\": epoch_test_prediction_loss,\n",
    "                            \"gradient_norm\": epoch_gradient_norm\n",
    "                            / num_datapoints_in_epoch,\n",
    "                        }\n",
    "                    )\n",
    "            # Save the final model\n",
    "            torch.save(self, \"final_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernetwork = EditorHypernetwork(\n",
    "    edit_dampening_factor=edit_dampening_factor,  # 1/10000,\n",
    "    use_layerwise_embeddings=False,\n",
    "    num_editing_heads=num_editing_heads,\n",
    "    edit_channel_width=editor_channel_width,\n",
    "    chop_editor_at_layer=chop_layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = GPT2Editor(\n",
    "    GPT2EditorConfig(\n",
    "        use_layerwise_embeddings=False,\n",
    "        num_editing_heads=num_editing_heads,\n",
    "        edit_channel_multiply_factor=edit_channel_multiply_factor,\n",
    "        edit_dampening_factor=edit_dampening_factor,\n",
    "        chop_editor_at_layer=chop_layer,\n",
    "    )\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disabling dropout for \n",
      "disabling dropout for editor_model\n",
      "disabling dropout for editor_model.transformer\n",
      "disabling dropout for editor_model.transformer.wte\n",
      "disabling dropout for editor_model.transformer.wpe\n",
      "disabling dropout for editor_model.transformer.drop\n",
      "disabling dropout for editor_model.transformer.h\n",
      "disabling dropout for editor_model.transformer.h.0\n",
      "disabling dropout for editor_model.transformer.h.0.ln_1\n",
      "disabling dropout for editor_model.transformer.h.0.attn\n",
      "disabling dropout for editor_model.transformer.h.0.attn.c_attn\n",
      "disabling dropout for editor_model.transformer.h.0.attn.c_proj\n",
      "disabling dropout for editor_model.transformer.h.0.attn.attn_dropout\n",
      "disabling dropout for editor_model.transformer.h.0.attn.resid_dropout\n",
      "disabling dropout for editor_model.transformer.h.0.ln_2\n",
      "disabling dropout for editor_model.transformer.h.0.mlp\n",
      "disabling dropout for editor_model.transformer.h.0.mlp.c_fc\n",
      "disabling dropout for editor_model.transformer.h.0.mlp.c_proj\n",
      "disabling dropout for editor_model.transformer.h.0.mlp.act\n",
      "disabling dropout for editor_model.transformer.h.0.mlp.dropout\n",
      "disabling dropout for editor_model.transformer.h.0.crossattention\n",
      "disabling dropout for editor_model.transformer.h.0.crossattention.c_attn\n",
      "disabling dropout for editor_model.transformer.h.0.crossattention.q_attn\n",
      "disabling dropout for editor_model.transformer.h.0.crossattention.c_proj\n",
      "disabling dropout for editor_model.transformer.h.0.crossattention.attn_dropout\n",
      "disabling dropout for editor_model.transformer.h.0.crossattention.resid_dropout\n",
      "disabling dropout for editor_model.transformer.h.0.ln_cross_attn\n",
      "disabling dropout for editor_model.transformer.ln_f\n",
      "disabling dropout for editor_model.lm_head\n",
      "disabling dropout for editor_model.lm_head.q_attn\n",
      "disabling dropout for editor_model.lm_head.k_attn\n",
      "disabling dropout for editor_model.lm_head.v_attn\n",
      "disabling dropout for editor_model.lm_head.out_proj\n",
      "disabling dropout for target_model\n",
      "disabling dropout for target_model.transformer\n",
      "disabling dropout for target_model.transformer.wte\n",
      "disabling dropout for target_model.transformer.wpe\n",
      "disabling dropout for target_model.transformer.drop\n",
      "disabling dropout for target_model.transformer.h\n",
      "disabling dropout for target_model.transformer.h.0\n",
      "disabling dropout for target_model.transformer.h.0.ln_1\n",
      "disabling dropout for target_model.transformer.h.0.attn\n",
      "disabling dropout for target_model.transformer.h.0.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.0.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.0.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.0.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.0.ln_2\n",
      "disabling dropout for target_model.transformer.h.0.mlp\n",
      "disabling dropout for target_model.transformer.h.0.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.0.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.0.mlp.act\n",
      "disabling dropout for target_model.transformer.h.0.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.1\n",
      "disabling dropout for target_model.transformer.h.1.ln_1\n",
      "disabling dropout for target_model.transformer.h.1.attn\n",
      "disabling dropout for target_model.transformer.h.1.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.1.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.1.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.1.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.1.ln_2\n",
      "disabling dropout for target_model.transformer.h.1.mlp\n",
      "disabling dropout for target_model.transformer.h.1.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.1.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.1.mlp.act\n",
      "disabling dropout for target_model.transformer.h.1.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.2\n",
      "disabling dropout for target_model.transformer.h.2.ln_1\n",
      "disabling dropout for target_model.transformer.h.2.attn\n",
      "disabling dropout for target_model.transformer.h.2.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.2.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.2.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.2.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.2.ln_2\n",
      "disabling dropout for target_model.transformer.h.2.mlp\n",
      "disabling dropout for target_model.transformer.h.2.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.2.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.2.mlp.act\n",
      "disabling dropout for target_model.transformer.h.2.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.3\n",
      "disabling dropout for target_model.transformer.h.3.ln_1\n",
      "disabling dropout for target_model.transformer.h.3.attn\n",
      "disabling dropout for target_model.transformer.h.3.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.3.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.3.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.3.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.3.ln_2\n",
      "disabling dropout for target_model.transformer.h.3.mlp\n",
      "disabling dropout for target_model.transformer.h.3.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.3.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.3.mlp.act\n",
      "disabling dropout for target_model.transformer.h.3.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.4\n",
      "disabling dropout for target_model.transformer.h.4.ln_1\n",
      "disabling dropout for target_model.transformer.h.4.attn\n",
      "disabling dropout for target_model.transformer.h.4.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.4.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.4.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.4.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.4.ln_2\n",
      "disabling dropout for target_model.transformer.h.4.mlp\n",
      "disabling dropout for target_model.transformer.h.4.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.4.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.4.mlp.act\n",
      "disabling dropout for target_model.transformer.h.4.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.5\n",
      "disabling dropout for target_model.transformer.h.5.ln_1\n",
      "disabling dropout for target_model.transformer.h.5.attn\n",
      "disabling dropout for target_model.transformer.h.5.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.5.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.5.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.5.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.5.ln_2\n",
      "disabling dropout for target_model.transformer.h.5.mlp\n",
      "disabling dropout for target_model.transformer.h.5.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.5.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.5.mlp.act\n",
      "disabling dropout for target_model.transformer.h.5.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.6\n",
      "disabling dropout for target_model.transformer.h.6.ln_1\n",
      "disabling dropout for target_model.transformer.h.6.attn\n",
      "disabling dropout for target_model.transformer.h.6.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.6.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.6.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.6.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.6.ln_2\n",
      "disabling dropout for target_model.transformer.h.6.mlp\n",
      "disabling dropout for target_model.transformer.h.6.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.6.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.6.mlp.act\n",
      "disabling dropout for target_model.transformer.h.6.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.7\n",
      "disabling dropout for target_model.transformer.h.7.ln_1\n",
      "disabling dropout for target_model.transformer.h.7.attn\n",
      "disabling dropout for target_model.transformer.h.7.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.7.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.7.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.7.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.7.ln_2\n",
      "disabling dropout for target_model.transformer.h.7.mlp\n",
      "disabling dropout for target_model.transformer.h.7.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.7.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.7.mlp.act\n",
      "disabling dropout for target_model.transformer.h.7.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.8\n",
      "disabling dropout for target_model.transformer.h.8.ln_1\n",
      "disabling dropout for target_model.transformer.h.8.attn\n",
      "disabling dropout for target_model.transformer.h.8.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.8.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.8.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.8.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.8.ln_2\n",
      "disabling dropout for target_model.transformer.h.8.mlp\n",
      "disabling dropout for target_model.transformer.h.8.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.8.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.8.mlp.act\n",
      "disabling dropout for target_model.transformer.h.8.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.9\n",
      "disabling dropout for target_model.transformer.h.9.ln_1\n",
      "disabling dropout for target_model.transformer.h.9.attn\n",
      "disabling dropout for target_model.transformer.h.9.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.9.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.9.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.9.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.9.ln_2\n",
      "disabling dropout for target_model.transformer.h.9.mlp\n",
      "disabling dropout for target_model.transformer.h.9.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.9.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.9.mlp.act\n",
      "disabling dropout for target_model.transformer.h.9.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.10\n",
      "disabling dropout for target_model.transformer.h.10.ln_1\n",
      "disabling dropout for target_model.transformer.h.10.attn\n",
      "disabling dropout for target_model.transformer.h.10.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.10.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.10.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.10.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.10.ln_2\n",
      "disabling dropout for target_model.transformer.h.10.mlp\n",
      "disabling dropout for target_model.transformer.h.10.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.10.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.10.mlp.act\n",
      "disabling dropout for target_model.transformer.h.10.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.11\n",
      "disabling dropout for target_model.transformer.h.11.ln_1\n",
      "disabling dropout for target_model.transformer.h.11.attn\n",
      "disabling dropout for target_model.transformer.h.11.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.11.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.11.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.11.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.11.ln_2\n",
      "disabling dropout for target_model.transformer.h.11.mlp\n",
      "disabling dropout for target_model.transformer.h.11.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.11.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.11.mlp.act\n",
      "disabling dropout for target_model.transformer.h.11.mlp.dropout\n",
      "disabling dropout for target_model.transformer.ln_f\n",
      "disabling dropout for target_model.lm_head\n",
      "disabling dropout for \n",
      "disabling dropout for hypernetwork\n",
      "disabling dropout for hypernetwork.transformer\n",
      "disabling dropout for hypernetwork.transformer.wte\n",
      "disabling dropout for hypernetwork.transformer.wpe\n",
      "disabling dropout for hypernetwork.transformer.drop\n",
      "disabling dropout for hypernetwork.transformer.h\n",
      "disabling dropout for hypernetwork.transformer.h.0\n",
      "disabling dropout for hypernetwork.transformer.h.0.ln_1\n",
      "disabling dropout for hypernetwork.transformer.h.0.attn\n",
      "disabling dropout for hypernetwork.transformer.h.0.attn.c_attn\n",
      "disabling dropout for hypernetwork.transformer.h.0.attn.c_proj\n",
      "disabling dropout for hypernetwork.transformer.h.0.attn.attn_dropout\n",
      "disabling dropout for hypernetwork.transformer.h.0.attn.resid_dropout\n",
      "disabling dropout for hypernetwork.transformer.h.0.ln_2\n",
      "disabling dropout for hypernetwork.transformer.h.0.mlp\n",
      "disabling dropout for hypernetwork.transformer.h.0.mlp.c_fc\n",
      "disabling dropout for hypernetwork.transformer.h.0.mlp.c_proj\n",
      "disabling dropout for hypernetwork.transformer.h.0.mlp.act\n",
      "disabling dropout for hypernetwork.transformer.h.0.mlp.dropout\n",
      "disabling dropout for hypernetwork.transformer.h.0.crossattention\n",
      "disabling dropout for hypernetwork.transformer.h.0.crossattention.c_attn\n",
      "disabling dropout for hypernetwork.transformer.h.0.crossattention.q_attn\n",
      "disabling dropout for hypernetwork.transformer.h.0.crossattention.c_proj\n",
      "disabling dropout for hypernetwork.transformer.h.0.crossattention.attn_dropout\n",
      "disabling dropout for hypernetwork.transformer.h.0.crossattention.resid_dropout\n",
      "disabling dropout for hypernetwork.transformer.h.0.ln_cross_attn\n",
      "disabling dropout for hypernetwork.transformer.ln_f\n",
      "disabling dropout for hypernetwork.lm_head\n",
      "disabling dropout for hypernetwork.lm_head.c_attn\n",
      "disabling dropout for hypernetwork.lm_head.c_attn.0\n",
      "disabling dropout for hypernetwork.lm_head.c_attn.1\n",
      "disabling dropout for hypernetwork.lm_head.q_attn\n",
      "disabling dropout for hypernetwork.lm_head.q_attn.0\n",
      "disabling dropout for hypernetwork.lm_head.q_attn.1\n",
      "disabling dropout for hypernetwork.lm_head.c_proj\n",
      "disabling dropout for hypernetwork.lm_head.c_proj.0\n",
      "disabling dropout for hypernetwork.lm_head.c_proj.1\n",
      "disabling dropout for hypernetwork.lm_head.attn_dropout\n",
      "disabling dropout for hypernetwork.lm_head.resid_dropout\n",
      "disabling dropout for target_model\n",
      "disabling dropout for target_model.transformer\n",
      "disabling dropout for target_model.transformer.wte\n",
      "disabling dropout for target_model.transformer.wpe\n",
      "disabling dropout for target_model.transformer.drop\n",
      "disabling dropout for target_model.transformer.h\n",
      "disabling dropout for target_model.transformer.h.0\n",
      "disabling dropout for target_model.transformer.h.0.ln_1\n",
      "disabling dropout for target_model.transformer.h.0.attn\n",
      "disabling dropout for target_model.transformer.h.0.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.0.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.0.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.0.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.0.ln_2\n",
      "disabling dropout for target_model.transformer.h.0.mlp\n",
      "disabling dropout for target_model.transformer.h.0.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.0.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.0.mlp.act\n",
      "disabling dropout for target_model.transformer.h.0.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.1\n",
      "disabling dropout for target_model.transformer.h.1.ln_1\n",
      "disabling dropout for target_model.transformer.h.1.attn\n",
      "disabling dropout for target_model.transformer.h.1.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.1.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.1.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.1.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.1.ln_2\n",
      "disabling dropout for target_model.transformer.h.1.mlp\n",
      "disabling dropout for target_model.transformer.h.1.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.1.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.1.mlp.act\n",
      "disabling dropout for target_model.transformer.h.1.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.2\n",
      "disabling dropout for target_model.transformer.h.2.ln_1\n",
      "disabling dropout for target_model.transformer.h.2.attn\n",
      "disabling dropout for target_model.transformer.h.2.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.2.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.2.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.2.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.2.ln_2\n",
      "disabling dropout for target_model.transformer.h.2.mlp\n",
      "disabling dropout for target_model.transformer.h.2.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.2.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.2.mlp.act\n",
      "disabling dropout for target_model.transformer.h.2.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.3\n",
      "disabling dropout for target_model.transformer.h.3.ln_1\n",
      "disabling dropout for target_model.transformer.h.3.attn\n",
      "disabling dropout for target_model.transformer.h.3.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.3.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.3.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.3.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.3.ln_2\n",
      "disabling dropout for target_model.transformer.h.3.mlp\n",
      "disabling dropout for target_model.transformer.h.3.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.3.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.3.mlp.act\n",
      "disabling dropout for target_model.transformer.h.3.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.4\n",
      "disabling dropout for target_model.transformer.h.4.ln_1\n",
      "disabling dropout for target_model.transformer.h.4.attn\n",
      "disabling dropout for target_model.transformer.h.4.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.4.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.4.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.4.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.4.ln_2\n",
      "disabling dropout for target_model.transformer.h.4.mlp\n",
      "disabling dropout for target_model.transformer.h.4.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.4.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.4.mlp.act\n",
      "disabling dropout for target_model.transformer.h.4.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.5\n",
      "disabling dropout for target_model.transformer.h.5.ln_1\n",
      "disabling dropout for target_model.transformer.h.5.attn\n",
      "disabling dropout for target_model.transformer.h.5.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.5.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.5.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.5.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.5.ln_2\n",
      "disabling dropout for target_model.transformer.h.5.mlp\n",
      "disabling dropout for target_model.transformer.h.5.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.5.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.5.mlp.act\n",
      "disabling dropout for target_model.transformer.h.5.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.6\n",
      "disabling dropout for target_model.transformer.h.6.ln_1\n",
      "disabling dropout for target_model.transformer.h.6.attn\n",
      "disabling dropout for target_model.transformer.h.6.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.6.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.6.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.6.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.6.ln_2\n",
      "disabling dropout for target_model.transformer.h.6.mlp\n",
      "disabling dropout for target_model.transformer.h.6.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.6.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.6.mlp.act\n",
      "disabling dropout for target_model.transformer.h.6.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.7\n",
      "disabling dropout for target_model.transformer.h.7.ln_1\n",
      "disabling dropout for target_model.transformer.h.7.attn\n",
      "disabling dropout for target_model.transformer.h.7.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.7.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.7.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.7.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.7.ln_2\n",
      "disabling dropout for target_model.transformer.h.7.mlp\n",
      "disabling dropout for target_model.transformer.h.7.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.7.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.7.mlp.act\n",
      "disabling dropout for target_model.transformer.h.7.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.8\n",
      "disabling dropout for target_model.transformer.h.8.ln_1\n",
      "disabling dropout for target_model.transformer.h.8.attn\n",
      "disabling dropout for target_model.transformer.h.8.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.8.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.8.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.8.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.8.ln_2\n",
      "disabling dropout for target_model.transformer.h.8.mlp\n",
      "disabling dropout for target_model.transformer.h.8.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.8.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.8.mlp.act\n",
      "disabling dropout for target_model.transformer.h.8.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.9\n",
      "disabling dropout for target_model.transformer.h.9.ln_1\n",
      "disabling dropout for target_model.transformer.h.9.attn\n",
      "disabling dropout for target_model.transformer.h.9.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.9.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.9.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.9.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.9.ln_2\n",
      "disabling dropout for target_model.transformer.h.9.mlp\n",
      "disabling dropout for target_model.transformer.h.9.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.9.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.9.mlp.act\n",
      "disabling dropout for target_model.transformer.h.9.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.10\n",
      "disabling dropout for target_model.transformer.h.10.ln_1\n",
      "disabling dropout for target_model.transformer.h.10.attn\n",
      "disabling dropout for target_model.transformer.h.10.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.10.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.10.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.10.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.10.ln_2\n",
      "disabling dropout for target_model.transformer.h.10.mlp\n",
      "disabling dropout for target_model.transformer.h.10.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.10.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.10.mlp.act\n",
      "disabling dropout for target_model.transformer.h.10.mlp.dropout\n",
      "disabling dropout for target_model.transformer.h.11\n",
      "disabling dropout for target_model.transformer.h.11.ln_1\n",
      "disabling dropout for target_model.transformer.h.11.attn\n",
      "disabling dropout for target_model.transformer.h.11.attn.c_attn\n",
      "disabling dropout for target_model.transformer.h.11.attn.c_proj\n",
      "disabling dropout for target_model.transformer.h.11.attn.attn_dropout\n",
      "disabling dropout for target_model.transformer.h.11.attn.resid_dropout\n",
      "disabling dropout for target_model.transformer.h.11.ln_2\n",
      "disabling dropout for target_model.transformer.h.11.mlp\n",
      "disabling dropout for target_model.transformer.h.11.mlp.c_fc\n",
      "disabling dropout for target_model.transformer.h.11.mlp.c_proj\n",
      "disabling dropout for target_model.transformer.h.11.mlp.act\n",
      "disabling dropout for target_model.transformer.h.11.mlp.dropout\n",
      "disabling dropout for target_model.transformer.ln_f\n",
      "disabling dropout for target_model.lm_head\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def disable_dropout(model: torch.nn.Module):\n",
    "    \"\"\"Disable dropout in a model.\"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        print(f\"disabling dropout for {name}\")\n",
    "        if isinstance(module, torch.nn.Dropout):\n",
    "            module.p = 0\n",
    "\n",
    "\n",
    "def constant_init(m, val=0.5):\n",
    "    for p in m.parameters():\n",
    "        nn.init.constant_(p.data, val)\n",
    "\n",
    "\n",
    "disable_dropout(hypernetwork)\n",
    "disable_dropout(new_model)\n",
    "\n",
    "init_fn = partial(constant_init, val=0.1)\n",
    "\n",
    "_ = hypernetwork.editor_model.apply(init_fn)\n",
    "_ = new_model.hypernetwork.apply(init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "test_input_ids = torch.zeros(2, 8).long().cuda()\n",
    "test_attention_mask = torch.ones_like(test_input_ids).cuda()\n",
    "encoder_hidden_states = torch.ones(2, 8, 768).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_hn_old = hypernetwork.editor_model(\n",
    "        input_ids=test_input_ids,\n",
    "        attention_mask=test_attention_mask,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "    )\n",
    "    print(\"+\" * 100)\n",
    "    out_hn_new = new_model.hypernetwork(\n",
    "        input_ids=test_input_ids,\n",
    "        attention_mask=test_attention_mask,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: let's test the models end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example inputs for the models\n",
    "inputs = {\n",
    "    \"input_ids\": torch.tensor([[101, 2057, 2024, 100, 102, 1824, 1822, 1232, 121, 488]])\n",
    "    .cuda()\n",
    "    .repeat(2, 1),  # Example input_ids\n",
    "    \"attention_mask\": torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "    .repeat(2, 1)\n",
    "    .cuda(),  # Example attention_mask\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"] = torch.nn.functional.pad(\n",
    "    inputs[\"input_ids\"],\n",
    "    (0, 32 - inputs[\"input_ids\"].shape[-1]),\n",
    "    value=tokenizer.pad_token_id,\n",
    ")\n",
    "inputs[\"attention_mask\"] = torch.nn.functional.pad(\n",
    "    inputs[\"attention_mask\"], (0, 32 - inputs[\"attention_mask\"].shape[-1]), value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# test_input_ids = torch.zeros(2, 8).long().cuda()\n",
    "# test_attention_mask = torch.cat( torch.ones_like(test_input_ids).cuda()\n",
    "# encoder_hidden_states = torch.ones(2, 8, 768).cuda()\n",
    "\n",
    "new_model.config.use_ghost_token = False\n",
    "stop_editing_index = 8\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_hn_old = hypernetwork(\n",
    "        editor_input_ids=inputs[\"input_ids\"],\n",
    "        target_input_ids=inputs[\"input_ids\"],\n",
    "        stop_editing_index=8,\n",
    "    )\n",
    "    print(\"+\" * 100)\n",
    "    out_hn_new = new_model.forward(\n",
    "        editor_input_ids=inputs[\"input_ids\"],\n",
    "        editor_attention_mask=inputs[\"attention_mask\"],\n",
    "        target_input_ids=inputs[\"input_ids\"],\n",
    "        target_attention_mask=inputs[\"attention_mask\"],\n",
    "        stop_editing_idx=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.3.0 available.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds = datasets.load_from_disk(\n",
    "    \"/home/sidnbaskaran/hypernetwork-editor/assets/data/wikipedia-original\"\n",
    ")\n",
    "\n",
    "max_length = 512\n",
    "mode = \"right\"\n",
    "\n",
    "\n",
    "def collate_fn(batch, max_length):\n",
    "    res = {}\n",
    "    for k in batch[0].keys():\n",
    "        if k != \"__index_level_0__\":\n",
    "            els = [x[k] for x in batch]\n",
    "            if k == \"tokenized_next_50_tokens\":\n",
    "                max_length = 50\n",
    "                for i in range(len(els)):\n",
    "                    els[i] = [j for j in els[i] if j != tokenizer.eos_token_id]\n",
    "                    if len(els[i]) > 50:\n",
    "                        els[i] = els[i][:50]\n",
    "            # for x in els:\n",
    "            #     x += [0] * (max_length - len(x))\n",
    "            # res[k] = torch.cat([y , torch.zeros(max_length - len(x))],for y in els)\n",
    "            # res[k]=torch.tensor([x + [0] * (max_length( - len(x)) for x in els], dtype=torch.long)\n",
    "\n",
    "            res[k] = torch.stack(\n",
    "                [\n",
    "                    torch.nn.functional.pad(\n",
    "                        torch.tensor(x),\n",
    "                        (0, max_length - len(x))\n",
    "                        if mode == \"right\"\n",
    "                        else (max_length - len(x), 0),\n",
    "                        value=tokenizer.eos_token_id,\n",
    "                    )\n",
    "                    for x in els\n",
    "                ]\n",
    "            ).int()\n",
    "    return res\n",
    "\n",
    "\n",
    "dl = DataLoader(\n",
    "    ds[\"train\"], batch_size=1, collate_fn=partial(collate_fn, max_length=max_length)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3347,   373,   257,  2888,   286,   262,  3999,  2260,  1074,   326,\n",
       "          1839,  3869,   379,  1111,   262, 14745,   376,  3824,    33,  6926,\n",
       "           338,  2159,  5454,   290,   262, 14489,   376,  3824,    33,  6926,\n",
       "           338,  2159, 10749,    13,  1375,   635,  1839,   257,  3869, 18279,\n",
       "           379,   262, 14489,  7740,  5776,    11,   706,   543,   673,  9880]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl))[\"tokenized_next_50_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_and_pad_left(first, second):\n",
    "    batch_size, first_seq = first.size()\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "    _, second_seq = second.size()\n",
    "    # Find the lengths in A and B\n",
    "    lengths_A = torch.sum(first != pad_token, dim=1)\n",
    "    lengths_B = torch.sum(second != pad_token, dim=1)\n",
    "\n",
    "    # initialize empty tensor\n",
    "    max_len = max(lengths_A + lengths_B)\n",
    "    result = torch.full(\n",
    "        (\n",
    "            batch_size,\n",
    "            max_len,\n",
    "        ),\n",
    "        pad_token,\n",
    "        device=first.device,\n",
    "        dtype=first.dtype,\n",
    "    )\n",
    "    # Concatenate A[i] and B[i] a, assume LEFT padding\n",
    "    for i in range(batch_size):\n",
    "        result[i, max_len - lengths_B[i] - lengths_A[i] : max_len - lengths_B[i]] = (\n",
    "            first[i, first_seq - lengths_A[i] :]\n",
    "        )\n",
    "        result[i, max_len - lengths_B[i] :] = second[i, second_seq - lengths_B[i] :]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def concat_and_pad_right(A, B):\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "    batch_size, n_tokens_A = A.size()\n",
    "    n_tokens_B = B.size(1)\n",
    "\n",
    "    # Find the lengths in A and B\n",
    "    lengths_A = torch.sum(A != pad_token, dim=1)\n",
    "    lengths_B = torch.sum(B != pad_token, dim=1)\n",
    "\n",
    "    # iniitalize empty tensor\n",
    "    result = torch.full(\n",
    "        (\n",
    "            batch_size,\n",
    "            max(lengths_A + lengths_B),\n",
    "        ),\n",
    "        pad_token,\n",
    "        device=A.device,\n",
    "    )\n",
    "\n",
    "    # Concatenate A[i] and B[i] a\n",
    "    for i in range(batch_size):\n",
    "        result[i, : lengths_A[i]] = A[i, : lengths_A[i]]\n",
    "        result[i, lengths_A[i] : lengths_A[i] + lengths_B[i]] = B[i, : lengths_B[i]]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "concat_pad_fn = concat_and_pad_left if mode == \"left\" else concat_and_pad_right\n",
    "\n",
    "\n",
    "def new_kl_loss(\n",
    "    logits,\n",
    "    target_model,\n",
    "    batch,\n",
    "):\n",
    "    edited_target_logps = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    edit_target_mask = batch[\"tokenized_next_50_tokens\"] != tokenizer.pad_token_id\n",
    "\n",
    "    # compute soft labels\n",
    "    with torch.no_grad():\n",
    "        pad_token = target_model.config.eos_token_id\n",
    "        combined_input_ids = concat_pad_fn(\n",
    "            batch[\"tokenized_first_sentence\"], batch[\"tokenized_next_50_tokens\"]\n",
    "        )\n",
    "\n",
    "        target_logits = target_model(\n",
    "            input_ids=combined_input_ids, attention_mask=combined_input_ids != pad_token\n",
    "        ).logits\n",
    "\n",
    "        lengths_A = torch.sum(batch[\"tokenized_first_sentence\"] != pad_token, dim=1)\n",
    "        lengths_B = torch.sum(batch[\"tokenized_next_50_tokens\"] != pad_token, dim=1)\n",
    "\n",
    "        # Create an empty tensor to store the predictions\n",
    "        target_seq_len = edited_target_logps.shape[-2]\n",
    "        shape = (\n",
    "            len(lengths_A),\n",
    "            target_seq_len,\n",
    "            target_model.config.vocab_size,\n",
    "        )\n",
    "        extracted_logits = torch.full(\n",
    "            shape, torch.nan, device=edited_target_logps.device\n",
    "        )\n",
    "        if mode == \"left\":\n",
    "            # Extract the predictions corresponding to B, assume LEFT padding\n",
    "            for i in range(len(lengths_A)):\n",
    "                extracted_logits[i, -lengths_B[i] :, :] = target_logits[\n",
    "                    i, -lengths_B[i] :, :\n",
    "                ]\n",
    "        elif mode == \"right\":\n",
    "            # Extract the predictions corresponding to B\n",
    "            for i in range(len(lengths_A)):\n",
    "                extracted_logits[i, : lengths_B[i], :] = target_logits[\n",
    "                    i, lengths_A[i] : lengths_A[i] + lengths_B[i], :\n",
    "                ]\n",
    "        target_logps = torch.nn.functional.log_softmax(extracted_logits, dim=-1)\n",
    "\n",
    "    # compute KL div loss\n",
    "    kl_div_loss = (\n",
    "        target_logps[edit_target_mask, :].exp()\n",
    "        * (target_logps[edit_target_mask, :] - edited_target_logps[edit_target_mask, :])\n",
    "    ).sum(-1)\n",
    "\n",
    "    return kl_div_loss.mean()\n",
    "\n",
    "\n",
    "def old_kl_loss(logits, target, batch):\n",
    "    log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "        logits[:, :, :].reshape(-1, 50257),\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    def f_data_to_soft_labels(A, B, num_predictions_max=50):\n",
    "        with torch.no_grad():\n",
    "            # Compute number of nonzero elements in each row of A and B\n",
    "            lengths_A = torch.sum(A != 50256, dim=1)\n",
    "            lengths_B = torch.sum(B != 50256, dim=1)\n",
    "            # Concatenate A and B along dimension 1\n",
    "            data = concat_pad_fn(A, B)\n",
    "            logits = target(data).logits\n",
    "            predictions = torch.nn.functional.softmax(logits, dim=2)\n",
    "\n",
    "            # Create an empty tensor to store the predictions\n",
    "            shape = (len(lengths_A), num_predictions_max, 50257)\n",
    "            hold_output = torch.full(shape, torch.nan, device=logits.device)\n",
    "\n",
    "            if mode == \"left\":\n",
    "                # Extract the predictions corresponding to B, assume LEFT padding\n",
    "                for i in range(len(lengths_A)):\n",
    "                    hold_output[i, -lengths_B[i] :, :] = predictions[\n",
    "                        i, -lengths_B[i] :, :\n",
    "                    ]\n",
    "            elif mode == \"right\":\n",
    "                # Extract the predictions corresponding to B\n",
    "                for i in range(len(lengths_A)):\n",
    "                    hold_output[i, : lengths_B[i], :] = predictions[\n",
    "                        i, lengths_A[i] : lengths_A[i] + lengths_B[i], :\n",
    "                    ]\n",
    "\n",
    "            return hold_output.reshape(-1, 50257)  # returns\n",
    "\n",
    "    # Now we must compute the soft labels! This is outsourced to the user-provided function, teacher_model\n",
    "    soft_labels = f_data_to_soft_labels(\n",
    "        batch[\"tokenized_first_sentence\"],\n",
    "        batch[\"tokenized_next_50_tokens\"],\n",
    "        num_predictions_max=50,\n",
    "    )\n",
    "    # check that the mask makes sense!\n",
    "    mask = (batch[\"tokenized_next_50_tokens\"] != 50256).reshape(-1)\n",
    "\n",
    "    prediction_loss = torch.nn.functional.kl_div(\n",
    "        log_prob_predictions[mask, :], soft_labels[mask, :], reduction=\"batchmean\"\n",
    "    )\n",
    "\n",
    "    return prediction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0: new_loss_new.item()=0.2964380979537964 new_loss_old.item()=0.29643791913986206\n",
      "step=1: new_loss_new.item()=0.5079061985015869 new_loss_old.item()=0.5079073905944824\n",
      "step=2: new_loss_new.item()=0.627036988735199 new_loss_old.item()=0.6270367503166199\n",
      "step=3: new_loss_new.item()=0.45889222621917725 new_loss_old.item()=0.4588923454284668\n",
      "step=4: new_loss_new.item()=0.5651694536209106 new_loss_old.item()=0.5651690363883972\n",
      "step=5: new_loss_new.item()=0.33995258808135986 new_loss_old.item()=0.3399530053138733\n",
      "step=6: new_loss_new.item()=0.6096089482307434 new_loss_old.item()=0.6096099615097046\n",
      "step=7: new_loss_new.item()=0.40355342626571655 new_loss_old.item()=0.4035540223121643\n",
      "step=8: new_loss_new.item()=1.3793654441833496 new_loss_old.item()=1.3793659210205078\n",
      "step=9: new_loss_new.item()=0.2196909338235855 new_loss_old.item()=0.21969106793403625\n",
      "step=10: new_loss_new.item()=0.45907294750213623 new_loss_old.item()=0.4590739905834198\n",
      "step=11: new_loss_new.item()=0.3992139399051666 new_loss_old.item()=0.3992149531841278\n",
      "step=12: new_loss_new.item()=0.14386244118213654 new_loss_old.item()=0.1438627392053604\n",
      "step=13: new_loss_new.item()=0.5017461180686951 new_loss_old.item()=0.5017460584640503\n",
      "step=14: new_loss_new.item()=0.7976548671722412 new_loss_old.item()=0.7976570129394531\n",
      "step=15: new_loss_new.item()=0.22361209988594055 new_loss_old.item()=0.2236127406358719\n",
      "step=16: new_loss_new.item()=0.3746143877506256 new_loss_old.item()=0.37461528182029724\n",
      "step=17: new_loss_new.item()=0.44604629278182983 new_loss_old.item()=0.4460468888282776\n",
      "step=18: new_loss_new.item()=0.3724677264690399 new_loss_old.item()=0.3724680542945862\n",
      "step=19: new_loss_new.item()=0.10497448593378067 new_loss_old.item()=0.10497483611106873\n",
      "step=20: new_loss_new.item()=0.30089497566223145 new_loss_old.item()=0.3008958697319031\n",
      "step=21: new_loss_new.item()=0.452347993850708 new_loss_old.item()=0.45234811305999756\n",
      "step=22: new_loss_new.item()=0.5324118733406067 new_loss_old.item()=0.5324121117591858\n",
      "step=23: new_loss_new.item()=0.2046334147453308 new_loss_old.item()=0.2046334147453308\n",
      "step=24: new_loss_new.item()=0.28174272179603577 new_loss_old.item()=0.2817445695400238\n",
      "step=25: new_loss_new.item()=0.34193456172943115 new_loss_old.item()=0.34193694591522217\n",
      "step=26: new_loss_new.item()=0.7219579219818115 new_loss_old.item()=0.7219628691673279\n",
      "step=27: new_loss_new.item()=1.1025642156600952 new_loss_old.item()=1.1025643348693848\n",
      "step=28: new_loss_new.item()=0.324867844581604 new_loss_old.item()=0.3248695433139801\n",
      "step=29: new_loss_new.item()=0.2891198396682739 new_loss_old.item()=0.28911980986595154\n",
      "step=30: new_loss_new.item()=0.8003684282302856 new_loss_old.item()=0.8003683090209961\n",
      "step=31: new_loss_new.item()=0.2606561779975891 new_loss_old.item()=0.26065653562545776\n",
      "step=32: new_loss_new.item()=0.32346612215042114 new_loss_old.item()=0.32346901297569275\n",
      "step=33: new_loss_new.item()=0.4788675308227539 new_loss_old.item()=0.4788682460784912\n",
      "step=34: new_loss_new.item()=0.402981162071228 new_loss_old.item()=0.40298187732696533\n",
      "step=35: new_loss_new.item()=0.6117449402809143 new_loss_old.item()=0.6117485165596008\n",
      "step=36: new_loss_new.item()=0.6052798628807068 new_loss_old.item()=0.6052822470664978\n",
      "step=37: new_loss_new.item()=0.5041640996932983 new_loss_old.item()=0.5041641592979431\n",
      "step=38: new_loss_new.item()=0.38862472772598267 new_loss_old.item()=0.38862481713294983\n",
      "step=39: new_loss_new.item()=0.4937122166156769 new_loss_old.item()=0.49371227622032166\n",
      "step=40: new_loss_new.item()=0.3932918608188629 new_loss_old.item()=0.39329254627227783\n",
      "step=41: new_loss_new.item()=0.6908292174339294 new_loss_old.item()=0.6908282041549683\n",
      "step=42: new_loss_new.item()=0.6346045136451721 new_loss_old.item()=0.6346057653427124\n",
      "step=43: new_loss_new.item()=0.9305844902992249 new_loss_old.item()=0.9305899739265442\n",
      "step=44: new_loss_new.item()=0.38353627920150757 new_loss_old.item()=0.3835371136665344\n",
      "step=45: new_loss_new.item()=0.23242367804050446 new_loss_old.item()=0.23242583870887756\n",
      "step=46: new_loss_new.item()=0.3762072026729584 new_loss_old.item()=0.37620753049850464\n",
      "step=47: new_loss_new.item()=0.6352593898773193 new_loss_old.item()=0.6352652311325073\n",
      "step=48: new_loss_new.item()=0.38285648822784424 new_loss_old.item()=0.3828577995300293\n",
      "step=49: new_loss_new.item()=0.8489399552345276 new_loss_old.item()=0.848944902420044\n",
      "step=50: new_loss_new.item()=0.47843465209007263 new_loss_old.item()=0.4784400463104248\n",
      "step=51: new_loss_new.item()=0.45934566855430603 new_loss_old.item()=0.45934775471687317\n",
      "step=52: new_loss_new.item()=1.1721572875976562 new_loss_old.item()=1.172156572341919\n",
      "step=53: new_loss_new.item()=0.34944725036621094 new_loss_old.item()=0.34944671392440796\n",
      "step=54: new_loss_new.item()=0.14102934300899506 new_loss_old.item()=0.14102928340435028\n",
      "step=55: new_loss_new.item()=0.4408828318119049 new_loss_old.item()=0.440886527299881\n",
      "step=56: new_loss_new.item()=0.3402668237686157 new_loss_old.item()=0.3402673304080963\n",
      "step=57: new_loss_new.item()=0.39361047744750977 new_loss_old.item()=0.39361345767974854\n",
      "step=58: new_loss_new.item()=0.7089321613311768 new_loss_old.item()=0.708935022354126\n",
      "step=59: new_loss_new.item()=0.6666033267974854 new_loss_old.item()=0.6666046380996704\n",
      "step=60: new_loss_new.item()=0.2677564322948456 new_loss_old.item()=0.2677561044692993\n",
      "step=61: new_loss_new.item()=0.46285462379455566 new_loss_old.item()=0.4628561735153198\n",
      "step=62: new_loss_new.item()=0.9213454723358154 new_loss_old.item()=0.9213460683822632\n",
      "step=63: new_loss_new.item()=0.4005274474620819 new_loss_old.item()=0.4005325138568878\n",
      "step=64: new_loss_new.item()=0.38197189569473267 new_loss_old.item()=0.3819759488105774\n",
      "step=65: new_loss_new.item()=0.3050004243850708 new_loss_old.item()=0.30500108003616333\n",
      "step=66: new_loss_new.item()=0.2884044349193573 new_loss_old.item()=0.28840526938438416\n",
      "step=67: new_loss_new.item()=0.1915828436613083 new_loss_old.item()=0.19158431887626648\n",
      "step=68: new_loss_new.item()=0.8586032390594482 new_loss_old.item()=0.8586046695709229\n",
      "step=69: new_loss_new.item()=0.6079455018043518 new_loss_old.item()=0.6079561114311218\n",
      "step=70: new_loss_new.item()=0.6932147741317749 new_loss_old.item()=0.6932174563407898\n",
      "step=71: new_loss_new.item()=0.9718744158744812 new_loss_old.item()=0.9718799591064453\n",
      "step=72: new_loss_new.item()=0.3360666334629059 new_loss_old.item()=0.3360673487186432\n",
      "step=73: new_loss_new.item()=0.2766418159008026 new_loss_old.item()=0.2766454517841339\n",
      "step=74: new_loss_new.item()=0.5131390690803528 new_loss_old.item()=0.5131403207778931\n",
      "step=75: new_loss_new.item()=0.4459136724472046 new_loss_old.item()=0.44591766595840454\n",
      "step=76: new_loss_new.item()=0.9576566815376282 new_loss_old.item()=0.957657516002655\n",
      "step=77: new_loss_new.item()=0.6584188938140869 new_loss_old.item()=0.6584252119064331\n",
      "step=78: new_loss_new.item()=0.652482271194458 new_loss_old.item()=0.6524828672409058\n",
      "step=79: new_loss_new.item()=0.28894737362861633 new_loss_old.item()=0.28894880414009094\n",
      "step=80: new_loss_new.item()=0.4557555913925171 new_loss_old.item()=0.45575952529907227\n",
      "step=81: new_loss_new.item()=0.14794796705245972 new_loss_old.item()=0.14794819056987762\n",
      "step=82: new_loss_new.item()=0.6782880425453186 new_loss_old.item()=0.6782888174057007\n",
      "step=83: new_loss_new.item()=0.6127727031707764 new_loss_old.item()=0.6127731204032898\n",
      "step=84: new_loss_new.item()=0.7246251702308655 new_loss_old.item()=0.7246271371841431\n",
      "step=85: new_loss_new.item()=0.14751219749450684 new_loss_old.item()=0.14751358330249786\n",
      "step=86: new_loss_new.item()=0.759793758392334 new_loss_old.item()=0.7598002552986145\n",
      "step=87: new_loss_new.item()=0.13332617282867432 new_loss_old.item()=0.1333259493112564\n",
      "step=88: new_loss_new.item()=0.8082181811332703 new_loss_old.item()=0.8082231283187866\n",
      "step=89: new_loss_new.item()=0.4980974495410919 new_loss_old.item()=0.49810388684272766\n",
      "step=90: new_loss_new.item()=0.5546003580093384 new_loss_old.item()=0.554601788520813\n",
      "step=91: new_loss_new.item()=0.9575608372688293 new_loss_old.item()=0.957570493221283\n",
      "step=92: new_loss_new.item()=0.7106508612632751 new_loss_old.item()=0.7106623649597168\n",
      "step=93: new_loss_new.item()=0.14752398431301117 new_loss_old.item()=0.14752376079559326\n",
      "step=94: new_loss_new.item()=0.23671653866767883 new_loss_old.item()=0.23671472072601318\n",
      "step=95: new_loss_new.item()=0.5060288906097412 new_loss_old.item()=0.5060317516326904\n",
      "step=96: new_loss_new.item()=0.7891537547111511 new_loss_old.item()=0.7891546487808228\n",
      "step=97: new_loss_new.item()=0.33266833424568176 new_loss_old.item()=0.3326706290245056\n",
      "step=98: new_loss_new.item()=1.2299097776412964 new_loss_old.item()=1.2299257516860962\n",
      "step=99: new_loss_new.item()=0.22026854753494263 new_loss_old.item()=0.220276340842247\n",
      "step=100: new_loss_new.item()=0.6742221713066101 new_loss_old.item()=0.6742234230041504\n",
      "step=101: new_loss_new.item()=0.8235273957252502 new_loss_old.item()=0.8235416412353516\n",
      "step=102: new_loss_new.item()=0.6724767684936523 new_loss_old.item()=0.6724823713302612\n",
      "step=103: new_loss_new.item()=0.2870512902736664 new_loss_old.item()=0.2870505452156067\n",
      "step=104: new_loss_new.item()=0.5817691087722778 new_loss_old.item()=0.581771969795227\n",
      "step=105: new_loss_new.item()=0.16991955041885376 new_loss_old.item()=0.16992102563381195\n",
      "step=106: new_loss_new.item()=0.41540107131004333 new_loss_old.item()=0.4154064655303955\n",
      "step=107: new_loss_new.item()=0.7414883375167847 new_loss_old.item()=0.741497278213501\n",
      "step=108: new_loss_new.item()=0.6029847264289856 new_loss_old.item()=0.6029915809631348\n",
      "step=109: new_loss_new.item()=0.305118203163147 new_loss_old.item()=0.3051260709762573\n",
      "step=110: new_loss_new.item()=0.45380139350891113 new_loss_old.item()=0.45380207896232605\n",
      "step=111: new_loss_new.item()=0.3138408958911896 new_loss_old.item()=0.3138425350189209\n",
      "step=112: new_loss_new.item()=0.8104498386383057 new_loss_old.item()=0.810454249382019\n",
      "step=113: new_loss_new.item()=0.24745957553386688 new_loss_old.item()=0.2474641501903534\n",
      "step=114: new_loss_new.item()=0.25578710436820984 new_loss_old.item()=0.25579020380973816\n",
      "step=115: new_loss_new.item()=0.2553234398365021 new_loss_old.item()=0.25532427430152893\n",
      "step=116: new_loss_new.item()=0.32075193524360657 new_loss_old.item()=0.3207542598247528\n",
      "step=117: new_loss_new.item()=0.4638008773326874 new_loss_old.item()=0.4637998342514038\n",
      "step=118: new_loss_new.item()=0.7688990831375122 new_loss_old.item()=0.7689003348350525\n",
      "step=119: new_loss_new.item()=0.2565103769302368 new_loss_old.item()=0.2565120756626129\n",
      "step=120: new_loss_new.item()=0.5713669061660767 new_loss_old.item()=0.5713688135147095\n",
      "step=121: new_loss_new.item()=0.15397602319717407 new_loss_old.item()=0.15397517383098602\n",
      "step=122: new_loss_new.item()=0.3985487222671509 new_loss_old.item()=0.39855003356933594\n",
      "step=123: new_loss_new.item()=0.2006121426820755 new_loss_old.item()=0.20061442255973816\n",
      "step=124: new_loss_new.item()=0.6372370719909668 new_loss_old.item()=0.6372447609901428\n",
      "step=125: new_loss_new.item()=0.45062053203582764 new_loss_old.item()=0.4506208300590515\n",
      "step=126: new_loss_new.item()=0.5809578895568848 new_loss_old.item()=0.5809606313705444\n",
      "step=127: new_loss_new.item()=0.3842674493789673 new_loss_old.item()=0.38427454233169556\n",
      "step=128: new_loss_new.item()=0.49727892875671387 new_loss_old.item()=0.4972822964191437\n",
      "step=129: new_loss_new.item()=0.3903895616531372 new_loss_old.item()=0.3903907835483551\n",
      "step=130: new_loss_new.item()=0.20240852236747742 new_loss_old.item()=0.2024146020412445\n",
      "step=131: new_loss_new.item()=0.5011829733848572 new_loss_old.item()=0.5011909008026123\n",
      "step=132: new_loss_new.item()=0.63641357421875 new_loss_old.item()=0.6364278793334961\n",
      "step=133: new_loss_new.item()=0.46047550439834595 new_loss_old.item()=0.46048179268836975\n",
      "step=134: new_loss_new.item()=0.4124538004398346 new_loss_old.item()=0.412454754114151\n",
      "step=135: new_loss_new.item()=0.4772299826145172 new_loss_old.item()=0.47723284363746643\n",
      "step=136: new_loss_new.item()=0.19719108939170837 new_loss_old.item()=0.19719085097312927\n",
      "step=137: new_loss_new.item()=0.273036390542984 new_loss_old.item()=0.2730393409729004\n",
      "step=138: new_loss_new.item()=0.8285896182060242 new_loss_old.item()=0.8285914063453674\n",
      "step=139: new_loss_new.item()=0.6722506284713745 new_loss_old.item()=0.6722521781921387\n",
      "step=140: new_loss_new.item()=1.4543083906173706 new_loss_old.item()=1.454307198524475\n",
      "step=141: new_loss_new.item()=0.744304895401001 new_loss_old.item()=0.7443082928657532\n",
      "step=142: new_loss_new.item()=0.3850093483924866 new_loss_old.item()=0.38501667976379395\n",
      "step=143: new_loss_new.item()=0.41222429275512695 new_loss_old.item()=0.4122256338596344\n",
      "step=144: new_loss_new.item()=0.6118537783622742 new_loss_old.item()=0.6118565797805786\n",
      "step=145: new_loss_new.item()=0.86212158203125 new_loss_old.item()=0.8621270656585693\n",
      "step=146: new_loss_new.item()=1.2357432842254639 new_loss_old.item()=1.2357560396194458\n",
      "step=147: new_loss_new.item()=0.4330544173717499 new_loss_old.item()=0.4330576956272125\n",
      "step=148: new_loss_new.item()=0.280248761177063 new_loss_old.item()=0.2802485525608063\n",
      "step=149: new_loss_new.item()=0.15348778665065765 new_loss_old.item()=0.15348824858665466\n",
      "step=150: new_loss_new.item()=0.8011130690574646 new_loss_old.item()=0.801118016242981\n",
      "step=151: new_loss_new.item()=0.3014615476131439 new_loss_old.item()=0.30146297812461853\n",
      "step=152: new_loss_new.item()=0.46484842896461487 new_loss_old.item()=0.46485769748687744\n",
      "step=153: new_loss_new.item()=0.6235453486442566 new_loss_old.item()=0.6235603094100952\n",
      "step=154: new_loss_new.item()=0.5175246000289917 new_loss_old.item()=0.5175362825393677\n",
      "step=155: new_loss_new.item()=0.7646279335021973 new_loss_old.item()=0.7646427154541016\n",
      "step=156: new_loss_new.item()=0.2522055506706238 new_loss_old.item()=0.2522074580192566\n",
      "step=157: new_loss_new.item()=0.368807315826416 new_loss_old.item()=0.36880794167518616\n",
      "step=158: new_loss_new.item()=0.41811713576316833 new_loss_old.item()=0.41812482476234436\n",
      "step=159: new_loss_new.item()=0.9504914283752441 new_loss_old.item()=0.9505000114440918\n",
      "step=160: new_loss_new.item()=0.3160785138607025 new_loss_old.item()=0.31608524918556213\n",
      "step=161: new_loss_new.item()=0.41226887702941895 new_loss_old.item()=0.4122694134712219\n",
      "step=162: new_loss_new.item()=0.32617050409317017 new_loss_old.item()=0.3261762261390686\n",
      "step=163: new_loss_new.item()=0.6991429924964905 new_loss_old.item()=0.6991555094718933\n",
      "step=164: new_loss_new.item()=0.5139760971069336 new_loss_old.item()=0.5139811038970947\n",
      "step=165: new_loss_new.item()=0.6995251178741455 new_loss_old.item()=0.6995256543159485\n",
      "step=166: new_loss_new.item()=0.6008129715919495 new_loss_old.item()=0.6008147597312927\n",
      "step=167: new_loss_new.item()=0.32582923769950867 new_loss_old.item()=0.32583409547805786\n",
      "step=168: new_loss_new.item()=1.2211472988128662 new_loss_old.item()=1.2211538553237915\n",
      "step=169: new_loss_new.item()=0.5925101041793823 new_loss_old.item()=0.5925188660621643\n",
      "step=170: new_loss_new.item()=2.2165699005126953 new_loss_old.item()=2.2165699005126953\n",
      "step=171: new_loss_new.item()=0.16470861434936523 new_loss_old.item()=0.16470903158187866\n",
      "step=172: new_loss_new.item()=0.2691821753978729 new_loss_old.item()=0.2691906690597534\n",
      "step=173: new_loss_new.item()=0.29772549867630005 new_loss_old.item()=0.2977287471294403\n",
      "step=174: new_loss_new.item()=1.1092617511749268 new_loss_old.item()=1.1092720031738281\n",
      "step=175: new_loss_new.item()=1.1801097393035889 new_loss_old.item()=1.180127501487732\n",
      "step=176: new_loss_new.item()=0.9614648818969727 new_loss_old.item()=0.9614667892456055\n",
      "step=177: new_loss_new.item()=0.44060033559799194 new_loss_old.item()=0.4406081736087799\n",
      "step=178: new_loss_new.item()=0.7477636933326721 new_loss_old.item()=0.7477658987045288\n",
      "step=179: new_loss_new.item()=1.0127760171890259 new_loss_old.item()=1.0128024816513062\n",
      "step=180: new_loss_new.item()=0.28421103954315186 new_loss_old.item()=0.2842196226119995\n",
      "step=181: new_loss_new.item()=1.1234749555587769 new_loss_old.item()=1.12348473072052\n",
      "step=182: new_loss_new.item()=0.6278825402259827 new_loss_old.item()=0.6278902292251587\n",
      "step=183: new_loss_new.item()=0.24492961168289185 new_loss_old.item()=0.2449369728565216\n",
      "step=184: new_loss_new.item()=1.248621940612793 new_loss_old.item()=1.24862539768219\n",
      "step=185: new_loss_new.item()=0.6904182434082031 new_loss_old.item()=0.6904229521751404\n",
      "step=186: new_loss_new.item()=0.522933304309845 new_loss_old.item()=0.5229470133781433\n",
      "step=187: new_loss_new.item()=0.43909308314323425 new_loss_old.item()=0.43910324573516846\n",
      "step=188: new_loss_new.item()=0.2829723358154297 new_loss_old.item()=0.28297606110572815\n",
      "step=189: new_loss_new.item()=0.5096943378448486 new_loss_old.item()=0.5097023844718933\n",
      "step=190: new_loss_new.item()=0.4194679260253906 new_loss_old.item()=0.4194774925708771\n",
      "step=191: new_loss_new.item()=0.4842388927936554 new_loss_old.item()=0.48424285650253296\n",
      "step=192: new_loss_new.item()=0.47485142946243286 new_loss_old.item()=0.47486618161201477\n",
      "step=193: new_loss_new.item()=0.40708616375923157 new_loss_old.item()=0.40709388256073\n",
      "step=194: new_loss_new.item()=0.38110053539276123 new_loss_old.item()=0.38110995292663574\n",
      "step=195: new_loss_new.item()=0.42619410157203674 new_loss_old.item()=0.42621245980262756\n",
      "step=196: new_loss_new.item()=0.42113882303237915 new_loss_old.item()=0.4211384952068329\n",
      "step=197: new_loss_new.item()=0.3972214162349701 new_loss_old.item()=0.39723536372184753\n",
      "step=198: new_loss_new.item()=0.4125102758407593 new_loss_old.item()=0.41251444816589355\n",
      "step=199: new_loss_new.item()=0.4187638759613037 new_loss_old.item()=0.41876670718193054\n",
      "step=200: new_loss_new.item()=0.44442442059516907 new_loss_old.item()=0.44442784786224365\n",
      "step=201: new_loss_new.item()=0.4301450252532959 new_loss_old.item()=0.43014708161354065\n",
      "step=202: new_loss_new.item()=0.6732402443885803 new_loss_old.item()=0.6732415556907654\n",
      "step=203: new_loss_new.item()=0.34807872772216797 new_loss_old.item()=0.3480784296989441\n",
      "step=204: new_loss_new.item()=0.48503535985946655 new_loss_old.item()=0.4850446581840515\n",
      "step=205: new_loss_new.item()=0.227473646402359 new_loss_old.item()=0.22748185694217682\n",
      "step=206: new_loss_new.item()=0.27047598361968994 new_loss_old.item()=0.2704884111881256\n",
      "step=207: new_loss_new.item()=0.36617064476013184 new_loss_old.item()=0.36617526412010193\n",
      "step=208: new_loss_new.item()=0.43298080563545227 new_loss_old.item()=0.4329950213432312\n",
      "step=209: new_loss_new.item()=0.3559405207633972 new_loss_old.item()=0.35594332218170166\n",
      "step=210: new_loss_new.item()=0.28354790806770325 new_loss_old.item()=0.283558189868927\n",
      "step=211: new_loss_new.item()=0.41974037885665894 new_loss_old.item()=0.4197462499141693\n",
      "step=212: new_loss_new.item()=0.27207666635513306 new_loss_old.item()=0.27207618951797485\n",
      "step=213: new_loss_new.item()=0.25561463832855225 new_loss_old.item()=0.25561726093292236\n",
      "step=214: new_loss_new.item()=0.5347858667373657 new_loss_old.item()=0.5347961783409119\n",
      "step=215: new_loss_new.item()=0.38944655656814575 new_loss_old.item()=0.38945865631103516\n",
      "step=216: new_loss_new.item()=0.5016922354698181 new_loss_old.item()=0.5017020106315613\n",
      "step=217: new_loss_new.item()=0.8199105262756348 new_loss_old.item()=0.8199345469474792\n",
      "step=218: new_loss_new.item()=0.3014126718044281 new_loss_old.item()=0.3014242351055145\n",
      "step=219: new_loss_new.item()=0.4427197277545929 new_loss_old.item()=0.4427201449871063\n",
      "step=220: new_loss_new.item()=0.12607723474502563 new_loss_old.item()=0.12607774138450623\n",
      "step=221: new_loss_new.item()=0.4715614318847656 new_loss_old.item()=0.4715731143951416\n",
      "step=222: new_loss_new.item()=0.7829763889312744 new_loss_old.item()=0.7829812169075012\n",
      "step=223: new_loss_new.item()=0.5105621218681335 new_loss_old.item()=0.5105741620063782\n",
      "step=224: new_loss_new.item()=0.30540090799331665 new_loss_old.item()=0.30540433526039124\n",
      "step=225: new_loss_new.item()=0.3147397041320801 new_loss_old.item()=0.31475067138671875\n",
      "step=226: new_loss_new.item()=0.38647541403770447 new_loss_old.item()=0.3864838778972626\n",
      "step=227: new_loss_new.item()=0.21079674363136292 new_loss_old.item()=0.2108001559972763\n",
      "step=228: new_loss_new.item()=1.1121810674667358 new_loss_old.item()=1.1121827363967896\n",
      "step=229: new_loss_new.item()=0.6132165789604187 new_loss_old.item()=0.6132233142852783\n",
      "step=230: new_loss_new.item()=0.3898580074310303 new_loss_old.item()=0.3898758590221405\n",
      "step=231: new_loss_new.item()=0.6840649247169495 new_loss_old.item()=0.6840702891349792\n",
      "step=232: new_loss_new.item()=0.7826173305511475 new_loss_old.item()=0.7826367020606995\n",
      "step=233: new_loss_new.item()=0.5119633674621582 new_loss_old.item()=0.5119630098342896\n",
      "step=234: new_loss_new.item()=0.3818374276161194 new_loss_old.item()=0.3818540871143341\n",
      "step=235: new_loss_new.item()=0.45222175121307373 new_loss_old.item()=0.4522313177585602\n",
      "step=236: new_loss_new.item()=0.1767154335975647 new_loss_old.item()=0.1767171174287796\n",
      "step=237: new_loss_new.item()=0.5827101469039917 new_loss_old.item()=0.5827169418334961\n",
      "step=238: new_loss_new.item()=0.1291906237602234 new_loss_old.item()=0.12918999791145325\n",
      "step=239: new_loss_new.item()=0.5483024716377258 new_loss_old.item()=0.5483202934265137\n",
      "step=240: new_loss_new.item()=0.7421327233314514 new_loss_old.item()=0.7421368360519409\n",
      "step=241: new_loss_new.item()=0.6084985136985779 new_loss_old.item()=0.6085172891616821\n",
      "step=242: new_loss_new.item()=1.098060965538025 new_loss_old.item()=1.09810471534729\n",
      "step=243: new_loss_new.item()=0.40798139572143555 new_loss_old.item()=0.4079827666282654\n",
      "step=244: new_loss_new.item()=0.44495460391044617 new_loss_old.item()=0.4449772536754608\n",
      "step=245: new_loss_new.item()=0.48869022727012634 new_loss_old.item()=0.48869386315345764\n",
      "step=246: new_loss_new.item()=0.15389913320541382 new_loss_old.item()=0.1538991779088974\n",
      "step=247: new_loss_new.item()=1.305261492729187 new_loss_old.item()=1.305301308631897\n",
      "step=248: new_loss_new.item()=0.1423853039741516 new_loss_old.item()=0.14238731563091278\n",
      "step=249: new_loss_new.item()=0.2948204278945923 new_loss_old.item()=0.29483240842819214\n",
      "step=250: new_loss_new.item()=0.6125847697257996 new_loss_old.item()=0.612586259841919\n",
      "step=251: new_loss_new.item()=0.8290585279464722 new_loss_old.item()=0.8290629386901855\n",
      "step=252: new_loss_new.item()=0.7087731957435608 new_loss_old.item()=0.7087860703468323\n",
      "step=253: new_loss_new.item()=0.4432803690433502 new_loss_old.item()=0.4432966709136963\n",
      "step=254: new_loss_new.item()=0.6860182285308838 new_loss_old.item()=0.6860235333442688\n",
      "step=255: new_loss_new.item()=0.6301625967025757 new_loss_old.item()=0.6301906108856201\n",
      "step=256: new_loss_new.item()=0.5577489137649536 new_loss_old.item()=0.5577474236488342\n",
      "step=257: new_loss_new.item()=0.42999839782714844 new_loss_old.item()=0.4300021231174469\n",
      "step=258: new_loss_new.item()=0.5918720364570618 new_loss_old.item()=0.591873049736023\n",
      "step=259: new_loss_new.item()=0.8311396241188049 new_loss_old.item()=0.8311419486999512\n",
      "step=260: new_loss_new.item()=0.9151566028594971 new_loss_old.item()=0.9151544570922852\n",
      "step=261: new_loss_new.item()=0.6604055166244507 new_loss_old.item()=0.6604108810424805\n",
      "step=262: new_loss_new.item()=0.5016159415245056 new_loss_old.item()=0.5016289949417114\n",
      "step=263: new_loss_new.item()=0.6877016425132751 new_loss_old.item()=0.6877220273017883\n",
      "step=264: new_loss_new.item()=1.0042601823806763 new_loss_old.item()=1.00426185131073\n",
      "step=265: new_loss_new.item()=0.5673505067825317 new_loss_old.item()=0.5673648715019226\n",
      "step=266: new_loss_new.item()=0.3710378110408783 new_loss_old.item()=0.3710438013076782\n",
      "step=267: new_loss_new.item()=0.2571273148059845 new_loss_old.item()=0.2571312487125397\n",
      "step=268: new_loss_new.item()=0.6600583791732788 new_loss_old.item()=0.660060703754425\n",
      "step=269: new_loss_new.item()=0.3782123625278473 new_loss_old.item()=0.3782188594341278\n",
      "step=270: new_loss_new.item()=0.2865815758705139 new_loss_old.item()=0.28658345341682434\n",
      "step=271: new_loss_new.item()=0.35904890298843384 new_loss_old.item()=0.3590546250343323\n",
      "step=272: new_loss_new.item()=0.25954142212867737 new_loss_old.item()=0.25955381989479065\n",
      "step=273: new_loss_new.item()=0.3281882107257843 new_loss_old.item()=0.32818907499313354\n",
      "step=274: new_loss_new.item()=0.4203021824359894 new_loss_old.item()=0.4203155040740967\n",
      "step=275: new_loss_new.item()=0.33749961853027344 new_loss_old.item()=0.33751288056373596\n",
      "step=276: new_loss_new.item()=0.3224656879901886 new_loss_old.item()=0.3224710524082184\n",
      "step=277: new_loss_new.item()=0.5699219107627869 new_loss_old.item()=0.5699360370635986\n",
      "step=278: new_loss_new.item()=0.6403393745422363 new_loss_old.item()=0.6403400301933289\n",
      "step=279: new_loss_new.item()=0.5192180871963501 new_loss_old.item()=0.5192245841026306\n",
      "step=280: new_loss_new.item()=0.7921590209007263 new_loss_old.item()=0.7921702265739441\n",
      "step=281: new_loss_new.item()=0.7318761944770813 new_loss_old.item()=0.7318805456161499\n",
      "step=282: new_loss_new.item()=1.3510167598724365 new_loss_old.item()=1.3510210514068604\n",
      "step=283: new_loss_new.item()=1.2521377801895142 new_loss_old.item()=1.2521501779556274\n",
      "step=284: new_loss_new.item()=0.4419846832752228 new_loss_old.item()=0.4419880211353302\n",
      "step=285: new_loss_new.item()=0.5217645168304443 new_loss_old.item()=0.5217896699905396\n",
      "step=286: new_loss_new.item()=0.5816794037818909 new_loss_old.item()=0.5816821455955505\n",
      "step=287: new_loss_new.item()=0.15217483043670654 new_loss_old.item()=0.15217477083206177\n",
      "step=288: new_loss_new.item()=0.5356376767158508 new_loss_old.item()=0.5356401205062866\n",
      "step=289: new_loss_new.item()=0.6677080988883972 new_loss_old.item()=0.6677171587944031\n",
      "step=290: new_loss_new.item()=1.799452543258667 new_loss_old.item()=1.7994621992111206\n",
      "step=291: new_loss_new.item()=0.31233036518096924 new_loss_old.item()=0.3123379647731781\n",
      "step=292: new_loss_new.item()=0.19578246772289276 new_loss_old.item()=0.1957836151123047\n",
      "step=293: new_loss_new.item()=0.722332239151001 new_loss_old.item()=0.722361147403717\n",
      "step=294: new_loss_new.item()=0.36905092000961304 new_loss_old.item()=0.36906686425209045\n",
      "step=295: new_loss_new.item()=0.5889341235160828 new_loss_old.item()=0.5889355540275574\n",
      "step=296: new_loss_new.item()=0.37006640434265137 new_loss_old.item()=0.3700873851776123\n",
      "step=297: new_loss_new.item()=0.4614284336566925 new_loss_old.item()=0.4614313840866089\n",
      "step=298: new_loss_new.item()=0.370654821395874 new_loss_old.item()=0.37067174911499023\n",
      "step=299: new_loss_new.item()=0.2471374273300171 new_loss_old.item()=0.24715225398540497\n",
      "step=300: new_loss_new.item()=0.39300066232681274 new_loss_old.item()=0.3930250406265259\n",
      "step=301: new_loss_new.item()=0.201541006565094 new_loss_old.item()=0.20154069364070892\n",
      "step=302: new_loss_new.item()=0.418049156665802 new_loss_old.item()=0.4180615544319153\n",
      "step=303: new_loss_new.item()=0.43341270089149475 new_loss_old.item()=0.4334293305873871\n",
      "step=304: new_loss_new.item()=0.5793278217315674 new_loss_old.item()=0.5793381333351135\n",
      "step=305: new_loss_new.item()=0.30781400203704834 new_loss_old.item()=0.30782926082611084\n",
      "step=306: new_loss_new.item()=0.5229385495185852 new_loss_old.item()=0.522947371006012\n",
      "step=307: new_loss_new.item()=0.36713141202926636 new_loss_old.item()=0.3671347200870514\n",
      "step=308: new_loss_new.item()=0.5696192979812622 new_loss_old.item()=0.5696364045143127\n",
      "step=309: new_loss_new.item()=0.41626304388046265 new_loss_old.item()=0.41628196835517883\n",
      "step=310: new_loss_new.item()=0.3048563301563263 new_loss_old.item()=0.30487263202667236\n",
      "step=311: new_loss_new.item()=0.5000082850456238 new_loss_old.item()=0.5000142455101013\n",
      "step=312: new_loss_new.item()=0.4645216166973114 new_loss_old.item()=0.4645387828350067\n",
      "step=313: new_loss_new.item()=0.4340013265609741 new_loss_old.item()=0.43401584029197693\n",
      "step=314: new_loss_new.item()=0.2685525715351105 new_loss_old.item()=0.26856690645217896\n",
      "step=315: new_loss_new.item()=0.22817568480968475 new_loss_old.item()=0.2281806468963623\n",
      "step=316: new_loss_new.item()=0.5360169410705566 new_loss_old.item()=0.5360327363014221\n",
      "step=317: new_loss_new.item()=0.24038293957710266 new_loss_old.item()=0.24038738012313843\n",
      "step=318: new_loss_new.item()=0.3356257975101471 new_loss_old.item()=0.33562761545181274\n",
      "step=319: new_loss_new.item()=0.6162865161895752 new_loss_old.item()=0.6162877082824707\n",
      "step=320: new_loss_new.item()=0.5378514528274536 new_loss_old.item()=0.5378686189651489\n",
      "step=321: new_loss_new.item()=0.3761226534843445 new_loss_old.item()=0.37613844871520996\n",
      "step=322: new_loss_new.item()=0.14823570847511292 new_loss_old.item()=0.1482432633638382\n",
      "step=323: new_loss_new.item()=0.22862564027309418 new_loss_old.item()=0.2286406308412552\n",
      "step=324: new_loss_new.item()=0.1912677139043808 new_loss_old.item()=0.1912684589624405\n",
      "step=325: new_loss_new.item()=0.56399005651474 new_loss_old.item()=0.563992977142334\n",
      "step=326: new_loss_new.item()=0.5896304845809937 new_loss_old.item()=0.5896614193916321\n",
      "step=327: new_loss_new.item()=1.0934890508651733 new_loss_old.item()=1.0935394763946533\n",
      "step=328: new_loss_new.item()=0.6485676169395447 new_loss_old.item()=0.648585855960846\n",
      "step=329: new_loss_new.item()=0.06349020451307297 new_loss_old.item()=0.06349017471075058\n",
      "step=330: new_loss_new.item()=0.18861865997314453 new_loss_old.item()=0.1886301189661026\n",
      "step=331: new_loss_new.item()=0.2009708732366562 new_loss_old.item()=0.20097525417804718\n",
      "step=332: new_loss_new.item()=0.7189628481864929 new_loss_old.item()=0.7189666628837585\n",
      "step=333: new_loss_new.item()=0.6889052391052246 new_loss_old.item()=0.6889259219169617\n",
      "step=334: new_loss_new.item()=0.5963770151138306 new_loss_old.item()=0.5963801145553589\n",
      "step=335: new_loss_new.item()=0.345378577709198 new_loss_old.item()=0.3453828990459442\n",
      "step=336: new_loss_new.item()=0.209402397274971 new_loss_old.item()=0.20940706133842468\n",
      "step=337: new_loss_new.item()=0.7187771201133728 new_loss_old.item()=0.718779981136322\n",
      "step=338: new_loss_new.item()=0.7980740666389465 new_loss_old.item()=0.7981176376342773\n",
      "step=339: new_loss_new.item()=0.4858247935771942 new_loss_old.item()=0.48584359884262085\n",
      "step=340: new_loss_new.item()=0.8643574118614197 new_loss_old.item()=0.8644075989723206\n",
      "step=341: new_loss_new.item()=0.4940745532512665 new_loss_old.item()=0.4941025674343109\n",
      "step=342: new_loss_new.item()=0.6427510976791382 new_loss_old.item()=0.6427536010742188\n",
      "step=343: new_loss_new.item()=0.5413476228713989 new_loss_old.item()=0.5413661599159241\n",
      "step=344: new_loss_new.item()=0.20681703090667725 new_loss_old.item()=0.20681819319725037\n",
      "step=345: new_loss_new.item()=0.5294052958488464 new_loss_old.item()=0.5294129252433777\n",
      "step=346: new_loss_new.item()=0.6220506429672241 new_loss_old.item()=0.6220625638961792\n",
      "step=347: new_loss_new.item()=0.4552823007106781 new_loss_old.item()=0.4553174078464508\n",
      "step=348: new_loss_new.item()=0.954045832157135 new_loss_old.item()=0.9541010856628418\n",
      "step=349: new_loss_new.item()=0.4539720118045807 new_loss_old.item()=0.45399755239486694\n",
      "step=350: new_loss_new.item()=0.4062858819961548 new_loss_old.item()=0.4063079357147217\n",
      "step=351: new_loss_new.item()=0.14218483865261078 new_loss_old.item()=0.14217941462993622\n",
      "step=352: new_loss_new.item()=0.7100802063941956 new_loss_old.item()=0.7101068496704102\n",
      "step=353: new_loss_new.item()=1.4332858324050903 new_loss_old.item()=1.4333441257476807\n",
      "step=354: new_loss_new.item()=0.5853063464164734 new_loss_old.item()=0.5853317379951477\n",
      "step=355: new_loss_new.item()=0.8504643440246582 new_loss_old.item()=0.8504669070243835\n",
      "step=356: new_loss_new.item()=0.7121840715408325 new_loss_old.item()=0.712188184261322\n",
      "step=357: new_loss_new.item()=0.4550950825214386 new_loss_old.item()=0.4551261067390442\n",
      "step=358: new_loss_new.item()=0.9487344622612 new_loss_old.item()=0.9487532377243042\n",
      "step=359: new_loss_new.item()=0.7067315578460693 new_loss_old.item()=0.7067376375198364\n",
      "step=360: new_loss_new.item()=0.22239530086517334 new_loss_old.item()=0.22239486873149872\n",
      "step=361: new_loss_new.item()=0.21125666797161102 new_loss_old.item()=0.21126198768615723\n",
      "step=362: new_loss_new.item()=0.25595104694366455 new_loss_old.item()=0.2559693455696106\n",
      "step=363: new_loss_new.item()=0.20278307795524597 new_loss_old.item()=0.20279774069786072\n",
      "step=364: new_loss_new.item()=0.2949901819229126 new_loss_old.item()=0.2950059473514557\n",
      "step=365: new_loss_new.item()=0.2009889781475067 new_loss_old.item()=0.2009906768798828\n",
      "step=366: new_loss_new.item()=0.4626309275627136 new_loss_old.item()=0.46265390515327454\n",
      "step=367: new_loss_new.item()=0.8153710961341858 new_loss_old.item()=0.8153610229492188\n",
      "step=368: new_loss_new.item()=0.8418309092521667 new_loss_old.item()=0.8418471217155457\n",
      "step=369: new_loss_new.item()=0.5092326402664185 new_loss_old.item()=0.509238600730896\n",
      "step=370: new_loss_new.item()=0.45320677757263184 new_loss_old.item()=0.4532260298728943\n",
      "step=371: new_loss_new.item()=0.3786463737487793 new_loss_old.item()=0.37865203619003296\n",
      "step=372: new_loss_new.item()=0.32994264364242554 new_loss_old.item()=0.3299577236175537\n",
      "step=373: new_loss_new.item()=0.5547206401824951 new_loss_old.item()=0.5547267198562622\n",
      "step=374: new_loss_new.item()=0.38688552379608154 new_loss_old.item()=0.386893093585968\n",
      "step=375: new_loss_new.item()=0.2907349467277527 new_loss_old.item()=0.2907513678073883\n",
      "step=376: new_loss_new.item()=0.28647956252098083 new_loss_old.item()=0.2865063548088074\n",
      "step=377: new_loss_new.item()=0.6580144166946411 new_loss_old.item()=0.6580160856246948\n",
      "step=378: new_loss_new.item()=0.5463065505027771 new_loss_old.item()=0.5463324785232544\n",
      "step=379: new_loss_new.item()=0.523842453956604 new_loss_old.item()=0.5238476991653442\n",
      "step=380: new_loss_new.item()=0.43165290355682373 new_loss_old.item()=0.4316500127315521\n",
      "step=381: new_loss_new.item()=0.34575507044792175 new_loss_old.item()=0.34575995802879333\n",
      "step=382: new_loss_new.item()=0.6863683462142944 new_loss_old.item()=0.6864054203033447\n",
      "step=383: new_loss_new.item()=1.087841510772705 new_loss_old.item()=1.0878444910049438\n",
      "step=384: new_loss_new.item()=0.42164626717567444 new_loss_old.item()=0.4216539263725281\n",
      "step=385: new_loss_new.item()=0.5695433616638184 new_loss_old.item()=0.5695673227310181\n",
      "step=386: new_loss_new.item()=0.9445365071296692 new_loss_old.item()=0.9445411562919617\n",
      "step=387: new_loss_new.item()=0.37780284881591797 new_loss_old.item()=0.377821683883667\n",
      "step=388: new_loss_new.item()=0.7004077434539795 new_loss_old.item()=0.7004244327545166\n",
      "step=389: new_loss_new.item()=0.32921910285949707 new_loss_old.item()=0.3292360305786133\n",
      "step=390: new_loss_new.item()=0.6930800080299377 new_loss_old.item()=0.693106472492218\n",
      "step=391: new_loss_new.item()=0.5333333611488342 new_loss_old.item()=0.5333536267280579\n",
      "step=392: new_loss_new.item()=0.8503891229629517 new_loss_old.item()=0.8503952026367188\n",
      "step=393: new_loss_new.item()=1.3173216581344604 new_loss_old.item()=1.3173842430114746\n",
      "step=394: new_loss_new.item()=0.6858919858932495 new_loss_old.item()=0.6859104037284851\n",
      "step=395: new_loss_new.item()=0.30863243341445923 new_loss_old.item()=0.3086514472961426\n",
      "step=396: new_loss_new.item()=0.7137964963912964 new_loss_old.item()=0.7138004302978516\n",
      "step=397: new_loss_new.item()=0.3317747116088867 new_loss_old.item()=0.3317788243293762\n",
      "step=398: new_loss_new.item()=0.5177338123321533 new_loss_old.item()=0.51773601770401\n",
      "step=399: new_loss_new.item()=0.33877983689308167 new_loss_old.item()=0.33878687024116516\n",
      "step=400: new_loss_new.item()=0.8106158971786499 new_loss_old.item()=0.8106206655502319\n",
      "step=401: new_loss_new.item()=0.3812171518802643 new_loss_old.item()=0.38121896982192993\n",
      "step=402: new_loss_new.item()=0.20314116775989532 new_loss_old.item()=0.2031489461660385\n",
      "step=403: new_loss_new.item()=0.2059638351202011 new_loss_old.item()=0.20598064363002777\n",
      "step=404: new_loss_new.item()=0.1663411408662796 new_loss_old.item()=0.1663416624069214\n",
      "step=405: new_loss_new.item()=0.8505623936653137 new_loss_old.item()=0.8505656123161316\n",
      "step=406: new_loss_new.item()=0.5586792826652527 new_loss_old.item()=0.5586925148963928\n",
      "step=407: new_loss_new.item()=0.662868320941925 new_loss_old.item()=0.6628736853599548\n",
      "step=408: new_loss_new.item()=0.17898838222026825 new_loss_old.item()=0.17899185419082642\n",
      "step=409: new_loss_new.item()=0.2637579143047333 new_loss_old.item()=0.26377856731414795\n",
      "step=410: new_loss_new.item()=0.679540753364563 new_loss_old.item()=0.6795961856842041\n",
      "step=411: new_loss_new.item()=0.6441484093666077 new_loss_old.item()=0.6441537141799927\n",
      "step=412: new_loss_new.item()=0.614238977432251 new_loss_old.item()=0.6142900586128235\n",
      "step=413: new_loss_new.item()=0.4494134485721588 new_loss_old.item()=0.44942706823349\n",
      "step=414: new_loss_new.item()=0.6163948178291321 new_loss_old.item()=0.6164148449897766\n",
      "step=415: new_loss_new.item()=0.3551216125488281 new_loss_old.item()=0.3551200330257416\n",
      "step=416: new_loss_new.item()=0.2057616412639618 new_loss_old.item()=0.20578081905841827\n",
      "step=417: new_loss_new.item()=0.9521270394325256 new_loss_old.item()=0.9521362781524658\n",
      "step=418: new_loss_new.item()=0.5585529208183289 new_loss_old.item()=0.5585488080978394\n",
      "step=419: new_loss_new.item()=0.5070878267288208 new_loss_old.item()=0.5071086287498474\n",
      "step=420: new_loss_new.item()=1.0485563278198242 new_loss_old.item()=1.0486146211624146\n",
      "step=421: new_loss_new.item()=0.14492614567279816 new_loss_old.item()=0.14492928981781006\n",
      "step=422: new_loss_new.item()=0.3963417410850525 new_loss_old.item()=0.39636358618736267\n",
      "step=423: new_loss_new.item()=0.3571957051753998 new_loss_old.item()=0.35720205307006836\n",
      "step=424: new_loss_new.item()=0.48571935296058655 new_loss_old.item()=0.48572641611099243\n",
      "step=425: new_loss_new.item()=0.48065587878227234 new_loss_old.item()=0.480684369802475\n",
      "step=426: new_loss_new.item()=0.7689186930656433 new_loss_old.item()=0.768924355506897\n",
      "step=427: new_loss_new.item()=0.3749147951602936 new_loss_old.item()=0.37491920590400696\n",
      "step=428: new_loss_new.item()=0.4661684036254883 new_loss_old.item()=0.46619951725006104\n",
      "step=429: new_loss_new.item()=0.476954847574234 new_loss_old.item()=0.4769589602947235\n",
      "step=430: new_loss_new.item()=0.19264912605285645 new_loss_old.item()=0.1926470547914505\n",
      "step=431: new_loss_new.item()=0.35756200551986694 new_loss_old.item()=0.35756170749664307\n",
      "step=432: new_loss_new.item()=0.3851296603679657 new_loss_old.item()=0.38514527678489685\n",
      "step=433: new_loss_new.item()=0.5953525900840759 new_loss_old.item()=0.5954068899154663\n",
      "step=434: new_loss_new.item()=0.7686579823493958 new_loss_old.item()=0.7686681151390076\n",
      "step=435: new_loss_new.item()=0.40558531880378723 new_loss_old.item()=0.4056089222431183\n",
      "step=436: new_loss_new.item()=0.27285057306289673 new_loss_old.item()=0.27286484837532043\n",
      "step=437: new_loss_new.item()=0.6578808426856995 new_loss_old.item()=0.657889187335968\n",
      "step=438: new_loss_new.item()=1.2558410167694092 new_loss_old.item()=1.2558975219726562\n",
      "step=439: new_loss_new.item()=0.6133105754852295 new_loss_old.item()=0.6133228540420532\n",
      "step=440: new_loss_new.item()=0.44159987568855286 new_loss_old.item()=0.44162076711654663\n",
      "step=441: new_loss_new.item()=1.0391064882278442 new_loss_old.item()=1.0391175746917725\n",
      "step=442: new_loss_new.item()=0.30219101905822754 new_loss_old.item()=0.30219605565071106\n",
      "step=443: new_loss_new.item()=0.8768388032913208 new_loss_old.item()=0.8768453598022461\n",
      "step=444: new_loss_new.item()=0.7715010046958923 new_loss_old.item()=0.771539032459259\n",
      "step=445: new_loss_new.item()=0.40191206336021423 new_loss_old.item()=0.40193480253219604\n",
      "step=446: new_loss_new.item()=0.9383724331855774 new_loss_old.item()=0.9383853673934937\n",
      "step=447: new_loss_new.item()=0.3949001729488373 new_loss_old.item()=0.3949063718318939\n",
      "step=448: new_loss_new.item()=0.573086678981781 new_loss_old.item()=0.5731057524681091\n",
      "step=449: new_loss_new.item()=0.5846347808837891 new_loss_old.item()=0.5846443772315979\n",
      "step=450: new_loss_new.item()=0.8295929431915283 new_loss_old.item()=0.8295958042144775\n",
      "step=451: new_loss_new.item()=0.33897051215171814 new_loss_old.item()=0.33899402618408203\n",
      "step=452: new_loss_new.item()=2.235625743865967 new_loss_old.item()=2.235706329345703\n",
      "step=453: new_loss_new.item()=0.2756567597389221 new_loss_old.item()=0.27566176652908325\n",
      "step=454: new_loss_new.item()=0.28142431378364563 new_loss_old.item()=0.281445175409317\n",
      "step=455: new_loss_new.item()=0.18741509318351746 new_loss_old.item()=0.18743209540843964\n",
      "step=456: new_loss_new.item()=0.23630554974079132 new_loss_old.item()=0.2363305389881134\n",
      "step=457: new_loss_new.item()=0.22495338320732117 new_loss_old.item()=0.22495250403881073\n",
      "step=458: new_loss_new.item()=0.6784005761146545 new_loss_old.item()=0.6784421801567078\n",
      "step=459: new_loss_new.item()=0.3910810053348541 new_loss_old.item()=0.3911002278327942\n",
      "step=460: new_loss_new.item()=1.2787386178970337 new_loss_old.item()=1.2788161039352417\n",
      "step=461: new_loss_new.item()=0.8171220421791077 new_loss_old.item()=0.817157506942749\n",
      "step=462: new_loss_new.item()=0.29226428270339966 new_loss_old.item()=0.2922884225845337\n",
      "step=463: new_loss_new.item()=0.7080503702163696 new_loss_old.item()=0.7080947756767273\n",
      "step=464: new_loss_new.item()=1.4935939311981201 new_loss_old.item()=1.493666172027588\n",
      "step=465: new_loss_new.item()=0.13081058859825134 new_loss_old.item()=0.1308123767375946\n",
      "step=466: new_loss_new.item()=0.32608920335769653 new_loss_old.item()=0.32608962059020996\n",
      "step=467: new_loss_new.item()=0.24902792274951935 new_loss_old.item()=0.24903400242328644\n",
      "step=468: new_loss_new.item()=0.8240044116973877 new_loss_old.item()=0.8240591883659363\n",
      "step=469: new_loss_new.item()=0.29599639773368835 new_loss_old.item()=0.29600340127944946\n",
      "step=470: new_loss_new.item()=0.16768021881580353 new_loss_old.item()=0.16769704222679138\n",
      "step=471: new_loss_new.item()=0.6166165471076965 new_loss_old.item()=0.6166265606880188\n",
      "step=472: new_loss_new.item()=1.6492335796356201 new_loss_old.item()=1.6492396593093872\n",
      "step=473: new_loss_new.item()=1.1380287408828735 new_loss_old.item()=1.1380524635314941\n",
      "step=474: new_loss_new.item()=0.29655787348747253 new_loss_old.item()=0.29656457901000977\n",
      "step=475: new_loss_new.item()=0.17490266263484955 new_loss_old.item()=0.17491759359836578\n",
      "step=476: new_loss_new.item()=0.49388638138771057 new_loss_old.item()=0.4938966631889343\n",
      "step=477: new_loss_new.item()=0.6556530594825745 new_loss_old.item()=0.6556946635246277\n",
      "step=478: new_loss_new.item()=0.3944586217403412 new_loss_old.item()=0.39446309208869934\n",
      "step=479: new_loss_new.item()=1.6592042446136475 new_loss_old.item()=1.659244418144226\n",
      "step=480: new_loss_new.item()=0.5841960906982422 new_loss_old.item()=0.5842089653015137\n",
      "step=481: new_loss_new.item()=0.24025797843933105 new_loss_old.item()=0.2402636706829071\n",
      "step=482: new_loss_new.item()=0.6036825180053711 new_loss_old.item()=0.6036852598190308\n",
      "step=483: new_loss_new.item()=0.3973049223423004 new_loss_old.item()=0.3973120152950287\n",
      "step=484: new_loss_new.item()=0.4572594165802002 new_loss_old.item()=0.4573100805282593\n",
      "step=485: new_loss_new.item()=0.16477365791797638 new_loss_old.item()=0.1647735983133316\n",
      "step=486: new_loss_new.item()=0.47616347670555115 new_loss_old.item()=0.4761725664138794\n",
      "step=487: new_loss_new.item()=0.5767229795455933 new_loss_old.item()=0.576741635799408\n",
      "step=488: new_loss_new.item()=0.06458599120378494 new_loss_old.item()=0.06458576023578644\n",
      "step=489: new_loss_new.item()=0.7929466962814331 new_loss_old.item()=0.7929559350013733\n",
      "step=490: new_loss_new.item()=0.8316027522087097 new_loss_old.item()=0.8316108584403992\n",
      "step=491: new_loss_new.item()=0.37495502829551697 new_loss_old.item()=0.374958872795105\n",
      "step=492: new_loss_new.item()=0.26347771286964417 new_loss_old.item()=0.2635003924369812\n",
      "step=493: new_loss_new.item()=0.10260796546936035 new_loss_old.item()=0.10261251777410507\n",
      "step=494: new_loss_new.item()=0.2672320008277893 new_loss_old.item()=0.26725852489471436\n",
      "step=495: new_loss_new.item()=0.13426604866981506 new_loss_old.item()=0.13426616787910461\n",
      "step=496: new_loss_new.item()=0.2651253342628479 new_loss_old.item()=0.2651485204696655\n",
      "step=497: new_loss_new.item()=1.24997079372406 new_loss_old.item()=1.2499890327453613\n",
      "step=498: new_loss_new.item()=0.370853990316391 new_loss_old.item()=0.3708614706993103\n",
      "step=499: new_loss_new.item()=0.32811957597732544 new_loss_old.item()=0.32811546325683594\n",
      "step=500: new_loss_new.item()=1.2018078565597534 new_loss_old.item()=1.2018210887908936\n",
      "step=501: new_loss_new.item()=0.4647013247013092 new_loss_old.item()=0.4647081792354584\n",
      "step=502: new_loss_new.item()=0.3648432791233063 new_loss_old.item()=0.3648470938205719\n",
      "step=503: new_loss_new.item()=0.4546664357185364 new_loss_old.item()=0.4546860456466675\n",
      "step=504: new_loss_new.item()=0.4623444974422455 new_loss_old.item()=0.46234869956970215\n",
      "step=505: new_loss_new.item()=0.370113343000412 new_loss_old.item()=0.37011852860450745\n",
      "step=506: new_loss_new.item()=0.67116379737854 new_loss_old.item()=0.6711891293525696\n",
      "step=507: new_loss_new.item()=0.19945010542869568 new_loss_old.item()=0.19944708049297333\n",
      "step=508: new_loss_new.item()=0.7068361639976501 new_loss_old.item()=0.7068468928337097\n",
      "step=509: new_loss_new.item()=0.5696480870246887 new_loss_old.item()=0.5696555972099304\n",
      "step=510: new_loss_new.item()=1.225926160812378 new_loss_old.item()=1.2259818315505981\n",
      "step=511: new_loss_new.item()=0.21762065589427948 new_loss_old.item()=0.2176215648651123\n",
      "step=512: new_loss_new.item()=0.2755478322505951 new_loss_old.item()=0.2755536139011383\n",
      "step=513: new_loss_new.item()=0.40755560994148254 new_loss_old.item()=0.4075760543346405\n",
      "step=514: new_loss_new.item()=0.5747512578964233 new_loss_old.item()=0.5747974514961243\n",
      "step=515: new_loss_new.item()=0.4509989321231842 new_loss_old.item()=0.4510430693626404\n",
      "step=516: new_loss_new.item()=0.3391478955745697 new_loss_old.item()=0.339154988527298\n",
      "step=517: new_loss_new.item()=0.1806461066007614 new_loss_old.item()=0.1806495189666748\n",
      "step=518: new_loss_new.item()=0.8580154180526733 new_loss_old.item()=0.858016848564148\n",
      "step=519: new_loss_new.item()=0.470673531293869 new_loss_old.item()=0.47069671750068665\n",
      "step=520: new_loss_new.item()=0.47671225666999817 new_loss_old.item()=0.47671470046043396\n",
      "step=521: new_loss_new.item()=0.06647013872861862 new_loss_old.item()=0.06646686047315598\n",
      "step=522: new_loss_new.item()=0.15649528801441193 new_loss_old.item()=0.15649600327014923\n",
      "step=523: new_loss_new.item()=0.6106861233711243 new_loss_old.item()=0.6106946468353271\n",
      "step=524: new_loss_new.item()=0.4367460310459137 new_loss_old.item()=0.4367755949497223\n",
      "step=525: new_loss_new.item()=0.4071098864078522 new_loss_old.item()=0.4071114659309387\n",
      "step=526: new_loss_new.item()=0.7827773690223694 new_loss_old.item()=0.7827889323234558\n",
      "step=527: new_loss_new.item()=0.6083592176437378 new_loss_old.item()=0.6083688139915466\n",
      "step=528: new_loss_new.item()=0.17036043107509613 new_loss_old.item()=0.1703682690858841\n",
      "step=529: new_loss_new.item()=1.291548490524292 new_loss_old.item()=1.2915500402450562\n",
      "step=530: new_loss_new.item()=0.39449506998062134 new_loss_old.item()=0.3944997191429138\n",
      "step=531: new_loss_new.item()=0.2715861201286316 new_loss_old.item()=0.27161023020744324\n",
      "step=532: new_loss_new.item()=0.7912759184837341 new_loss_old.item()=0.7912951707839966\n",
      "step=533: new_loss_new.item()=0.2732585072517395 new_loss_old.item()=0.2732824683189392\n",
      "step=534: new_loss_new.item()=0.20466142892837524 new_loss_old.item()=0.20468275249004364\n",
      "step=535: new_loss_new.item()=0.6035215854644775 new_loss_old.item()=0.6035472750663757\n",
      "step=536: new_loss_new.item()=0.3526350259780884 new_loss_old.item()=0.3526364862918854\n",
      "step=537: new_loss_new.item()=0.507762610912323 new_loss_old.item()=0.5077677965164185\n",
      "step=538: new_loss_new.item()=0.40374574065208435 new_loss_old.item()=0.4037742614746094\n",
      "step=539: new_loss_new.item()=0.6242372393608093 new_loss_old.item()=0.6242495775222778\n",
      "step=540: new_loss_new.item()=0.5189119577407837 new_loss_old.item()=0.5189603567123413\n",
      "step=541: new_loss_new.item()=1.1504981517791748 new_loss_old.item()=1.1505348682403564\n",
      "step=542: new_loss_new.item()=0.6358569264411926 new_loss_old.item()=0.6358596086502075\n",
      "step=543: new_loss_new.item()=0.26993250846862793 new_loss_old.item()=0.26993757486343384\n",
      "step=544: new_loss_new.item()=0.37267813086509705 new_loss_old.item()=0.37267813086509705\n",
      "step=545: new_loss_new.item()=0.7853748202323914 new_loss_old.item()=0.785403847694397\n",
      "step=546: new_loss_new.item()=0.6425257921218872 new_loss_old.item()=0.6425252556800842\n",
      "step=547: new_loss_new.item()=0.3501557409763336 new_loss_old.item()=0.350179523229599\n",
      "step=548: new_loss_new.item()=0.45942068099975586 new_loss_old.item()=0.45945361256599426\n",
      "step=549: new_loss_new.item()=0.8979873061180115 new_loss_old.item()=0.8979942798614502\n",
      "step=550: new_loss_new.item()=0.6591407060623169 new_loss_old.item()=0.6591769456863403\n",
      "step=551: new_loss_new.item()=0.41181910037994385 new_loss_old.item()=0.4118451774120331\n",
      "step=552: new_loss_new.item()=0.19657190144062042 new_loss_old.item()=0.19657431542873383\n",
      "step=553: new_loss_new.item()=0.8050149083137512 new_loss_old.item()=0.8050425052642822\n",
      "step=554: new_loss_new.item()=0.3635416030883789 new_loss_old.item()=0.3635624647140503\n",
      "step=555: new_loss_new.item()=0.33424556255340576 new_loss_old.item()=0.3342675268650055\n",
      "step=556: new_loss_new.item()=0.4654983580112457 new_loss_old.item()=0.4655264914035797\n",
      "step=557: new_loss_new.item()=0.47805947065353394 new_loss_old.item()=0.4780884087085724\n",
      "step=558: new_loss_new.item()=0.7055497169494629 new_loss_old.item()=0.7055841684341431\n",
      "step=559: new_loss_new.item()=0.22324325144290924 new_loss_old.item()=0.2232619673013687\n",
      "step=560: new_loss_new.item()=0.41085559129714966 new_loss_old.item()=0.41090017557144165\n",
      "step=561: new_loss_new.item()=0.3411146104335785 new_loss_old.item()=0.3411402404308319\n",
      "step=562: new_loss_new.item()=0.5318235754966736 new_loss_old.item()=0.5318365693092346\n",
      "step=563: new_loss_new.item()=0.261897474527359 new_loss_old.item()=0.2619035542011261\n",
      "step=564: new_loss_new.item()=0.6912462115287781 new_loss_old.item()=0.6912918090820312\n",
      "step=565: new_loss_new.item()=0.3841537535190582 new_loss_old.item()=0.38416212797164917\n",
      "step=566: new_loss_new.item()=0.5990403294563293 new_loss_old.item()=0.5990902185440063\n",
      "step=567: new_loss_new.item()=0.3356510102748871 new_loss_old.item()=0.3356684446334839\n",
      "step=568: new_loss_new.item()=0.6038441061973572 new_loss_old.item()=0.603886604309082\n",
      "step=569: new_loss_new.item()=0.427615761756897 new_loss_old.item()=0.427621990442276\n",
      "step=570: new_loss_new.item()=0.55023193359375 new_loss_old.item()=0.5502595901489258\n",
      "step=571: new_loss_new.item()=0.3759477138519287 new_loss_old.item()=0.37597280740737915\n",
      "step=572: new_loss_new.item()=0.5836468935012817 new_loss_old.item()=0.5836417078971863\n",
      "step=573: new_loss_new.item()=0.5926077961921692 new_loss_old.item()=0.5926299095153809\n",
      "step=574: new_loss_new.item()=0.18538261950016022 new_loss_old.item()=0.18538159132003784\n",
      "step=575: new_loss_new.item()=0.7522252798080444 new_loss_old.item()=0.7522372603416443\n",
      "step=576: new_loss_new.item()=1.0913108587265015 new_loss_old.item()=1.0913960933685303\n",
      "step=577: new_loss_new.item()=0.4920710325241089 new_loss_old.item()=0.49208641052246094\n",
      "step=578: new_loss_new.item()=0.4737175703048706 new_loss_old.item()=0.4737263023853302\n",
      "step=579: new_loss_new.item()=0.3466196656227112 new_loss_old.item()=0.34664949774742126\n",
      "step=580: new_loss_new.item()=0.2822512686252594 new_loss_old.item()=0.28227606415748596\n",
      "step=581: new_loss_new.item()=0.4171968698501587 new_loss_old.item()=0.41723039746284485\n",
      "step=582: new_loss_new.item()=1.1869885921478271 new_loss_old.item()=1.1870023012161255\n",
      "step=583: new_loss_new.item()=0.8416268825531006 new_loss_old.item()=0.8416659832000732\n",
      "step=584: new_loss_new.item()=0.5832895636558533 new_loss_old.item()=0.5833012461662292\n",
      "step=585: new_loss_new.item()=0.6813396215438843 new_loss_old.item()=0.6813545227050781\n",
      "step=586: new_loss_new.item()=0.3888453543186188 new_loss_old.item()=0.3888565003871918\n",
      "step=587: new_loss_new.item()=0.6635434627532959 new_loss_old.item()=0.6635648012161255\n",
      "step=588: new_loss_new.item()=0.4398344159126282 new_loss_old.item()=0.43986162543296814\n",
      "step=589: new_loss_new.item()=0.34894850850105286 new_loss_old.item()=0.348977655172348\n",
      "step=590: new_loss_new.item()=0.24396975338459015 new_loss_old.item()=0.24397145211696625\n",
      "step=591: new_loss_new.item()=0.554900050163269 new_loss_old.item()=0.5549426674842834\n",
      "step=592: new_loss_new.item()=0.7373936772346497 new_loss_old.item()=0.7374105453491211\n",
      "step=593: new_loss_new.item()=0.5850955843925476 new_loss_old.item()=0.5850996971130371\n",
      "step=594: new_loss_new.item()=0.39670494198799133 new_loss_old.item()=0.3967059552669525\n",
      "step=595: new_loss_new.item()=0.391936331987381 new_loss_old.item()=0.3919624388217926\n",
      "step=596: new_loss_new.item()=0.5284616947174072 new_loss_old.item()=0.5284764170646667\n",
      "step=597: new_loss_new.item()=0.4358319342136383 new_loss_old.item()=0.4358600676059723\n",
      "step=598: new_loss_new.item()=0.15991677343845367 new_loss_old.item()=0.1599235087633133\n",
      "step=599: new_loss_new.item()=0.5694145560264587 new_loss_old.item()=0.5694235563278198\n",
      "step=600: new_loss_new.item()=0.7078801989555359 new_loss_old.item()=0.7078961133956909\n",
      "step=601: new_loss_new.item()=0.5037153959274292 new_loss_old.item()=0.5037233829498291\n",
      "step=602: new_loss_new.item()=0.8479360938072205 new_loss_old.item()=0.8479639887809753\n",
      "step=603: new_loss_new.item()=0.3685151934623718 new_loss_old.item()=0.36854088306427\n",
      "step=604: new_loss_new.item()=0.4079064428806305 new_loss_old.item()=0.4079377353191376\n",
      "step=605: new_loss_new.item()=1.1495447158813477 new_loss_old.item()=1.1496440172195435\n",
      "step=606: new_loss_new.item()=0.7296034097671509 new_loss_old.item()=0.7296143174171448\n",
      "step=607: new_loss_new.item()=0.11472778767347336 new_loss_old.item()=0.11472951620817184\n",
      "step=608: new_loss_new.item()=0.36226722598075867 new_loss_old.item()=0.3622795641422272\n",
      "step=609: new_loss_new.item()=0.3945485055446625 new_loss_old.item()=0.39454999566078186\n",
      "step=610: new_loss_new.item()=0.4211563467979431 new_loss_old.item()=0.4211882948875427\n",
      "step=611: new_loss_new.item()=0.23290391266345978 new_loss_old.item()=0.2329052835702896\n",
      "step=612: new_loss_new.item()=0.31067949533462524 new_loss_old.item()=0.3106946647167206\n",
      "step=613: new_loss_new.item()=0.29458174109458923 new_loss_old.item()=0.2945934534072876\n",
      "step=614: new_loss_new.item()=0.35947534441947937 new_loss_old.item()=0.3595138490200043\n",
      "step=615: new_loss_new.item()=0.9214402437210083 new_loss_old.item()=0.9214907288551331\n",
      "step=616: new_loss_new.item()=0.4867488741874695 new_loss_old.item()=0.48675721883773804\n",
      "step=617: new_loss_new.item()=0.5121168494224548 new_loss_old.item()=0.5121184587478638\n",
      "step=618: new_loss_new.item()=0.3833288252353668 new_loss_old.item()=0.3833576440811157\n",
      "step=619: new_loss_new.item()=0.08774882555007935 new_loss_old.item()=0.08774814009666443\n",
      "step=620: new_loss_new.item()=0.3626452684402466 new_loss_old.item()=0.36265137791633606\n",
      "step=621: new_loss_new.item()=0.3562158942222595 new_loss_old.item()=0.3562192916870117\n",
      "step=622: new_loss_new.item()=0.5279562473297119 new_loss_old.item()=0.5280154943466187\n",
      "step=623: new_loss_new.item()=0.3472956418991089 new_loss_old.item()=0.34730264544487\n",
      "step=624: new_loss_new.item()=0.44378021359443665 new_loss_old.item()=0.4437943994998932\n",
      "step=625: new_loss_new.item()=0.1550697386264801 new_loss_old.item()=0.155075803399086\n",
      "step=626: new_loss_new.item()=0.09273884445428848 new_loss_old.item()=0.09273699671030045\n",
      "step=627: new_loss_new.item()=0.9012239575386047 new_loss_old.item()=0.9013010859489441\n",
      "step=628: new_loss_new.item()=0.7350311875343323 new_loss_old.item()=0.7350437045097351\n",
      "step=629: new_loss_new.item()=0.7291856408119202 new_loss_old.item()=0.7292221784591675\n",
      "step=630: new_loss_new.item()=0.36512553691864014 new_loss_old.item()=0.36512792110443115\n",
      "step=631: new_loss_new.item()=0.39749279618263245 new_loss_old.item()=0.3974955379962921\n",
      "step=632: new_loss_new.item()=0.5373005867004395 new_loss_old.item()=0.5373024344444275\n",
      "step=633: new_loss_new.item()=1.4417141675949097 new_loss_old.item()=1.4417624473571777\n",
      "step=634: new_loss_new.item()=0.6653811931610107 new_loss_old.item()=0.6654386520385742\n",
      "step=635: new_loss_new.item()=1.1664382219314575 new_loss_old.item()=1.1664352416992188\n",
      "step=636: new_loss_new.item()=0.32938918471336365 new_loss_old.item()=0.3293931484222412\n",
      "step=637: new_loss_new.item()=0.3991246521472931 new_loss_old.item()=0.3991555869579315\n",
      "step=638: new_loss_new.item()=0.7411460876464844 new_loss_old.item()=0.7411624193191528\n",
      "step=639: new_loss_new.item()=0.6491974592208862 new_loss_old.item()=0.6492002606391907\n",
      "step=640: new_loss_new.item()=1.5498664379119873 new_loss_old.item()=1.5498909950256348\n",
      "step=641: new_loss_new.item()=0.4106409251689911 new_loss_old.item()=0.4106557071208954\n",
      "step=642: new_loss_new.item()=0.8273762464523315 new_loss_old.item()=0.8273789882659912\n",
      "step=643: new_loss_new.item()=0.303379088640213 new_loss_old.item()=0.3033842444419861\n",
      "step=644: new_loss_new.item()=0.5590938329696655 new_loss_old.item()=0.5591496229171753\n",
      "step=645: new_loss_new.item()=0.5590639114379883 new_loss_old.item()=0.5590750575065613\n",
      "step=646: new_loss_new.item()=0.44026118516921997 new_loss_old.item()=0.4402953088283539\n",
      "step=647: new_loss_new.item()=0.27926331758499146 new_loss_old.item()=0.27929776906967163\n",
      "step=648: new_loss_new.item()=0.32409152388572693 new_loss_old.item()=0.32409361004829407\n",
      "step=649: new_loss_new.item()=0.6509293913841248 new_loss_old.item()=0.6509422063827515\n",
      "step=650: new_loss_new.item()=0.5961133241653442 new_loss_old.item()=0.5961317420005798\n",
      "step=651: new_loss_new.item()=0.2093288153409958 new_loss_old.item()=0.2093573659658432\n",
      "step=652: new_loss_new.item()=0.5525304079055786 new_loss_old.item()=0.5525637269020081\n",
      "step=653: new_loss_new.item()=0.24419192969799042 new_loss_old.item()=0.2441999912261963\n",
      "step=654: new_loss_new.item()=0.7338719964027405 new_loss_old.item()=0.7338567972183228\n",
      "step=655: new_loss_new.item()=0.8867384195327759 new_loss_old.item()=0.8867719769477844\n",
      "step=656: new_loss_new.item()=0.19672700762748718 new_loss_old.item()=0.1967492550611496\n",
      "step=657: new_loss_new.item()=0.4768312871456146 new_loss_old.item()=0.4768330454826355\n",
      "step=658: new_loss_new.item()=0.9740898013114929 new_loss_old.item()=0.9741979837417603\n",
      "step=659: new_loss_new.item()=0.18031957745552063 new_loss_old.item()=0.1803223341703415\n",
      "step=660: new_loss_new.item()=0.9501961469650269 new_loss_old.item()=0.9502093195915222\n",
      "step=661: new_loss_new.item()=0.2144017219543457 new_loss_old.item()=0.21442657709121704\n",
      "step=662: new_loss_new.item()=0.5325812101364136 new_loss_old.item()=0.5326294302940369\n",
      "step=663: new_loss_new.item()=0.4458126425743103 new_loss_old.item()=0.44582399725914\n",
      "step=664: new_loss_new.item()=0.5907344818115234 new_loss_old.item()=0.5907687544822693\n",
      "step=665: new_loss_new.item()=0.119794100522995 new_loss_old.item()=0.1197945773601532\n",
      "step=666: new_loss_new.item()=0.35072192549705505 new_loss_old.item()=0.3507482707500458\n",
      "step=667: new_loss_new.item()=0.7966682314872742 new_loss_old.item()=0.7966745495796204\n",
      "step=668: new_loss_new.item()=0.48756176233291626 new_loss_old.item()=0.487558513879776\n",
      "step=669: new_loss_new.item()=0.07833508402109146 new_loss_old.item()=0.07833544164896011\n",
      "step=670: new_loss_new.item()=0.5925157070159912 new_loss_old.item()=0.5925241708755493\n",
      "step=671: new_loss_new.item()=0.13891495764255524 new_loss_old.item()=0.1389172375202179\n",
      "step=672: new_loss_new.item()=0.16924743354320526 new_loss_old.item()=0.16924980282783508\n",
      "step=673: new_loss_new.item()=0.3820245563983917 new_loss_old.item()=0.3820545971393585\n",
      "step=674: new_loss_new.item()=1.033764123916626 new_loss_old.item()=1.0337600708007812\n",
      "step=675: new_loss_new.item()=0.535183310508728 new_loss_old.item()=0.5352132320404053\n",
      "step=676: new_loss_new.item()=0.3277099132537842 new_loss_old.item()=0.32772329449653625\n",
      "step=677: new_loss_new.item()=0.5477592945098877 new_loss_old.item()=0.5477710366249084\n",
      "step=678: new_loss_new.item()=0.8700881600379944 new_loss_old.item()=0.8700868487358093\n",
      "step=679: new_loss_new.item()=0.2694806754589081 new_loss_old.item()=0.26948240399360657\n",
      "step=680: new_loss_new.item()=0.4395487606525421 new_loss_old.item()=0.43958932161331177\n",
      "step=681: new_loss_new.item()=0.5742318034172058 new_loss_old.item()=0.5742374062538147\n",
      "step=682: new_loss_new.item()=0.17110596597194672 new_loss_old.item()=0.1711152344942093\n",
      "step=683: new_loss_new.item()=0.3469051420688629 new_loss_old.item()=0.34691497683525085\n",
      "step=684: new_loss_new.item()=0.3206815719604492 new_loss_old.item()=0.3206789195537567\n",
      "step=685: new_loss_new.item()=0.29849815368652344 new_loss_old.item()=0.29850491881370544\n",
      "step=686: new_loss_new.item()=0.41800108551979065 new_loss_old.item()=0.41800886392593384\n",
      "step=687: new_loss_new.item()=0.32570746541023254 new_loss_old.item()=0.3257367014884949\n",
      "step=688: new_loss_new.item()=0.367225706577301 new_loss_old.item()=0.3672274053096771\n",
      "step=689: new_loss_new.item()=0.156346395611763 new_loss_old.item()=0.15634785592556\n",
      "step=690: new_loss_new.item()=0.26147380471229553 new_loss_old.item()=0.26149803400039673\n",
      "step=691: new_loss_new.item()=0.6507307887077332 new_loss_old.item()=0.6507712602615356\n",
      "step=692: new_loss_new.item()=0.233922079205513 new_loss_old.item()=0.2339344322681427\n",
      "step=693: new_loss_new.item()=0.4887266755104065 new_loss_old.item()=0.4887568950653076\n",
      "step=694: new_loss_new.item()=0.5029682517051697 new_loss_old.item()=0.5030108690261841\n",
      "step=695: new_loss_new.item()=0.3717575669288635 new_loss_old.item()=0.37180382013320923\n",
      "step=696: new_loss_new.item()=0.27769502997398376 new_loss_old.item()=0.27770039439201355\n",
      "step=697: new_loss_new.item()=0.13329093158245087 new_loss_old.item()=0.13329578936100006\n",
      "step=698: new_loss_new.item()=0.48877090215682983 new_loss_old.item()=0.48877331614494324\n",
      "step=699: new_loss_new.item()=0.5251494646072388 new_loss_old.item()=0.5251606702804565\n",
      "step=700: new_loss_new.item()=0.2452203333377838 new_loss_old.item()=0.24523498117923737\n",
      "step=701: new_loss_new.item()=0.5453715920448303 new_loss_old.item()=0.5453774333000183\n",
      "step=702: new_loss_new.item()=0.4902394711971283 new_loss_old.item()=0.4902670979499817\n",
      "step=703: new_loss_new.item()=0.45754557847976685 new_loss_old.item()=0.4575933516025543\n",
      "step=704: new_loss_new.item()=0.541669487953186 new_loss_old.item()=0.5417109131813049\n",
      "step=705: new_loss_new.item()=0.9058937430381775 new_loss_old.item()=0.9059119820594788\n",
      "step=706: new_loss_new.item()=0.27919331192970276 new_loss_old.item()=0.279206246137619\n",
      "step=707: new_loss_new.item()=0.7142379283905029 new_loss_old.item()=0.7142808437347412\n",
      "step=708: new_loss_new.item()=0.5342214107513428 new_loss_old.item()=0.5342667102813721\n",
      "step=709: new_loss_new.item()=0.2652144134044647 new_loss_old.item()=0.265235960483551\n",
      "step=710: new_loss_new.item()=0.626362144947052 new_loss_old.item()=0.6263779401779175\n",
      "step=711: new_loss_new.item()=0.5690068006515503 new_loss_old.item()=0.5690485239028931\n",
      "step=712: new_loss_new.item()=0.7972781658172607 new_loss_old.item()=0.7973197102546692\n",
      "step=713: new_loss_new.item()=0.9314475655555725 new_loss_old.item()=0.9314876198768616\n",
      "step=714: new_loss_new.item()=0.33121687173843384 new_loss_old.item()=0.33122625946998596\n",
      "step=715: new_loss_new.item()=1.012220025062561 new_loss_old.item()=1.0123203992843628\n",
      "step=716: new_loss_new.item()=0.3115499019622803 new_loss_old.item()=0.31158363819122314\n",
      "step=717: new_loss_new.item()=0.783837616443634 new_loss_old.item()=0.783841609954834\n",
      "step=718: new_loss_new.item()=0.2409355491399765 new_loss_old.item()=0.24093978106975555\n",
      "step=719: new_loss_new.item()=0.5746020078659058 new_loss_old.item()=0.5746118426322937\n",
      "step=720: new_loss_new.item()=0.449672132730484 new_loss_old.item()=0.4496753215789795\n",
      "step=721: new_loss_new.item()=0.5162107944488525 new_loss_old.item()=0.5162402391433716\n",
      "step=722: new_loss_new.item()=0.698673665523529 new_loss_old.item()=0.6986549496650696\n",
      "step=723: new_loss_new.item()=0.13860104978084564 new_loss_old.item()=0.13859783113002777\n",
      "step=724: new_loss_new.item()=0.7244924306869507 new_loss_old.item()=0.724498987197876\n",
      "step=725: new_loss_new.item()=0.25711196660995483 new_loss_old.item()=0.2571201026439667\n",
      "step=726: new_loss_new.item()=0.9506961703300476 new_loss_old.item()=0.9507012367248535\n",
      "step=727: new_loss_new.item()=0.9957337379455566 new_loss_old.item()=0.9957707524299622\n",
      "step=728: new_loss_new.item()=0.4393516778945923 new_loss_old.item()=0.4393618106842041\n",
      "step=729: new_loss_new.item()=0.5561572909355164 new_loss_old.item()=0.5561665296554565\n",
      "step=730: new_loss_new.item()=0.4087432622909546 new_loss_old.item()=0.4087638258934021\n",
      "step=731: new_loss_new.item()=0.895912766456604 new_loss_old.item()=0.8959400653839111\n",
      "step=732: new_loss_new.item()=0.28144532442092896 new_loss_old.item()=0.28144994378089905\n",
      "step=733: new_loss_new.item()=0.5703185796737671 new_loss_old.item()=0.5703309774398804\n",
      "step=734: new_loss_new.item()=0.5465363264083862 new_loss_old.item()=0.5465760231018066\n",
      "step=735: new_loss_new.item()=0.5700282454490662 new_loss_old.item()=0.5700693130493164\n",
      "step=736: new_loss_new.item()=0.9999496936798096 new_loss_old.item()=1.0000231266021729\n",
      "step=737: new_loss_new.item()=0.5394602417945862 new_loss_old.item()=0.5395129919052124\n",
      "step=738: new_loss_new.item()=0.30864274501800537 new_loss_old.item()=0.3086676597595215\n",
      "step=739: new_loss_new.item()=1.1475814580917358 new_loss_old.item()=1.1476216316223145\n",
      "step=740: new_loss_new.item()=0.24229952692985535 new_loss_old.item()=0.24233059585094452\n",
      "step=741: new_loss_new.item()=0.45469799637794495 new_loss_old.item()=0.4547025263309479\n",
      "step=742: new_loss_new.item()=0.749241828918457 new_loss_old.item()=0.7492817640304565\n",
      "step=743: new_loss_new.item()=0.6994343996047974 new_loss_old.item()=0.6994574069976807\n",
      "step=744: new_loss_new.item()=0.5609738230705261 new_loss_old.item()=0.5610319972038269\n",
      "step=745: new_loss_new.item()=0.35040590167045593 new_loss_old.item()=0.35040709376335144\n",
      "step=746: new_loss_new.item()=0.1485637128353119 new_loss_old.item()=0.14855995774269104\n",
      "step=747: new_loss_new.item()=1.9424943923950195 new_loss_old.item()=1.9424852132797241\n",
      "step=748: new_loss_new.item()=0.4693748950958252 new_loss_old.item()=0.469413161277771\n",
      "step=749: new_loss_new.item()=0.6126340627670288 new_loss_old.item()=0.6126630306243896\n",
      "step=750: new_loss_new.item()=0.6042594909667969 new_loss_old.item()=0.604263424873352\n",
      "step=751: new_loss_new.item()=1.0622503757476807 new_loss_old.item()=1.0623596906661987\n",
      "step=752: new_loss_new.item()=0.35335052013397217 new_loss_old.item()=0.35335448384284973\n",
      "step=753: new_loss_new.item()=0.5347707271575928 new_loss_old.item()=0.5347651243209839\n",
      "step=754: new_loss_new.item()=0.7532469034194946 new_loss_old.item()=0.7532789707183838\n",
      "step=755: new_loss_new.item()=0.6763790249824524 new_loss_old.item()=0.6763885617256165\n",
      "step=756: new_loss_new.item()=0.4712466597557068 new_loss_old.item()=0.47126004099845886\n",
      "step=757: new_loss_new.item()=0.31975269317626953 new_loss_old.item()=0.31977781653404236\n",
      "step=758: new_loss_new.item()=0.5176244378089905 new_loss_old.item()=0.517633855342865\n",
      "step=759: new_loss_new.item()=0.46191275119781494 new_loss_old.item()=0.4619528353214264\n",
      "step=760: new_loss_new.item()=0.4544070363044739 new_loss_old.item()=0.4544392228126526\n",
      "step=761: new_loss_new.item()=0.3043707609176636 new_loss_old.item()=0.30437520146369934\n",
      "step=762: new_loss_new.item()=0.25211837887763977 new_loss_old.item()=0.2521236836910248\n",
      "step=763: new_loss_new.item()=0.6271005272865295 new_loss_old.item()=0.6271515488624573\n",
      "step=764: new_loss_new.item()=0.34049099683761597 new_loss_old.item()=0.3405348062515259\n",
      "step=765: new_loss_new.item()=0.15188106894493103 new_loss_old.item()=0.15188489854335785\n",
      "step=766: new_loss_new.item()=1.672465443611145 new_loss_old.item()=1.6724743843078613\n",
      "step=767: new_loss_new.item()=0.5547498464584351 new_loss_old.item()=0.5548094511032104\n",
      "step=768: new_loss_new.item()=0.353425532579422 new_loss_old.item()=0.3534717857837677\n",
      "step=769: new_loss_new.item()=0.19033320248126984 new_loss_old.item()=0.19033931195735931\n",
      "step=770: new_loss_new.item()=0.33921343088150024 new_loss_old.item()=0.33921679854393005\n",
      "step=771: new_loss_new.item()=0.39176684617996216 new_loss_old.item()=0.39179956912994385\n",
      "step=772: new_loss_new.item()=1.1069600582122803 new_loss_old.item()=1.1069399118423462\n",
      "step=773: new_loss_new.item()=0.42210182547569275 new_loss_old.item()=0.4221142828464508\n",
      "step=774: new_loss_new.item()=0.3325311243534088 new_loss_old.item()=0.3325350284576416\n",
      "step=775: new_loss_new.item()=1.6677526235580444 new_loss_old.item()=1.6677716970443726\n",
      "step=776: new_loss_new.item()=0.679847240447998 new_loss_old.item()=0.6798704266548157\n",
      "step=777: new_loss_new.item()=0.8110774755477905 new_loss_old.item()=0.8111199140548706\n",
      "step=778: new_loss_new.item()=0.34249258041381836 new_loss_old.item()=0.3425181210041046\n",
      "step=779: new_loss_new.item()=0.41834479570388794 new_loss_old.item()=0.41835516691207886\n",
      "step=780: new_loss_new.item()=0.08339786529541016 new_loss_old.item()=0.08340001851320267\n",
      "step=781: new_loss_new.item()=0.5531731843948364 new_loss_old.item()=0.5532073974609375\n",
      "step=782: new_loss_new.item()=0.4014638066291809 new_loss_old.item()=0.40149614214897156\n",
      "step=783: new_loss_new.item()=0.312022864818573 new_loss_old.item()=0.31205451488494873\n",
      "step=784: new_loss_new.item()=0.551101803779602 new_loss_old.item()=0.5511110424995422\n",
      "step=785: new_loss_new.item()=0.29771894216537476 new_loss_old.item()=0.29771944880485535\n",
      "step=786: new_loss_new.item()=0.5024244785308838 new_loss_old.item()=0.5024909973144531\n",
      "step=787: new_loss_new.item()=0.8714112639427185 new_loss_old.item()=0.8714556694030762\n",
      "step=788: new_loss_new.item()=0.5626441240310669 new_loss_old.item()=0.5626581311225891\n",
      "step=789: new_loss_new.item()=0.6774707436561584 new_loss_old.item()=0.6775223612785339\n",
      "step=790: new_loss_new.item()=0.48065274953842163 new_loss_old.item()=0.4806595742702484\n",
      "step=791: new_loss_new.item()=0.2077115774154663 new_loss_old.item()=0.2077193260192871\n",
      "step=792: new_loss_new.item()=0.7988104224205017 new_loss_old.item()=0.7988150119781494\n",
      "step=793: new_loss_new.item()=0.21375930309295654 new_loss_old.item()=0.21375732123851776\n",
      "step=794: new_loss_new.item()=0.7519629001617432 new_loss_old.item()=0.7520350217819214\n",
      "step=795: new_loss_new.item()=0.26516368985176086 new_loss_old.item()=0.2651662826538086\n",
      "step=796: new_loss_new.item()=0.30670812726020813 new_loss_old.item()=0.3067395091056824\n",
      "step=797: new_loss_new.item()=0.7667759656906128 new_loss_old.item()=0.766812264919281\n",
      "step=798: new_loss_new.item()=0.20643308758735657 new_loss_old.item()=0.20642907917499542\n",
      "step=799: new_loss_new.item()=0.41809046268463135 new_loss_old.item()=0.4181160628795624\n",
      "step=800: new_loss_new.item()=0.28856217861175537 new_loss_old.item()=0.2885649800300598\n",
      "step=801: new_loss_new.item()=0.6255775094032288 new_loss_old.item()=0.6255818009376526\n",
      "step=802: new_loss_new.item()=1.0615490674972534 new_loss_old.item()=1.0616532564163208\n",
      "step=803: new_loss_new.item()=0.16502085328102112 new_loss_old.item()=0.16503429412841797\n",
      "step=804: new_loss_new.item()=0.07471762597560883 new_loss_old.item()=0.0747186541557312\n",
      "step=805: new_loss_new.item()=0.6739014387130737 new_loss_old.item()=0.6739552617073059\n",
      "step=806: new_loss_new.item()=0.08137287199497223 new_loss_old.item()=0.0813736543059349\n",
      "step=807: new_loss_new.item()=0.6990141868591309 new_loss_old.item()=0.6990259289741516\n",
      "step=808: new_loss_new.item()=0.5342310070991516 new_loss_old.item()=0.5342538952827454\n",
      "step=809: new_loss_new.item()=0.20182238519191742 new_loss_old.item()=0.20185118913650513\n",
      "step=810: new_loss_new.item()=0.7291399240493774 new_loss_old.item()=0.7292194366455078\n",
      "step=811: new_loss_new.item()=0.4608638286590576 new_loss_old.item()=0.46087032556533813\n",
      "step=812: new_loss_new.item()=0.6628089547157288 new_loss_old.item()=0.6628180742263794\n",
      "step=813: new_loss_new.item()=0.5124202966690063 new_loss_old.item()=0.5124266743659973\n",
      "step=814: new_loss_new.item()=0.7826744318008423 new_loss_old.item()=0.7826932072639465\n",
      "step=815: new_loss_new.item()=0.6310261487960815 new_loss_old.item()=0.6310323476791382\n",
      "step=816: new_loss_new.item()=0.6555355787277222 new_loss_old.item()=0.6555471420288086\n",
      "step=817: new_loss_new.item()=0.9441953301429749 new_loss_old.item()=0.9442592859268188\n",
      "step=818: new_loss_new.item()=0.9985830783843994 new_loss_old.item()=0.9986079931259155\n",
      "step=819: new_loss_new.item()=0.7377737164497375 new_loss_old.item()=0.7377917170524597\n",
      "step=820: new_loss_new.item()=0.6400091648101807 new_loss_old.item()=0.6400190591812134\n",
      "step=821: new_loss_new.item()=0.46653929352760315 new_loss_old.item()=0.466593861579895\n",
      "step=822: new_loss_new.item()=0.7393127679824829 new_loss_old.item()=0.7393191456794739\n",
      "step=823: new_loss_new.item()=0.896146833896637 new_loss_old.item()=0.8961673378944397\n",
      "step=824: new_loss_new.item()=0.2522996664047241 new_loss_old.item()=0.2523137629032135\n",
      "step=825: new_loss_new.item()=0.2666162848472595 new_loss_old.item()=0.26662787795066833\n",
      "step=826: new_loss_new.item()=0.3991398215293884 new_loss_old.item()=0.3991833031177521\n",
      "step=827: new_loss_new.item()=2.9679694175720215 new_loss_old.item()=2.968014717102051\n",
      "step=828: new_loss_new.item()=0.4786246120929718 new_loss_old.item()=0.4786653518676758\n",
      "step=829: new_loss_new.item()=0.42076921463012695 new_loss_old.item()=0.4207901954650879\n",
      "step=830: new_loss_new.item()=0.3084322512149811 new_loss_old.item()=0.3084656894207001\n",
      "step=831: new_loss_new.item()=0.435080885887146 new_loss_old.item()=0.4351336359977722\n",
      "step=832: new_loss_new.item()=0.3393039107322693 new_loss_old.item()=0.33931106328964233\n",
      "step=833: new_loss_new.item()=0.4053806960582733 new_loss_old.item()=0.4053971469402313\n",
      "step=834: new_loss_new.item()=0.6717994809150696 new_loss_old.item()=0.6718875765800476\n",
      "step=835: new_loss_new.item()=0.8908979892730713 new_loss_old.item()=0.8909122347831726\n",
      "step=836: new_loss_new.item()=0.41840946674346924 new_loss_old.item()=0.4184558093547821\n",
      "step=837: new_loss_new.item()=0.6285421848297119 new_loss_old.item()=0.6285572052001953\n",
      "step=838: new_loss_new.item()=0.3971081078052521 new_loss_old.item()=0.3971156179904938\n",
      "step=839: new_loss_new.item()=0.3181353211402893 new_loss_old.item()=0.3181432783603668\n",
      "step=840: new_loss_new.item()=0.6574694514274597 new_loss_old.item()=0.6575119495391846\n",
      "step=841: new_loss_new.item()=0.969707727432251 new_loss_old.item()=0.9697538018226624\n",
      "step=842: new_loss_new.item()=0.2645407021045685 new_loss_old.item()=0.26455169916152954\n",
      "step=843: new_loss_new.item()=1.1934170722961426 new_loss_old.item()=1.1935179233551025\n",
      "step=844: new_loss_new.item()=0.5289294123649597 new_loss_old.item()=0.528965413570404\n",
      "step=845: new_loss_new.item()=0.2404264509677887 new_loss_old.item()=0.2404620349407196\n",
      "step=846: new_loss_new.item()=0.7344077229499817 new_loss_old.item()=0.7344238758087158\n",
      "step=847: new_loss_new.item()=0.4902592599391937 new_loss_old.item()=0.490294486284256\n",
      "step=848: new_loss_new.item()=0.8793599009513855 new_loss_old.item()=0.8794161081314087\n",
      "step=849: new_loss_new.item()=0.12041749805212021 new_loss_old.item()=0.1204189732670784\n",
      "step=850: new_loss_new.item()=0.561857283115387 new_loss_old.item()=0.5618643164634705\n",
      "step=851: new_loss_new.item()=1.0277819633483887 new_loss_old.item()=1.0278154611587524\n",
      "step=852: new_loss_new.item()=1.3796026706695557 new_loss_old.item()=1.379746437072754\n",
      "step=853: new_loss_new.item()=1.005257248878479 new_loss_old.item()=1.0053484439849854\n",
      "step=854: new_loss_new.item()=0.5307672023773193 new_loss_old.item()=0.530781090259552\n",
      "step=855: new_loss_new.item()=0.287456750869751 new_loss_old.item()=0.28749290108680725\n",
      "step=856: new_loss_new.item()=1.5018565654754639 new_loss_old.item()=1.5018775463104248\n",
      "step=857: new_loss_new.item()=0.986347496509552 new_loss_old.item()=0.9863954186439514\n",
      "step=858: new_loss_new.item()=0.29445427656173706 new_loss_old.item()=0.29445937275886536\n",
      "step=859: new_loss_new.item()=0.9562973976135254 new_loss_old.item()=0.9563614130020142\n",
      "step=860: new_loss_new.item()=0.3479326069355011 new_loss_old.item()=0.3479465842247009\n",
      "step=861: new_loss_new.item()=0.3583332598209381 new_loss_old.item()=0.3583400547504425\n",
      "step=862: new_loss_new.item()=0.4785729944705963 new_loss_old.item()=0.4785861074924469\n",
      "step=863: new_loss_new.item()=0.5240806341171265 new_loss_old.item()=0.5240886211395264\n",
      "step=864: new_loss_new.item()=0.3121950626373291 new_loss_old.item()=0.3122139573097229\n",
      "step=865: new_loss_new.item()=0.4759270250797272 new_loss_old.item()=0.47593048214912415\n",
      "step=866: new_loss_new.item()=0.5055825114250183 new_loss_old.item()=0.5056638121604919\n",
      "step=867: new_loss_new.item()=0.7006343007087708 new_loss_old.item()=0.7007241249084473\n",
      "step=868: new_loss_new.item()=0.38792696595191956 new_loss_old.item()=0.38793477416038513\n",
      "step=869: new_loss_new.item()=0.33507415652275085 new_loss_old.item()=0.3350924849510193\n",
      "step=870: new_loss_new.item()=0.18254122138023376 new_loss_old.item()=0.18254172801971436\n",
      "step=871: new_loss_new.item()=0.6218502521514893 new_loss_old.item()=0.6218697428703308\n",
      "step=872: new_loss_new.item()=0.13829772174358368 new_loss_old.item()=0.1382979452610016\n",
      "step=873: new_loss_new.item()=0.6060178279876709 new_loss_old.item()=0.6060285568237305\n",
      "step=874: new_loss_new.item()=0.1755073070526123 new_loss_old.item()=0.17550890147686005\n",
      "step=875: new_loss_new.item()=0.8532116413116455 new_loss_old.item()=0.8532631993293762\n",
      "step=876: new_loss_new.item()=0.5140645503997803 new_loss_old.item()=0.5140791535377502\n",
      "step=877: new_loss_new.item()=0.5110171437263489 new_loss_old.item()=0.5110233426094055\n",
      "step=878: new_loss_new.item()=0.07610754668712616 new_loss_old.item()=0.07610316574573517\n",
      "step=879: new_loss_new.item()=0.3692360520362854 new_loss_old.item()=0.36925071477890015\n",
      "step=880: new_loss_new.item()=0.5478807091712952 new_loss_old.item()=0.5479478240013123\n",
      "step=881: new_loss_new.item()=0.6383832693099976 new_loss_old.item()=0.6384193897247314\n",
      "step=882: new_loss_new.item()=0.9017412066459656 new_loss_old.item()=0.9017635583877563\n",
      "step=883: new_loss_new.item()=0.9764315485954285 new_loss_old.item()=0.9764422178268433\n",
      "step=884: new_loss_new.item()=0.32717451453208923 new_loss_old.item()=0.3271724581718445\n",
      "step=885: new_loss_new.item()=0.5597597360610962 new_loss_old.item()=0.5597595572471619\n",
      "step=886: new_loss_new.item()=0.230539008975029 new_loss_old.item()=0.23054388165473938\n",
      "step=887: new_loss_new.item()=0.42843863368034363 new_loss_old.item()=0.42843785881996155\n",
      "step=888: new_loss_new.item()=0.4898322522640228 new_loss_old.item()=0.48987504839897156\n",
      "step=889: new_loss_new.item()=0.7849247455596924 new_loss_old.item()=0.7849656343460083\n",
      "step=890: new_loss_new.item()=0.1917293667793274 new_loss_old.item()=0.19173459708690643\n",
      "step=891: new_loss_new.item()=0.37737181782722473 new_loss_old.item()=0.37740859389305115\n",
      "step=892: new_loss_new.item()=0.5354658365249634 new_loss_old.item()=0.5354627370834351\n",
      "step=893: new_loss_new.item()=0.8334586024284363 new_loss_old.item()=0.833564043045044\n",
      "step=894: new_loss_new.item()=0.6997259855270386 new_loss_old.item()=0.6997286677360535\n",
      "step=895: new_loss_new.item()=0.36739373207092285 new_loss_old.item()=0.36743634939193726\n",
      "step=896: new_loss_new.item()=0.673648476600647 new_loss_old.item()=0.6736631989479065\n",
      "step=897: new_loss_new.item()=1.0276373624801636 new_loss_old.item()=1.0277146100997925\n",
      "step=898: new_loss_new.item()=0.2868368327617645 new_loss_old.item()=0.28684961795806885\n",
      "step=899: new_loss_new.item()=0.42759454250335693 new_loss_old.item()=0.42762768268585205\n",
      "step=900: new_loss_new.item()=0.6897796392440796 new_loss_old.item()=0.689820408821106\n",
      "step=901: new_loss_new.item()=0.6404044032096863 new_loss_old.item()=0.6404023170471191\n",
      "step=902: new_loss_new.item()=0.588782012462616 new_loss_old.item()=0.5887890458106995\n",
      "step=903: new_loss_new.item()=0.7276734709739685 new_loss_old.item()=0.7276754379272461\n",
      "step=904: new_loss_new.item()=0.9878538846969604 new_loss_old.item()=0.987888514995575\n",
      "step=905: new_loss_new.item()=0.5877314209938049 new_loss_old.item()=0.5877555012702942\n",
      "step=906: new_loss_new.item()=1.048874855041504 new_loss_old.item()=1.0489944219589233\n",
      "step=907: new_loss_new.item()=0.9927520751953125 new_loss_old.item()=0.9928684234619141\n",
      "step=908: new_loss_new.item()=0.6066923141479492 new_loss_old.item()=0.6067126989364624\n",
      "step=909: new_loss_new.item()=0.2765548527240753 new_loss_old.item()=0.27659210562705994\n",
      "step=910: new_loss_new.item()=0.4543437063694 new_loss_old.item()=0.454393595457077\n",
      "step=911: new_loss_new.item()=0.5118874907493591 new_loss_old.item()=0.5118842720985413\n",
      "step=912: new_loss_new.item()=0.24521782994270325 new_loss_old.item()=0.2452520728111267\n",
      "step=913: new_loss_new.item()=1.0488563776016235 new_loss_old.item()=1.0488821268081665\n",
      "step=914: new_loss_new.item()=0.4266667664051056 new_loss_old.item()=0.4267047643661499\n",
      "step=915: new_loss_new.item()=1.0901765823364258 new_loss_old.item()=1.0903077125549316\n",
      "step=916: new_loss_new.item()=0.8509959578514099 new_loss_old.item()=0.8509988784790039\n",
      "step=917: new_loss_new.item()=0.5117388963699341 new_loss_old.item()=0.5117437839508057\n",
      "step=918: new_loss_new.item()=0.8869168758392334 new_loss_old.item()=0.8870345950126648\n",
      "step=919: new_loss_new.item()=1.3459709882736206 new_loss_old.item()=1.346124291419983\n",
      "step=920: new_loss_new.item()=0.3447120189666748 new_loss_old.item()=0.34472569823265076\n",
      "step=921: new_loss_new.item()=0.41515016555786133 new_loss_old.item()=0.41520214080810547\n",
      "step=922: new_loss_new.item()=0.6980668902397156 new_loss_old.item()=0.698134183883667\n",
      "step=923: new_loss_new.item()=0.5351400971412659 new_loss_old.item()=0.5351787209510803\n",
      "step=924: new_loss_new.item()=0.5563517808914185 new_loss_old.item()=0.5563818216323853\n",
      "step=925: new_loss_new.item()=0.46656885743141174 new_loss_old.item()=0.4665854275226593\n",
      "step=926: new_loss_new.item()=0.873046875 new_loss_old.item()=0.8730911612510681\n",
      "step=927: new_loss_new.item()=0.706929624080658 new_loss_old.item()=0.707009494304657\n",
      "step=928: new_loss_new.item()=0.5232136845588684 new_loss_old.item()=0.5232582688331604\n",
      "step=929: new_loss_new.item()=0.7862241268157959 new_loss_old.item()=0.7862533330917358\n",
      "step=930: new_loss_new.item()=0.816439151763916 new_loss_old.item()=0.8165111541748047\n",
      "step=931: new_loss_new.item()=0.37042954564094543 new_loss_old.item()=0.37044674158096313\n",
      "step=932: new_loss_new.item()=0.3703320026397705 new_loss_old.item()=0.3703773021697998\n",
      "step=933: new_loss_new.item()=0.43729984760284424 new_loss_old.item()=0.43730318546295166\n",
      "step=934: new_loss_new.item()=0.2487761378288269 new_loss_old.item()=0.24881577491760254\n",
      "step=935: new_loss_new.item()=0.3984449505805969 new_loss_old.item()=0.39847156405448914\n",
      "step=936: new_loss_new.item()=0.1556331366300583 new_loss_old.item()=0.15564195811748505\n",
      "step=937: new_loss_new.item()=0.603086531162262 new_loss_old.item()=0.6031531691551208\n",
      "step=938: new_loss_new.item()=0.4274274706840515 new_loss_old.item()=0.4274454414844513\n",
      "step=939: new_loss_new.item()=0.3028455376625061 new_loss_old.item()=0.30288973450660706\n",
      "step=940: new_loss_new.item()=1.1418086290359497 new_loss_old.item()=1.1418856382369995\n",
      "step=941: new_loss_new.item()=0.6193514466285706 new_loss_old.item()=0.6193892359733582\n",
      "step=942: new_loss_new.item()=0.5668692588806152 new_loss_old.item()=0.5669494867324829\n",
      "step=943: new_loss_new.item()=0.5321739912033081 new_loss_old.item()=0.5322091579437256\n",
      "step=944: new_loss_new.item()=0.3914594054222107 new_loss_old.item()=0.3914620876312256\n",
      "step=945: new_loss_new.item()=0.43523332476615906 new_loss_old.item()=0.4352763891220093\n",
      "step=946: new_loss_new.item()=0.7360463738441467 new_loss_old.item()=0.7360522150993347\n",
      "step=947: new_loss_new.item()=0.3159484267234802 new_loss_old.item()=0.3159804046154022\n",
      "step=948: new_loss_new.item()=0.3718526065349579 new_loss_old.item()=0.3718891441822052\n",
      "step=949: new_loss_new.item()=0.10953958332538605 new_loss_old.item()=0.10954158753156662\n",
      "step=950: new_loss_new.item()=0.21380943059921265 new_loss_old.item()=0.2138085514307022\n",
      "step=951: new_loss_new.item()=0.5792180895805359 new_loss_old.item()=0.5792531371116638\n",
      "step=952: new_loss_new.item()=0.562144935131073 new_loss_old.item()=0.5621617436408997\n",
      "step=953: new_loss_new.item()=0.48215451836586 new_loss_old.item()=0.48216527700424194\n",
      "step=954: new_loss_new.item()=0.6165350675582886 new_loss_old.item()=0.6165397763252258\n",
      "step=955: new_loss_new.item()=0.8141574263572693 new_loss_old.item()=0.8142086267471313\n",
      "step=956: new_loss_new.item()=0.7232457995414734 new_loss_old.item()=0.7233691811561584\n",
      "step=957: new_loss_new.item()=0.14886733889579773 new_loss_old.item()=0.1488739550113678\n",
      "step=958: new_loss_new.item()=0.3029383420944214 new_loss_old.item()=0.30298155546188354\n",
      "step=959: new_loss_new.item()=0.5801860690116882 new_loss_old.item()=0.5802287459373474\n",
      "step=960: new_loss_new.item()=0.6143207550048828 new_loss_old.item()=0.61435467004776\n",
      "step=961: new_loss_new.item()=0.27539631724357605 new_loss_old.item()=0.2754063010215759\n",
      "step=962: new_loss_new.item()=0.7968835830688477 new_loss_old.item()=0.7968868017196655\n",
      "step=963: new_loss_new.item()=0.5151941180229187 new_loss_old.item()=0.5152471661567688\n",
      "step=964: new_loss_new.item()=0.20057432353496552 new_loss_old.item()=0.20059821009635925\n",
      "step=965: new_loss_new.item()=0.7405582070350647 new_loss_old.item()=0.740598738193512\n",
      "step=966: new_loss_new.item()=0.5087472200393677 new_loss_old.item()=0.5087878108024597\n",
      "step=967: new_loss_new.item()=0.4302663207054138 new_loss_old.item()=0.4302808344364166\n",
      "step=968: new_loss_new.item()=0.5845645070075989 new_loss_old.item()=0.5846233367919922\n",
      "step=969: new_loss_new.item()=0.21124772727489471 new_loss_old.item()=0.21128395199775696\n",
      "step=970: new_loss_new.item()=0.416164755821228 new_loss_old.item()=0.41620710492134094\n",
      "step=971: new_loss_new.item()=0.2998255491256714 new_loss_old.item()=0.29985255002975464\n",
      "step=972: new_loss_new.item()=0.6567104458808899 new_loss_old.item()=0.656787097454071\n",
      "step=973: new_loss_new.item()=0.6670008301734924 new_loss_old.item()=0.6670174598693848\n",
      "step=974: new_loss_new.item()=0.29350119829177856 new_loss_old.item()=0.29355618357658386\n",
      "step=975: new_loss_new.item()=0.2219889909029007 new_loss_old.item()=0.2219921201467514\n",
      "step=976: new_loss_new.item()=0.9583384990692139 new_loss_old.item()=0.9583538174629211\n",
      "step=977: new_loss_new.item()=0.5094413757324219 new_loss_old.item()=0.5094493627548218\n",
      "step=978: new_loss_new.item()=0.7730609774589539 new_loss_old.item()=0.7730680704116821\n",
      "step=979: new_loss_new.item()=0.23675791919231415 new_loss_old.item()=0.2367876023054123\n",
      "step=980: new_loss_new.item()=0.7057710289955139 new_loss_old.item()=0.7057871222496033\n",
      "step=981: new_loss_new.item()=0.3263954520225525 new_loss_old.item()=0.32641205191612244\n",
      "step=982: new_loss_new.item()=0.3466431498527527 new_loss_old.item()=0.34669020771980286\n",
      "step=983: new_loss_new.item()=0.36124804615974426 new_loss_old.item()=0.36126723885536194\n",
      "step=984: new_loss_new.item()=0.2221517115831375 new_loss_old.item()=0.2221919447183609\n",
      "step=985: new_loss_new.item()=0.618943452835083 new_loss_old.item()=0.6189537048339844\n",
      "step=986: new_loss_new.item()=0.47906315326690674 new_loss_old.item()=0.4791334569454193\n",
      "step=987: new_loss_new.item()=1.1218661069869995 new_loss_old.item()=1.121884822845459\n",
      "step=988: new_loss_new.item()=0.6518740653991699 new_loss_old.item()=0.6519230008125305\n",
      "step=989: new_loss_new.item()=0.377972275018692 new_loss_old.item()=0.37801671028137207\n",
      "step=990: new_loss_new.item()=0.2049267590045929 new_loss_old.item()=0.2049352377653122\n",
      "step=991: new_loss_new.item()=0.3284017741680145 new_loss_old.item()=0.32841241359710693\n",
      "step=992: new_loss_new.item()=1.1343574523925781 new_loss_old.item()=1.1344976425170898\n",
      "step=993: new_loss_new.item()=0.47080934047698975 new_loss_old.item()=0.47082066535949707\n",
      "step=994: new_loss_new.item()=0.3113544285297394 new_loss_old.item()=0.3113982081413269\n",
      "step=995: new_loss_new.item()=0.4460434317588806 new_loss_old.item()=0.446052223443985\n",
      "step=996: new_loss_new.item()=0.6214576959609985 new_loss_old.item()=0.6214983463287354\n",
      "step=997: new_loss_new.item()=0.5342046618461609 new_loss_old.item()=0.5342300534248352\n",
      "step=998: new_loss_new.item()=0.4734925925731659 new_loss_old.item()=0.47355782985687256\n",
      "step=999: new_loss_new.item()=1.1317328214645386 new_loss_old.item()=1.1318457126617432\n"
     ]
    }
   ],
   "source": [
    "# test_input_ids = torch.zeros(2, 8).long().cuda()\n",
    "# test_attention_mask = torch.cat( torch.ones_like(test_input_ids).cuda()\n",
    "# encoder_hidden_states = torch.ones(2, 8, 768).cuda()\n",
    "\n",
    "new_model.config.use_ghost_token = False\n",
    "stop_editing_index = 8\n",
    "\n",
    "batches, idx = 1000, 0\n",
    "\n",
    "\n",
    "opt_new = optim.SGD(new_model.parameters(), lr=1e-3)\n",
    "opt_old = optim.SGD(hypernetwork.parameters(), lr=1e-3)\n",
    "\n",
    "losses_old = []\n",
    "grads_old = []\n",
    "losses_new = []\n",
    "grads_new = []\n",
    "\n",
    "for step, batch in enumerate(dl):\n",
    "    idx += 1\n",
    "\n",
    "    if idx > batches:\n",
    "        break\n",
    "\n",
    "    batch = {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    out_hn_old = hypernetwork(\n",
    "        editor_input_ids=batch[\"tokenized_first_sentence\"],\n",
    "        target_input_ids=batch[\"tokenized_next_50_tokens\"],\n",
    "        stop_editing_index=8,\n",
    "    )\n",
    "\n",
    "    out_hn_new = new_model.forward(\n",
    "        editor_input_ids=batch[\"tokenized_first_sentence\"],\n",
    "        editor_attention_mask=(\n",
    "            batch[\"tokenized_first_sentence\"] != tokenizer.pad_token_id\n",
    "        ).int(),\n",
    "        target_input_ids=batch[\"tokenized_next_50_tokens\"],\n",
    "        target_attention_mask=(\n",
    "            batch[\"tokenized_next_50_tokens\"] != tokenizer.pad_token_id\n",
    "        ).int(),\n",
    "        stop_editing_idx=8,\n",
    "    )\n",
    "\n",
    "    # old_loss_old = old_kl_loss(\n",
    "    #     out_hn_old[\"logits\"],\n",
    "    #     hypernetwork.target_model,\n",
    "    #     batch,\n",
    "    # ).item()\n",
    "    # old_loss_new = old_kl_loss(\n",
    "    #     out_hn_new.logits,\n",
    "    #     hypernetwork.target_model,\n",
    "    #     batch,\n",
    "    # ).item()\n",
    "\n",
    "    new_loss_old = new_kl_loss(\n",
    "        out_hn_old[\"logits\"],\n",
    "        hypernetwork.target_model,\n",
    "        batch,\n",
    "    )\n",
    "    new_loss_new = new_kl_loss(\n",
    "        out_hn_new.logits,\n",
    "        hypernetwork.target_model,\n",
    "        batch,\n",
    "    )\n",
    "\n",
    "    opt_new.zero_grad()\n",
    "    opt_old.zero_grad()\n",
    "\n",
    "    new_loss_new.backward()\n",
    "    new_loss_old.backward()\n",
    "\n",
    "    opt_new.step()\n",
    "    opt_old.step()\n",
    "\n",
    "    # get gradients\n",
    "    grads_new = [\n",
    "        param.grad.detach().flatten()\n",
    "        for param in new_model.parameters()\n",
    "        if param.grad is not None\n",
    "    ]\n",
    "    norm_new = torch.cat(grads_new).norm()\n",
    "\n",
    "    grads_old = [\n",
    "        param.grad.detach().flatten()\n",
    "        for param in hypernetwork.parameters()\n",
    "        if param.grad is not None\n",
    "    ]\n",
    "    norm_old = torch.cat(grads_old).norm()\n",
    "\n",
    "    # print(f\"{norm_new.item()=}, {norm_old.item()=}\")\n",
    "    print(f\"{step=}: {new_loss_new.item()=} {new_loss_old.item()=}\")\n",
    "\n",
    "    losses_old.append(new_loss_old.item())\n",
    "    losses_new.append(new_loss_new.item())\n",
    "    grads_old.append(norm_old.item())\n",
    "    grads_new.append(norm_new.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(losses_new, label=\"new\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.arange(0, 8).unsqueeze(0).cuda()\n",
    "input_ids = torch.zeros(1, 5).long().cuda()\n",
    "\n",
    "right_padded = torch.nn.functional.pad(input_ids, (0, 3), mode=\"constant\", value=50256)\n",
    "rp_pos = (right_padded != 50256).cumsum(-1)\n",
    "attn_mask_rp = (right_padded != 50256).int()\n",
    "\n",
    "left_padded = torch.nn.functional.pad(input_ids, (3, 0), mode=\"constant\", value=50256)\n",
    "lp_pos = (left_padded != 50256).cumsum(-1)\n",
    "attn_mask_lp = (left_padded != 50256).int()\n",
    "\n",
    "lpout = hypernetwork.target_model(\n",
    "    input_ids=left_padded, attention_mask=attn_mask_lp, position_ids=lp_pos\n",
    ")\n",
    "rpout = hypernetwork.target_model(\n",
    "    input_ids=right_padded, attention_mask=attn_mask_rp, position_ids=rp_pos\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 1, 1, 1, 1, 1]], device='cuda:0', dtype=torch.int32),\n",
       " tensor([[1, 1, 1, 1, 1, 0, 0, 0]], device='cuda:0', dtype=torch.int32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask_lp, attn_mask_rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 1, 2, 3, 4, 5]], device='cuda:0'),\n",
       " tensor([[1, 2, 3, 4, 5, 5, 5, 5]], device='cuda:0'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp_pos, rp_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(lpout.logits, rpout.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "editor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
