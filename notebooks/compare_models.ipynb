{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import types\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import yaml\n",
    "from torch import compile, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Model, GPT2Tokenizer\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "\n",
    "import wandb\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from models.gpt2 import GPT2Editor\n",
    "from models.gpt2.config import GPT2EditorConfig\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_editing_index = 10  # 4090 or A6000j\n",
    "edit_channel_multiply_factor = 1\n",
    "num_editing_heads = 32  # more seems to be better for this #per sid's suggestion: can add more heads in every layer. This is probably a really great suggestion\n",
    "editor_channel_width = 768 * edit_channel_multiply_factor\n",
    "max_grad_clip = 4.0\n",
    "chop_layer = 1\n",
    "lr = 3e-4\n",
    "edit_dampening_factor = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cross_attention_to_layer(block, config):\n",
    "    block.crossattention = GPT2Attention(config, is_cross_attention=True)\n",
    "    block.ln_cross_attn = nn.LayerNorm(\n",
    "        normalized_shape=768, eps=config.layer_norm_epsilon\n",
    "    )\n",
    "    original_query_weights = block.attn.c_attn.weight[:, :768]\n",
    "    original_keys_values = block.attn.c_attn.weight[:, 768:]\n",
    "    original_query_bias = block.attn.c_attn.bias[:768]\n",
    "    original_keys_values_bias = block.attn.c_attn.bias[768:]\n",
    "    with torch.no_grad():\n",
    "        # Initialize the new layer with these parameters\n",
    "        block.crossattention.q_attn.weight = nn.Parameter(original_query_weights)\n",
    "        block.crossattention.q_attn.bias = nn.Parameter(original_query_bias)\n",
    "        block.crossattention.c_attn.weight = nn.Parameter(original_keys_values)\n",
    "        block.crossattention.c_attn.bias = nn.Parameter(original_keys_values_bias)\n",
    "    return\n",
    "\n",
    "\n",
    "class Editor_Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Controls whether the head will do a global softmax in all positions & layers\n",
    "        # If True, the attn is global and will sum to 1\n",
    "        # If False, the attn is a logistic fxn independently for every layer & token\n",
    "        # I suspect we will also want to penalize the intervention norm\n",
    "        self.num_editing_heads = (\n",
    "            config.num_editing_heads\n",
    "        )  # should default to 1, but we're going to test adding more\n",
    "        self.edit_channel_width = config.edit_channel_width\n",
    "        if self.edit_channel_width % self.num_editing_heads != 0:\n",
    "            print(\"Error: config hidden size is not divisible by num_editing_heads\")\n",
    "        self.head_dim = self.edit_channel_width // self.num_editing_heads\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        max_positions = (\n",
    "            config.max_position_embeddings\n",
    "        )  # does this do anything? can try killing this later\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(\n",
    "                torch.ones((max_positions, max_positions), dtype=torch.bool)\n",
    "            ).view(1, 1, max_positions, max_positions),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n",
    "\n",
    "        # We compute Q and K as a single nn.linear; but will later break apart into subcomponents\n",
    "\n",
    "        ## Before modification to a variable channel-width\n",
    "        # self.q_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.k_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.v_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        self.q_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.k_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.v_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.out_proj = nn.Linear(self.edit_channel_width, self.embed_dim)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, head_dim).\"\"\"\n",
    "        new_shape = x.size()[:-1] + (self.num_editing_heads, self.head_dim)\n",
    "        return x.view(*new_shape)\n",
    "\n",
    "    def _new_reverse_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        # Assume that we are doing softmax attention\n",
    "        # Project and split the query, key, value tensors\n",
    "        split_query = self._split_heads(query)\n",
    "        split_key = self._split_heads(key)\n",
    "        split_value = self._split_heads(value)\n",
    "\n",
    "        # Double-application (is this actually good/better for some reason?)\n",
    "        # self._split_heads(self.q_attn(query))\n",
    "        # self._split_heads(self.k_attn(key))\n",
    "        # self._split_heads(self.v_attn(value))\n",
    "\n",
    "        if split_query.dim() != 4:\n",
    "            print(\"Error: Expected query to be 4D tensor, but got something else!\")\n",
    "        if split_key.dim() != 3:\n",
    "            print(\"Error: Expected key to be 3D tensor, but got something else!\")\n",
    "        if split_value.dim() != 3:\n",
    "            print(\"Error: Expected value to be 3D tensor, but got something else!\")\n",
    "\n",
    "        # Query should be shaped as (batch_index, sequence_index, head_index, head_dim)\n",
    "        # Key and value should be shaped as (batch_index, head_index, head_dim)\n",
    "\n",
    "        # print(\n",
    "        #     \"SHAPES PRIOR TO ATTN CALC\",\n",
    "        #     split_query.permute(0, 2, 1, 3).shape,\n",
    "        #     split_key.unsqueeze(-1).shape,\n",
    "        #     split_value.unsqueeze(-1).shape,\n",
    "        # )\n",
    "\n",
    "        print(f\"{split_query.shape=}, {split_key.shape=}, {split_value.shape=}\")\n",
    "\n",
    "        # split_query: (bsz, seq, num_head, head_dim)\n",
    "        # split_key:   (bsz, num_heads, head_dim, 1)\n",
    "\n",
    "        # out: (bsz, num_heads, seq, 1) -> (bsz, num_heads, seq)\n",
    "\n",
    "        KQ_weights = torch.matmul(\n",
    "            split_query.permute(0, 2, 1, 3), split_key.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Scaling factor\n",
    "        KQ_weights = KQ_weights / torch.full(\n",
    "            [],\n",
    "            split_value.size(-1) ** 0.5,\n",
    "            dtype=KQ_weights.dtype,\n",
    "            device=KQ_weights.device,\n",
    "        )\n",
    "\n",
    "        print(f\"{KQ_weights.norm()=}\")\n",
    "        print(f\"{KQ_weights.shape=}\")\n",
    "\n",
    "        # Then we take the softmax within the positional divisions\n",
    "        softmaxed_weights = nn.functional.softmax(KQ_weights, dim=-1)\n",
    "\n",
    "        print(f\"{softmaxed_weights.norm()=}\")\n",
    "\n",
    "        print(\n",
    "            f\"{softmaxed_weights.unsqueeze(-1).shape=}, {split_value.unsqueeze(-2).shape=}\"\n",
    "        )\n",
    "\n",
    "        # Adjusting value selection for head dimension\n",
    "        attn_output = torch.matmul(\n",
    "            softmaxed_weights.unsqueeze(-1), split_value.unsqueeze(-2)\n",
    "        )\n",
    "\n",
    "        print(f\"{attn_output.norm()=}\")\n",
    "\n",
    "        # combine heads: change 50, 8, 104, 96 to 50, 104, 768\n",
    "        # first, permute\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3)\n",
    "\n",
    "        # combin heads x head_dims\n",
    "        attn_output = attn_output.reshape(\n",
    "            -1, attn_output.size(1), attn_output.size(2) * attn_output.size(3)\n",
    "        )\n",
    "        # now project back\n",
    "        projected_output = self.out_proj(attn_output)\n",
    "\n",
    "        return projected_output, softmaxed_weights\n",
    "\n",
    "    def _reverse_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        if key.dim() == 4:\n",
    "            K_reduced = key[\n",
    "                :, :, -1, :\n",
    "            ]  # R# Check: that the second dimension of K is only a single element when we have batching\n",
    "            KQ_weights = torch.bmm(K_reduced, query.transpose(1, 2))\n",
    "            logistic_weights = torch.atan(KQ_weights)\n",
    "            attn_output = torch.bmm(\n",
    "                logistic_weights.transpose(1, 2),\n",
    "                value[\n",
    "                    :, :, -1, :\n",
    "                ],  # we take the editor output only over the final token position\n",
    "            )\n",
    "\n",
    "        if key.dim() == 3:\n",
    "            QK_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "            logistic_weights = torch.atan(QK_weights)\n",
    "            attn_output = torch.matmul(logistic_weights, value)\n",
    "\n",
    "        return attn_output, logistic_weights\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        editor_hidden_states,\n",
    "        target_hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        print(f\"OLD {editor_hidden_states.norm()=}, {target_hidden_states.norm()=}\")\n",
    "        print(\n",
    "            f\"OLD {self.q_attn.weight.mean()=}, {self.k_attn.weight.mean()=}, {self.v_attn.weight.mean()=}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"OLD {self.q_attn.weight.shape=}, {self.k_attn.weight.shape=}, {self.v_attn.weight.shape=}\"\n",
    "        )\n",
    "\n",
    "        # Here, the query is the target hidden encoder, the key is the editor, and the value is the editor\n",
    "        query = self.q_attn(target_hidden_states)\n",
    "        if editor_hidden_states.dim() == 3:\n",
    "            key = self.k_attn(\n",
    "                # I don't quite understand why sometimes editor_hidden_states is 4 dimensional, sometimes 3\n",
    "                # seems like it's sometimes 20, 1, 4, 768 and sometimes 20, 4, 768. what gives?\n",
    "                editor_hidden_states[:, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "            value = self.v_attn(\n",
    "                # [:, 0, :1, :]\n",
    "                editor_hidden_states[:, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "\n",
    "        if editor_hidden_states.dim() == 4:\n",
    "            key = self.k_attn(\n",
    "                editor_hidden_states[:, 0, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "            value = self.v_attn(\n",
    "                # [:, 0, :1, :]\n",
    "                editor_hidden_states[:, 0, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "\n",
    "        print(f\"OLD {query.norm()=}, {key.norm()=}, {value.norm()=}\")\n",
    "\n",
    "        attn_output, attn_weights = self._new_reverse_attn(query, key, value)\n",
    "\n",
    "        print(f\"OLD {attn_output.norm()=}, {attn_weights.norm()=}\")\n",
    "\n",
    "        if output_attentions:\n",
    "            return attn_output, attn_weights\n",
    "        else:\n",
    "            return attn_output\n",
    "\n",
    "\n",
    "def new_forward(\n",
    "    self,\n",
    "    input_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "    attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    token_type_ids: Optional[torch.LongTensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    head_mask: Optional[torch.FloatTensor] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "    encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    # labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple]:\n",
    "    r\"\"\"\n",
    "    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "        Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "        `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "        are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "    \"\"\"\n",
    "\n",
    "    transformer_outputs = self.transformer(\n",
    "        input_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "        head_mask=head_mask,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        encoder_attention_mask=encoder_attention_mask,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "\n",
    "    print(f\"OLD {input_ids.shape=}, {attention_mask.shape=}\")\n",
    "\n",
    "    hidden_states = transformer_outputs[0]\n",
    "\n",
    "    # print(\"HIDDEN STATE SHAPE\", hidden_states.shape)\n",
    "\n",
    "    # Set device for model parallelism\n",
    "    if self.model_parallel and torch.cuda.is_available():\n",
    "        torch.cuda.set_device(self.transformer.first_device)\n",
    "        hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "    print(hidden_states.shape, encoder_hidden_states.shape)\n",
    "\n",
    "    # lm_logits = self.lm_head(hidden_states)\n",
    "    reverse_attention_output = self.lm_head(\n",
    "        hidden_states, encoder_hidden_states, output_attentions=output_attentions\n",
    "    )\n",
    "\n",
    "    # print(\"REVERSE ATTENTION OUTPUT SHAPE\", reverse_attention_output[0].shape)\n",
    "\n",
    "    return reverse_attention_output\n",
    "\n",
    "\n",
    "def replace_linear_final_layer_with_bespoke_reverse_attention(model):\n",
    "    model.lm_head = Editor_Attention(config=model.config)\n",
    "    model.forward = new_forward.__get__(model, GPT2LMHeadModel)\n",
    "    return\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def add_fwd_hooks(module_hooks):\n",
    "    \"\"\"\n",
    "    Context manager for temporarily adding forward hooks to a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    module_hooks\n",
    "        A list of pairs: (module, fnc) The function will be registered as a\n",
    "            forward hook on the module\n",
    "    \"\"\"\n",
    "    try:\n",
    "        handles = []\n",
    "        for mod, hk in module_hooks:\n",
    "            handles.append(mod.register_forward_hook(hk))\n",
    "        yield\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "\n",
    "def assign_layer_indices(model):\n",
    "    \"\"\"\n",
    "    Assigns a custom attribute 'layer_index' to each transformer layer in the GPT-2 model.\n",
    "    This function iterates over the transformer blocks and assigns an index to each.\n",
    "    \"\"\"\n",
    "    model.transformer.wte.layer_index = 0\n",
    "    for i, layer in enumerate(model.transformer.h):\n",
    "        layer.layer_index = i + 1\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# assign_layer_indices(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.compile #Apparently this fails when used inside jupyter notebooks but is fine if i make dedicated scripts\n",
    "class EditorHypernetwork(nn.Module):\n",
    "    # Separating the editor config file, from its base model's configurations\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_editing_heads=32,\n",
    "        edit_channel_width=768,  # controls dimensionality given to attention heads in the last layer of the editor\n",
    "        use_layerwise_embeddings=True,\n",
    "        chop_editor_at_layer=None,\n",
    "        edit_dampening_factor=0.001,  # tuning parameter to help the edits not be initialized too large\n",
    "        kill_token_zero=False,  # multiplies edits to token pos zero by zero\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Construct Editor Model\n",
    "        # Load the configuration from the YAML file\n",
    "        # with open(editor_yaml_file_path, 'r') as file:\n",
    "        #     self.config = yaml.safe_load(file)\n",
    "        if torch.cuda.is_available():\n",
    "            self.editor_model = (\n",
    "                GPT2LMHeadModel.from_pretrained(\"gpt2\").cuda().eval()\n",
    "            )  # have recently added .cuda() so it uses the gpu\n",
    "        else:\n",
    "            self.editor_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"mps\").eval()\n",
    "\n",
    "        # Add cross-attention to each layer\n",
    "        self.editor_model.config.add_cross_attention = True\n",
    "        self.editor_model.config.num_editing_heads = num_editing_heads\n",
    "        self.editor_model.config.chop_layer = chop_editor_at_layer\n",
    "        self.editor_model.config.kill_token_zero = kill_token_zero\n",
    "        self.editor_model.config.edit_channel_width = edit_channel_width\n",
    "\n",
    "        if chop_editor_at_layer is None:\n",
    "            chop_editor_at_layer = 12\n",
    "\n",
    "        for i in range(chop_editor_at_layer):\n",
    "            add_cross_attention_to_layer(\n",
    "                self.editor_model.transformer.h[i], self.editor_model.config\n",
    "            )\n",
    "\n",
    "        # Delete extra layers beyond the chop_layer\n",
    "        self.editor_model.transformer.h = self.editor_model.transformer.h[\n",
    "            :chop_editor_at_layer\n",
    "        ]\n",
    "\n",
    "        # Replace the final linear layer with special reverse attention output\n",
    "        self.editor_model.lm_head = Editor_Attention(config=self.editor_model.config)\n",
    "        self.editor_model.forward = new_forward.__get__(\n",
    "            self.editor_model, GPT2LMHeadModel\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.editor_model.cuda()\n",
    "        else:\n",
    "            self.editor_model.to(\"mps\")\n",
    "\n",
    "        # Construct Target Model\n",
    "        if torch.cuda.is_available():\n",
    "            self.target_model = (\n",
    "                transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\").cuda().eval()\n",
    "            )\n",
    "        else:\n",
    "            self.target_model = (\n",
    "                transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "                .to(\"mps\")\n",
    "                .eval()\n",
    "            )\n",
    "        for param in self.target_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        assign_layer_indices(self.target_model)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.target_model.cuda()\n",
    "        else:\n",
    "            self.target_model.to(\"mps\")\n",
    "\n",
    "        # Add module for layerwise embeddings\n",
    "        if use_layerwise_embeddings:\n",
    "            self.use_layerwise_embeddings = True\n",
    "            self.layerwise_embeddings = torch.randn(13, 768, requires_grad=True).to(\n",
    "                \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "            )\n",
    "        else:\n",
    "            self.use_layerwise_embeddings = False\n",
    "            self.layerwise_embeddings = 0\n",
    "\n",
    "        self.edit_dampening_factor = edit_dampening_factor\n",
    "\n",
    "        self.residual_cache = None\n",
    "        self.opt = None\n",
    "        self.lossfn = None\n",
    "        self.lam = None\n",
    "        self.penalty_loss = None\n",
    "        self.training_loss = None\n",
    "\n",
    "    # Gets the hidden states from the target model, if necessary\n",
    "    def run_target_model_for_encoded_hidden_states(self, target_ids):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.target_model(target_ids, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            return hidden_states\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        editor_input_ids,\n",
    "        target_input_ids,\n",
    "        target_hidden_states=None,\n",
    "        output_target_hidden_states=False,\n",
    "        output_edited_hidden_states=False,\n",
    "        output_edit_vectors=False,\n",
    "        output_editor_attention=False,\n",
    "        stop_editing_index=None,\n",
    "        batch_edit_vectors=None,\n",
    "    ):\n",
    "        # Run target model for encoded hidden states\n",
    "        if target_hidden_states is None:\n",
    "            target_hidden_states = torch.stack(\n",
    "                self.run_target_model_for_encoded_hidden_states(\n",
    "                    target_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "                ),  # seems to break while we are passing thru batch_size=1; the last (12th =) has different dimensions\n",
    "                dim=2,\n",
    "            )\n",
    "        # dimensions of target_hidden_states:\n",
    "        # batch_size, token_sequence_length, num_layers = 13, resid_width = 768\n",
    "\n",
    "        # If we are stopping editing at stop_editing_index, then we eliminate target_hidden_states beyond that index\n",
    "        if stop_editing_index is not None:\n",
    "            target_hidden_states = target_hidden_states[\n",
    "                :, :stop_editing_index, :, :\n",
    "            ].clone()\n",
    "\n",
    "        # Normalize along the last dimension\n",
    "        normalization_factors = target_hidden_states.norm(dim=-1, keepdim=True)\n",
    "        target_hidden_states = target_hidden_states / normalization_factors\n",
    "\n",
    "        # Error catching:\n",
    "        if batch_edit_vectors is not None:\n",
    "            if output_edit_vectors or output_editor_attention:\n",
    "                return \"Error: Inputting your own batch_edit_vectors means the model does not construct the outputs you are requesting\"\n",
    "\n",
    "        # Run editor model, get edit vectors\n",
    "        if batch_edit_vectors is None:\n",
    "            if self.use_layerwise_embeddings:\n",
    "                # Now, add in the layerwise embeddings\n",
    "                embedded_hidden_states = (\n",
    "                    target_hidden_states + self.layerwise_embeddings[None, None, :, :]\n",
    "                )\n",
    "\n",
    "                collapsed_target_hidden_states = embedded_hidden_states.reshape(\n",
    "                    target_hidden_states.shape[0],\n",
    "                    target_hidden_states.shape[1] * target_hidden_states.shape[2],\n",
    "                    target_hidden_states.shape[3],\n",
    "                )\n",
    "            else:\n",
    "                collapsed_target_hidden_states = target_hidden_states.reshape(\n",
    "                    target_hidden_states.shape[0],\n",
    "                    target_hidden_states.shape[1] * target_hidden_states.shape[2],\n",
    "                    target_hidden_states.shape[3],\n",
    "                )\n",
    "\n",
    "            # print(\"EDITOR INPUT ID SHAPE\", editor_input_ids.shape)\n",
    "            # print(\"TARGET HIDDEN SHAPE\", target_hidden_states.shape)\n",
    "\n",
    "            editor_output = self.editor_model(\n",
    "                editor_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "                encoder_hidden_states=collapsed_target_hidden_states,\n",
    "                output_attentions=output_editor_attention,\n",
    "            )\n",
    "            # Multiply the outputs by normalization factors\n",
    "            if output_editor_attention:\n",
    "                temp_edit_vectors = editor_output[0]\n",
    "                # Might want to reshape this too but whatever\n",
    "                batch_editor_attention = editor_output[1]\n",
    "            else:\n",
    "                temp_edit_vectors = editor_output\n",
    "\n",
    "            # print(\"TEMP EDIT VECTORS SHAPE\", temp_edit_vectors.shape)\n",
    "\n",
    "            # Renormalize to the scale of the target hidden states\n",
    "            # and reshape to proper dimensions\n",
    "            batch_edit_vectors = (\n",
    "                self.edit_dampening_factor\n",
    "                * normalization_factors\n",
    "                * temp_edit_vectors.reshape(\n",
    "                    temp_edit_vectors.shape[0], stop_editing_index, 13, 768\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # If we are stopping editing at stop_editing_index,\n",
    "        # this pads batch_edit_vectors with 0's to the right of the edited positions\n",
    "        if stop_editing_index is not None:\n",
    "            batch_edit_vectors = torch.cat(\n",
    "                (\n",
    "                    batch_edit_vectors,\n",
    "                    torch.zeros(\n",
    "                        batch_edit_vectors.shape[0],\n",
    "                        target_input_ids.shape[1] - stop_editing_index,\n",
    "                        13,\n",
    "                        768,\n",
    "                    ),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        # Run target model with edit vectors. This adds the edit vectors to the given hidden state at the specified batch index, position, and layer\n",
    "        def edit_add(module, input, output):\n",
    "            layer_index = module.layer_index\n",
    "            output[0][:] = output[0] + batch_edit_vectors[:, :, layer_index, :]\n",
    "            if self.editor_model.config.kill_token_zero == True:\n",
    "                output[0][:, 0, :] = 0\n",
    "\n",
    "        def embedding_edit_add(module, input, output):\n",
    "            output[:] = output + batch_edit_vectors[:, :, 0, :]\n",
    "            if self.editor_model.config.kill_token_zero == True:\n",
    "                output[:, 0, :] = 0\n",
    "\n",
    "        # Now editing the target model\n",
    "        hooks1 = [(self.target_model.transformer.wte, embedding_edit_add)]\n",
    "        hooks2 = [(self.target_model.transformer.h[L], edit_add) for L in range(12)]\n",
    "        hooks = hooks1 + hooks2\n",
    "        with add_fwd_hooks(hooks):\n",
    "            # THIS IS THE LINE WHERE THE MODEL IS CALLED (AND THE EDITOR IS CALLED AT\n",
    "            # THE END OF `layer` AS A SIDE EFFECT)\n",
    "            target_result = self.target_model(\n",
    "                target_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "                output_hidden_states=output_edited_hidden_states,\n",
    "            )\n",
    "\n",
    "        logits = target_result.logits\n",
    "\n",
    "        output = {}\n",
    "        output[\"logits\"] = logits\n",
    "        if output_target_hidden_states:\n",
    "            output[\"target_hidden_states\"] = (\n",
    "                target_hidden_states * normalization_factors\n",
    "            )\n",
    "        if output_edited_hidden_states:\n",
    "            output[\"edited_hidden_states\"] = target_result.hidden_states\n",
    "        if output_edit_vectors:\n",
    "            output[\"edit_vectors\"] = batch_edit_vectors\n",
    "        if output_editor_attention:\n",
    "            output[\"editor_attention\"] = batch_editor_attention\n",
    "        return output\n",
    "\n",
    "    # Generate text using the target model, with a new edit application at every step.\n",
    "    # This is a very slow way to generate text.\n",
    "    # If you only want to edit first k tokens, use the forward pass instead with stop_editing_index = k\n",
    "    def inspect_batch_prediction_ouptuts(self, batch):\n",
    "        with torch.no_grad():\n",
    "            batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "            self.editor_inputs = batch[\"tokenized_first_sentence\"][i].unsqueeze(0)\n",
    "            self.target_inputs = batch[\"tokenized_next_50_tokens\"][i].unsqueeze(0)\n",
    "            self.prediction = self.forward(\n",
    "                self.editor_inputs,\n",
    "                self.target_inputs,\n",
    "                stop_editing_index=stop_editing_index,\n",
    "                output_target_hidden_states=False,\n",
    "                output_edited_hidden_states=False,\n",
    "                output_edit_vectors=False,\n",
    "                output_editor_attention=False,\n",
    "            )\n",
    "            # compute most likely tokens from the logits\n",
    "            predicted_ids = [\n",
    "                torch.argmax(pred, dim=-1) for pred in self.prediction[\"logits\"]\n",
    "            ]\n",
    "            # convert the token ids to strings\n",
    "            predicted_strings = [tokenizer.decode(pred) for pred in predicted_ids]\n",
    "            return predicted_strings\n",
    "\n",
    "    def evaluate_KL_test_loss_nogradient(\n",
    "        self, dataloader, f_data_to_soft_labels=None, stop_editing_index=8\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            sum_weighted_losses = 0.0\n",
    "            total_samples = 0\n",
    "            for batch in dataloader:\n",
    "                current_batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "                self.editor_inputs = batch[\"tokenized_first_sentence\"].squeeze(1)\n",
    "                self.target_inputs = batch[\"tokenized_next_50_tokens\"].squeeze(1)\n",
    "                self.prediction = self.forward(  # check the batch size\n",
    "                    self.editor_inputs,\n",
    "                    self.target_inputs,\n",
    "                    stop_editing_index=stop_editing_index,\n",
    "                )\n",
    "                log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                    self.prediction[\"logits\"][:, stop_editing_index:, :].reshape(\n",
    "                        -1, 50257\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "                # Now we must compute the soft labels!\n",
    "                # join the last 50 tokens to the editor inputs\n",
    "                soft_labels = f_data_to_soft_labels(\n",
    "                    batch[\"tokenized_first_sentence\"],\n",
    "                    batch[\"tokenized_next_50_tokens\"],\n",
    "                    num_predictions_max=50,\n",
    "                )\n",
    "                mask = (batch[\"tokenized_next_50_tokens\"] != 50256).reshape(-1)\n",
    "                self.loss = torch.nn.functional.kl_div(\n",
    "                    log_prob_predictions[mask, :],\n",
    "                    soft_labels[mask, :],\n",
    "                    reduction=\"batchmean\",\n",
    "                )\n",
    "                # Weight the loss by current batch size and update the sum of weighted losses\n",
    "                sum_weighted_losses += self.loss.item() * current_batch_size\n",
    "                total_samples += current_batch_size\n",
    "            weighted_average_loss = sum_weighted_losses / total_samples\n",
    "        return weighted_average_loss\n",
    "\n",
    "    def evaluate_crossentropy_test_loss_nogradient(\n",
    "        self, dataloader, stop_editing_index=8\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            sum_weighted_losses = 0.0\n",
    "            total_tokens = 0\n",
    "            for batch in dataloader:\n",
    "                # current_batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "                self.editor_inputs = batch[\"tokenized_first_sentence\"].squeeze(1)\n",
    "                self.target_inputs = batch[\"tokenized_next_50_tokens\"].squeeze(1)\n",
    "                self.prediction = self.forward(  # check the batch size\n",
    "                    self.editor_inputs,\n",
    "                    self.target_inputs,\n",
    "                    stop_editing_index=stop_editing_index,\n",
    "                )\n",
    "                log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                    self.prediction[\"logits\"][:, stop_editing_index:, :].reshape(\n",
    "                        -1, 50257\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "                # Create a mask to exclude padded tokens\n",
    "                target_labels = self.target_inputs[:, stop_editing_index:].reshape(-1)\n",
    "                mask = (\n",
    "                    target_labels != 50256\n",
    "                )  # Assuming padded tokens are represented by 0\n",
    "\n",
    "                # Compute the cross-entropy loss with masking\n",
    "                criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                loss = criterion(log_prob_predictions, target_labels)\n",
    "                current_mask_sum = mask.sum()\n",
    "                loss = (loss * mask).sum() / current_mask_sum\n",
    "\n",
    "                # Weight the loss by current batch size and update the sum of weighted losses\n",
    "                sum_weighted_losses += loss * current_mask_sum\n",
    "                total_tokens += current_mask_sum\n",
    "            weighted_average_loss = sum_weighted_losses / total_tokens\n",
    "        return weighted_average_loss\n",
    "\n",
    "    def run_train(\n",
    "        self,\n",
    "        train_loader,\n",
    "        test_loader=None,\n",
    "        stop_editing_index=8,\n",
    "        epochs=1,\n",
    "        KL_divergence_loss=False,\n",
    "        lam=0,  # 20000\n",
    "        lam_testing_penalty=0,  # 100000\n",
    "        f_data_to_soft_labels=None,\n",
    "        checkpoint_interval=60,  # save checkpoint every 60 minutes\n",
    "    ):\n",
    "        self.opt = optim.AdamW(\n",
    "            self.parameters(), lr=lr, weight_decay=0.01\n",
    "        )  # usually: lr = 5e-5. 1e-3 worked well!\n",
    "\n",
    "        if KL_divergence_loss:\n",
    "            self.lossfn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        else:\n",
    "            self.lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Create a tqdm progress bar\n",
    "            with tqdm(\n",
    "                total=len(train_loader),\n",
    "                desc=f\"Epoch {epoch + 1}/{epochs}\",\n",
    "                unit=\"batch\",\n",
    "                disable=True,\n",
    "            ) as pbar:\n",
    "                num_datapoints_in_epoch = 0\n",
    "                epoch_train_loss = 0\n",
    "                epoch_gradient_norm = 0\n",
    "                # Train loop\n",
    "                batch_index = -1  # index of first batch will be 0\n",
    "\n",
    "                for step, batch in enumerate(\n",
    "                    train_loader\n",
    "                ):  # not sure what this does for fractional batches. meh whatev\n",
    "                    batch_index += 1\n",
    "                    self.batch = batch\n",
    "                    current_batch_size = len(batch[\"tokenized_next_50_tokens\"])\n",
    "                    num_datapoints_in_epoch += current_batch_size\n",
    "                    self.opt.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    self.prediction = self.forward(\n",
    "                        batch[\"tokenized_first_sentence\"],\n",
    "                        batch[\"tokenized_next_50_tokens\"],\n",
    "                        stop_editing_index=stop_editing_index,\n",
    "                        output_target_hidden_states=True,\n",
    "                        output_edited_hidden_states=True,\n",
    "                        output_edit_vectors=True,\n",
    "                    )\n",
    "\n",
    "                    # Compute the penalty (edit size relative to the hidden state)\n",
    "                    self.lam = lam\n",
    "                    edit_ratio = self.prediction[\"edit_vectors\"].norm(dim=-1)[\n",
    "                        :, :stop_editing_index, :\n",
    "                    ] / self.prediction[\"target_hidden_states\"].norm(dim=-1)\n",
    "                    self.per_datapoint_penalty_loss = self.lam * torch.sum(\n",
    "                        edit_ratio, dim=[1, 2]\n",
    "                    )\n",
    "                    self.penalty_loss = torch.mean(self.per_datapoint_penalty_loss)\n",
    "\n",
    "                    # Compute the data loss\n",
    "                    if KL_divergence_loss:\n",
    "                        log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                            self.prediction[\"logits\"][:, :, :].reshape(-1, 50257),\n",
    "                            dim=1,\n",
    "                        )\n",
    "                        # Now we must compute the soft labels! This is outsourced to the user-provided function, teacher_model\n",
    "                        self.soft_labels = f_data_to_soft_labels(\n",
    "                            batch[\"tokenized_first_sentence\"],\n",
    "                            batch[\"tokenized_next_50_tokens\"],\n",
    "                            num_predictions_max=50,\n",
    "                        )\n",
    "                        # check that the mask makes sense!\n",
    "                        mask = (batch[\"tokenized_next_50_tokens\"] != 50256).reshape(-1)\n",
    "                        self.prediction_loss = self.lossfn(\n",
    "                            log_prob_predictions[mask, :], self.soft_labels[mask, :]\n",
    "                        )\n",
    "                        # NOTE: currently I am letting the loss predict on tokens inside the editing window\n",
    "                        # I didn't do this in the previous testing! Nor is it the case in crossentropy\n",
    "\n",
    "                    else:\n",
    "                        log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                            self.prediction[\"logits\"][\n",
    "                                :, stop_editing_index:, :\n",
    "                            ].reshape(-1, 50257),\n",
    "                            dim=1,\n",
    "                        )\n",
    "                        # Create a mask to exclude padded tokens\n",
    "                        # NOTE: here we are disallowing prediction on the first stop_editing_index tokens.\n",
    "                        # Code is currently formatted such that this is not the case for KL\n",
    "                        target_labels = batch[\"tokenized_next_50_tokens\"][\n",
    "                            :, stop_editing_index:\n",
    "                        ].reshape(-1)\n",
    "                        mask = target_labels != 50256\n",
    "                        # Compute the cross-entropy loss with masking\n",
    "                        criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                        loss = criterion(log_prob_predictions, target_labels.long())\n",
    "                        current_mask_sum = mask.sum()\n",
    "                        self.prediction_loss = (loss * mask).sum() / current_mask_sum\n",
    "\n",
    "                    # Compute the total loss and backpropagate\n",
    "                    self.training_loss = self.prediction_loss + self.penalty_loss\n",
    "                    self.training_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(\n",
    "                        self.parameters(), max_grad_clip\n",
    "                    )  # just implemented this! dunno if a cap of 1 to large, so I'm messing with reducing it\n",
    "\n",
    "                    # Check for nan gradients\n",
    "                    # if check_nan_gradients(self):\n",
    "                    #     break\n",
    "\n",
    "                    # Backwards step\n",
    "                    self.opt.step()\n",
    "\n",
    "                    # metrics\n",
    "                    epoch_train_loss += self.training_loss.item() * current_batch_size\n",
    "                    gradients = [\n",
    "                        p.grad.view(-1) for p in self.parameters() if p.grad is not None\n",
    "                    ]\n",
    "                    all_gradients = torch.cat(gradients)\n",
    "                    gradient_norm = torch.norm(all_gradients).item()\n",
    "                    epoch_gradient_norm += gradient_norm * current_batch_size\n",
    "\n",
    "                    metrics = {\n",
    "                        \"step\": step * (epoch + 1),\n",
    "                        \"train_batch_total_loss\": self.training_loss.item(),\n",
    "                        \"train_batch_prediction_loss\": self.prediction_loss.item(),\n",
    "                        \"train_batch_penalty_loss\": self.penalty_loss,\n",
    "                        \"train_batch_gradient_norm\": gradient_norm,\n",
    "                    }\n",
    "\n",
    "                    if wandb.run:\n",
    "                        wandb.log(metrics)\n",
    "                    if step % 100 == 0:\n",
    "                        print(metrics)\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)  # note: this was incorrectly displaying before!\n",
    "\n",
    "                    # Check if it's time to save a checkpoint\n",
    "                    current_time = time.time()\n",
    "                    # first loop initialization\n",
    "                    if batch_index == 0 and epoch == 0:\n",
    "                        last_checkpoint_time = -100000\n",
    "\n",
    "                    if current_time - last_checkpoint_time >= checkpoint_interval * 60:\n",
    "                        # Save the checkpoint\n",
    "                        torch.save(\n",
    "                            self.state_dict(),\n",
    "                            f\"checkpoint_epoch_{epoch}_batch_{pbar.n}.pt\",\n",
    "                        )\n",
    "                        last_checkpoint_time = current_time\n",
    "                        # announce checkpoint save\n",
    "                        print(\"Checkpoint saved at epoch\", epoch, \"batch\", pbar.n)\n",
    "\n",
    "                ####END BATCH LOOP\n",
    "                #########################\n",
    "\n",
    "                # epoch loss\n",
    "                # epoch_test_prediction_loss = self.evaluate_crossentropy_test_loss_nogradient(\n",
    "                #     test_loader,\n",
    "                #     stop_editing_index,\n",
    "                #     batch_size\n",
    "                # )\n",
    "                if KL_divergence_loss:\n",
    "                    epoch_test_prediction_loss = self.evaluate_KL_test_loss_nogradient(\n",
    "                        dataloader=test_loader,\n",
    "                        f_data_to_soft_labels=f_data_to_soft_labels,\n",
    "                        stop_editing_index=stop_editing_index,\n",
    "                    )\n",
    "\n",
    "                # # Calculate and accumulate gradient norm for logging\n",
    "                # gradients = [p.grad.view(-1) for p in self.parameters() if p.grad is not None]\n",
    "                # all_gradients = torch.cat(gradients)\n",
    "                # gradient_norm = torch.norm(all_gradients).item()\n",
    "                # epoch_gradient_norm += gradient_norm\n",
    "\n",
    "                if wandb.run:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"epoch_train_total_loss\": epoch_train_loss\n",
    "                            / num_datapoints_in_epoch,\n",
    "                            \"test_prediction_loss\": epoch_test_prediction_loss,\n",
    "                            \"gradient_norm\": epoch_gradient_norm\n",
    "                            / num_datapoints_in_epoch,\n",
    "                        }\n",
    "                    )\n",
    "            # Save the final model\n",
    "            torch.save(self, \"final_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernetwork = EditorHypernetwork(\n",
    "    edit_dampening_factor=edit_dampening_factor,  # 1/10000,\n",
    "    use_layerwise_embeddings=False,\n",
    "    num_editing_heads=num_editing_heads,\n",
    "    edit_channel_width=editor_channel_width,\n",
    "    chop_editor_at_layer=chop_layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = GPT2Editor(\n",
    "    GPT2EditorConfig(\n",
    "        use_layerwise_embeddings=False,\n",
    "        num_editing_heads=num_editing_heads,\n",
    "        edit_channel_multiply_factor=edit_channel_multiply_factor,\n",
    "        edit_dampening_factor=edit_dampening_factor,\n",
    "        chop_editor_at_layer=chop_layer,\n",
    "    )\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def disable_dropout(model: torch.nn.Module):\n",
    "    \"\"\"Disable dropout in a model.\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.Dropout):\n",
    "            module.p = 0\n",
    "\n",
    "\n",
    "def constant_init(m, val=0.5):\n",
    "    for p in m.parameters():\n",
    "        nn.init.constant_(p.data, val)\n",
    "\n",
    "\n",
    "disable_dropout(hypernetwork.editor_model)\n",
    "disable_dropout(new_model.hypernetwork)\n",
    "\n",
    "init_fn = partial(constant_init, val=0.1)\n",
    "\n",
    "_ = hypernetwork.editor_model.apply(init_fn)\n",
    "_ = new_model.hypernetwork.apply(init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLD input_ids.shape=torch.Size([2, 8]), attention_mask.shape=torch.Size([2, 8])\n",
      "torch.Size([2, 8, 768]) torch.Size([2, 8, 768])\n",
      "OLD editor_hidden_states.norm()=tensor(11.0851, device='cuda:0'), target_hidden_states.norm()=tensor(110.8512, device='cuda:0')\n",
      "OLD self.q_attn.weight.mean()=tensor(0.1000, device='cuda:0'), self.k_attn.weight.mean()=tensor(0.1000, device='cuda:0'), self.v_attn.weight.mean()=tensor(0.1000, device='cuda:0')\n",
      "OLD self.q_attn.weight.shape=torch.Size([768, 768]), self.k_attn.weight.shape=torch.Size([768, 768]), self.v_attn.weight.shape=torch.Size([768, 768])\n",
      "OLD query.norm()=tensor(8524.4561, device='cuda:0'), key.norm()=tensor(304.9125, device='cuda:0'), value.norm()=tensor(304.9125, device='cuda:0')\n",
      "split_query.shape=torch.Size([2, 8, 32, 24]), split_key.shape=torch.Size([2, 32, 24]), split_value.shape=torch.Size([2, 32, 24])\n",
      "KQ_weights.norm()=tensor(66320.2734, device='cuda:0')\n",
      "KQ_weights.shape=torch.Size([2, 32, 8])\n",
      "softmaxed_weights.norm()=tensor(2.8284, device='cuda:0')\n",
      "softmaxed_weights.unsqueeze(-1).shape=torch.Size([2, 32, 8, 1]), split_value.unsqueeze(-2).shape=torch.Size([2, 32, 1, 24])\n",
      "attn_output.norm()=tensor(107.8028, device='cuda:0')\n",
      "OLD attn_output.norm()=tensor(8290.3428, device='cuda:0'), attn_weights.norm()=tensor(2.8284, device='cuda:0')\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "NEW attn_output.norm()=tensor(107.8028, device='cuda:0'), attn_weights.norm()=tensor(2.8284, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_input_ids = torch.zeros(2, 8).long().cuda()\n",
    "test_attention_mask = torch.ones_like(test_input_ids).cuda()\n",
    "encoder_hidden_states = torch.ones(2, 8, 768).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_hn_old = hypernetwork.editor_model(\n",
    "        input_ids=test_input_ids,\n",
    "        attention_mask=test_attention_mask,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "    )\n",
    "    print(\"+\" * 100)\n",
    "    out_hn_new = new_model.hypernetwork(\n",
    "        input_ids=test_input_ids,\n",
    "        attention_mask=test_attention_mask,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 8, 24]) torch.Size([2, 32, 24, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 8, 32, 24)\n",
    "b = torch.randn(2, 32, 24)\n",
    "\n",
    "print(a.permute(0, 2, 1, 3).shape, b.unsqueeze(-1).shape)\n",
    "\n",
    "out = torch.matmul(a.permute(0, 2, 1, 3), b.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_hn_old.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_hn_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         ...,\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880]],\n",
       "\n",
       "        [[74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         ...,\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_hn_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         ...,\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880]],\n",
       "\n",
       "        [[74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         ...,\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880],\n",
       "         [74.7880, 74.7880, 74.7880,  ..., 74.7880, 74.7880, 74.7880]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_hn_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "editor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
