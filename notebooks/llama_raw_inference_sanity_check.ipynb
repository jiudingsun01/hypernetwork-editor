{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 138\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# city_train_set = generate_ravel_dataset(1000, split=\"train\", filtering_dict_paths=[\"./notebooks/ravel_llama-3-8b_city_prompt_to_output_statistics.json\"])\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# city_test_set =  generate_ravel_dataset(100, split=\"test\", filtering_dict_paths=[\"./notebooks/ravel_llama-3-8b_city_prompt_to_output_statistics.json\"])\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m city_train_set \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_ravel_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltering_dict_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m city_test_set \u001b[38;5;241m=\u001b[39m  generate_ravel_dataset(\u001b[38;5;241m100\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, filtering_dict_paths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# city_train_set = load_from_disk(\"./experiment_models_new/city_train_set\")\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# city_test_set = load_from_disk(\"./experiment_models_new/city_test_set\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 50\u001b[0m, in \u001b[0;36mgenerate_ravel_dataset\u001b[0;34m(n_samples, split, domains, domain_excluded_attributes, filtering_dict_paths, seed)\u001b[0m\n\u001b[1;32m     46\u001b[0m dataset \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     48\u001b[0m sample_per_domain \u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(domains)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m domain, excluded_attributes, filtering_dict_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdomains\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain_excluded_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltering_dict_paths\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     52\u001b[0m     templates \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeople in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m usually speak\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is in the country of\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContinent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is in the continent of\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     56\u001b[0m     }\n\u001b[1;32m     58\u001b[0m     prompt_idxs_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m5\u001b[39m],\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContinent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     62\u001b[0m     }\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast, LlamaConfig, LlamaModel\n",
    "import torch\n",
    "from torch import compile\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import contextlib\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/work/frink/models/llama3-8B-HF\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def generate_ravel_dataset(n_samples, split=\"train\", domains=[\"city\"], domain_excluded_attributes=[[\"Latitude\", \"Longitude\", \"Timezone\"]], filtering_dict_paths=[None],  seed=42):\n",
    "            \n",
    "    # Seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    \n",
    "    sample_per_domain = n_samples // len(domains)\n",
    "    \n",
    "    for domain, excluded_attributes, filtering_dict_path in zip(domains, domain_excluded_attributes, filtering_dict_paths):\n",
    "                        \n",
    "        templates = {\n",
    "            \"Language\": [\"People in %s usually speak\"],\n",
    "            \"Country\": [\"%s is in the country of\"],\n",
    "            \"Continent\": [\"%s is in the continent of\"]\n",
    "        }\n",
    "        \n",
    "        prompt_idxs_dict = {\n",
    "            \"Language\": [3],\n",
    "            \"Country\": [5],\n",
    "            \"Continent\": [3]\n",
    "        }\n",
    "        \n",
    "        entities = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_entity_attributes.json\"), \"r\"))\n",
    "        entities_split = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_entity_to_split.json\"), \"r\"))\n",
    "        \n",
    "        all_attributes = [a for a in list(templates.keys()) if a not in excluded_attributes]\n",
    "\n",
    "        entities_train = {k: v for k, v in entities.items() if entities_split[k] == \"train\"}\n",
    "        name_train = list(entities_train.keys())\n",
    "\n",
    "        entities_test = {k: v for k, v in entities.items() if entities_split[k] != \"train\"}\n",
    "        name_test = list(entities_test.keys())\n",
    "        \n",
    "        if split == \"train\":\n",
    "            entity_dict, entity_name = entities_train, name_train\n",
    "        elif split == \"test\":\n",
    "            entity_dict, entity_name = entities_test, name_test\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train' or 'test'\")\n",
    "        \n",
    "        if filtering_dict_path is not None:\n",
    "            filtering_dict = json.load(open(filtering_dict_path, \"r\"))\n",
    "            filtered_key = []\n",
    "            \n",
    "            for entity in filtering_dict.keys():\n",
    "                model_knows = True\n",
    "                \n",
    "                entity_token_len = len(tokenizer(entity)[\"input_ids\"])\n",
    "                \n",
    "                if entity_token_len > 1:\n",
    "                    model_knows = False\n",
    "                \n",
    "                for attribute in all_attributes:                    \n",
    "                    split_template_idx = [list(filtering_dict[entity][attribute].values())[i] for i in prompt_idxs_dict[attribute]]\n",
    "                    if True not in split_template_idx:\n",
    "                        model_knows = False\n",
    "                        break\n",
    "                if not model_knows:\n",
    "                    filtered_key.append(entity)\n",
    "            print(f\"Filtering out {len(filtered_key)} out of {len(filtering_dict)} entities that the model does not know!\")\n",
    "            filtering_dict = {k: v for k, v in filtering_dict.items() if k not in filtered_key}\n",
    "        else:\n",
    "            filtering_dict = None\n",
    "        \n",
    "        for _ in tqdm(range(sample_per_domain)):\n",
    "            \n",
    "            data = {}\n",
    "            \n",
    "            if filtering_dict is None:\n",
    "                source_entity, base_entity = random.sample(entity_name, 2)\n",
    "                attribute = random.choice(all_attributes)\n",
    "                another_attribute = random.choice(all_attributes)\n",
    "                source_entity_dict, base_entity_dict = entity_dict[source_entity], entity_dict[base_entity]\n",
    "                source_template, base_template = random.choice(templates[another_attribute]), random.choice(templates[attribute])\n",
    "            else:\n",
    "                source_entity, base_entity = random.sample([k for k in entity_name if k in filtering_dict.keys()], 2)\n",
    "                attribute = random.choice(all_attributes)\n",
    "                another_attribute = random.choice(all_attributes)\n",
    "                source_entity_dict, base_entity_dict = entity_dict[source_entity], entity_dict[base_entity]\n",
    "                source_template, base_template = random.choice(templates[another_attribute]), random.choice(templates[attribute])\n",
    "                \n",
    "            data[\"input_text\"] = base_template % base_entity\n",
    "            data[\"counterfactual_input_text\"] = source_template % source_entity\n",
    "            data[\"edit_instruction\"] = f\"{base_entity} ; {source_entity} - {attribute}\"\n",
    "            data[\"target\"] = base_entity_dict[attribute]\n",
    "            data[\"counterfactual_target\"] = source_entity_dict[attribute]            \n",
    "                \n",
    "            dataset.append(data)\n",
    "                \n",
    "    dataset = Dataset.from_list(dataset)\n",
    "    return dataset\n",
    "\n",
    "# city_train_set = generate_ravel_dataset(1000, split=\"train\", filtering_dict_paths=[\"./notebooks/ravel_llama-3-8b_city_prompt_to_output_statistics.json\"])\n",
    "# city_test_set =  generate_ravel_dataset(100, split=\"test\", filtering_dict_paths=[\"./notebooks/ravel_llama-3-8b_city_prompt_to_output_statistics.json\"])\n",
    "\n",
    "\n",
    "# city_train_set = generate_ravel_dataset(1000, split=\"train\", filtering_dict_paths=None)\n",
    "# city_test_set =  generate_ravel_dataset(100, split=\"test\", filtering_dict_paths=None)\n",
    "# city_train_set = load_from_disk(\"./experiment_models_new/city_train_set\")\n",
    "# city_test_set = load_from_disk(\"./experiment_models_new/city_test_set\")\n",
    "\n",
    "def ravel_collate_fn(batch):\n",
    "    \n",
    "    def tokenize_text_inputs(texts, counterfactual_texts, target_texts):\n",
    "        \n",
    "        input_texts = [text + \" \" + target for text, target in zip(texts, target_texts)]\n",
    "        input_texts = [text.replace(\" \\\" \", \" \\\" \") for text in input_texts]\n",
    "        \n",
    "        tokenized = tokenizer(input_texts, return_tensors=\"pt\", padding=True, max_length=50, truncation=True)\n",
    "        tokenized_counterfactual = tokenizer(counterfactual_texts, return_tensors=\"pt\", padding=True, max_length=50, truncation=True)\n",
    "        tokenized_labels = []\n",
    "        \n",
    "        for input_ids, input_text in zip(tokenized[\"input_ids\"], texts):\n",
    "            input_length = tokenizer(input_text, return_tensors=\"pt\", padding=False)[\"input_ids\"].shape[-1]\n",
    "            input_length += torch.sum(input_ids == tokenizer.pad_token_id)\n",
    "            \n",
    "            label = torch.full_like(input_ids, -100)\n",
    "            label[input_length:] = input_ids[input_length:]\n",
    "            tokenized_labels.append(label)\n",
    "        \n",
    "        tokenized_labels = torch.stack(tokenized_labels)\n",
    "        return {\n",
    "            \"base_input_ids\": tokenized[\"input_ids\"],\n",
    "            \"base_attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"source_input_ids\": tokenized_counterfactual[\"input_ids\"],\n",
    "            \"source_attention_mask\": tokenized_counterfactual[\"attention_mask\"],\n",
    "            \"labels\": tokenized_labels\n",
    "        }\n",
    "        \n",
    "    prompts, edit_instructions, targets, counterfactual_prompts = [], [], [], []\n",
    "    for b in batch:\n",
    "        prompts.append(b[\"input_text\"])\n",
    "        edit_instructions.append(b[\"edit_instruction\"])\n",
    "        targets.append(b[\"target\"])\n",
    "        counterfactual_prompts.append(b[\"counterfactual_input_text\"])\n",
    "        \n",
    "    editor_input_ids = tokenizer(edit_instructions, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    \n",
    "    returned_dict = {\n",
    "        \"editor_input_ids\": editor_input_ids,\n",
    "        **tokenize_text_inputs(prompts, counterfactual_prompts, targets),\n",
    "    }    \n",
    "    \n",
    "    return returned_dict\n",
    "\n",
    "batch_size = 16  # 50 or so\n",
    "data_loader = DataLoader(\n",
    "    city_train_set, batch_size=batch_size, collate_fn=ravel_collate_fn, shuffle=False\n",
    ")  # batch_size, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(\n",
    "    city_test_set, batch_size=batch_size, collate_fn=ravel_collate_fn, shuffle=False\n",
    ")\n",
    "\n",
    "for batch in data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e966ae661ea44db782f3cb2176067bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @torch.compile #Apparently this fails when used inside jupyter notebooks but is fine if i make dedicated scripts\n",
    "from models.llama3.model import LlamaInterpretor, LlamaInterpretorConfig, RavelInterpretorHypernetwork\n",
    "from models.utils import InterpretorModelOutput\n",
    "from transformers import LlamaForCausalLM\n",
    "import wandb\n",
    "\n",
    "llama = LlamaForCausalLM.from_pretrained(\"/work/frink/models/llama3-8B-HF\")\n",
    "llama = llama.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100, 15506], device='cuda:0')\n",
      " Spanish\n",
      " Spanish\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4606],\n",
      "       device='cuda:0')\n",
      " Europe\n",
      " South\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6498],\n",
      "       device='cuda:0')\n",
      " English\n",
      " English\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100, 10384], device='cuda:0')\n",
      " Africa\n",
      " Africa\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3723, 4273],\n",
      "       device='cuda:0')\n",
      " United States\n",
      " his States\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "        48475,    64], device='cuda:0')\n",
      " Hausa\n",
      " Englisha\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100, 35217], device='cuda:0')\n",
      " Arabic\n",
      " the\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4606],\n",
      "       device='cuda:0')\n",
      " Europe\n",
      " Africa\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4892, 5270],\n",
      "       device='cuda:0')\n",
      " North America\n",
      " Africa America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4892, 5270],\n",
      "       device='cuda:0')\n",
      " North America\n",
      " North America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3723, 4273],\n",
      "       device='cuda:0')\n",
      " United States\n",
      " the States\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3723, 4273],\n",
      "       device='cuda:0')\n",
      " United States\n",
      " the States\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  6498, 38406,  6015,    72, 44754,\n",
      "         1631,   462], device='cuda:0')\n",
      " English,Gaeli,Kymri\n",
      " English.aelic,oreanri\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100, 43130], device='cuda:0')\n",
      " Chad\n",
      " the\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6498],\n",
      "       device='cuda:0')\n",
      " English\n",
      " the\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100, 13936], device='cuda:0')\n",
      " Asia\n",
      " North\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " Europe America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4987, 5270], device='cuda:0')\n",
      " South America\n",
      " Asia America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 3723, 4273], device='cuda:0')\n",
      " United States\n",
      " Portugal States\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " English\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 8524], device='cuda:0')\n",
      " Russia\n",
      " the\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " Africa America\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 32777],\n",
      "       device='cuda:0')\n",
      " Cuba\n",
      " Colombia\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " North America\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 10384],\n",
      "       device='cuda:0')\n",
      " Africa\n",
      " Asia\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 13936],\n",
      "       device='cuda:0')\n",
      " Asia\n",
      " Asia\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 32777],\n",
      "       device='cuda:0')\n",
      " Cuba\n",
      " Colombia\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 27490],\n",
      "       device='cuda:0')\n",
      " Thai\n",
      " the\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 15155],\n",
      "       device='cuda:0')\n",
      " Italian\n",
      " Italian\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 10384],\n",
      "       device='cuda:0')\n",
      " Africa\n",
      " Asia\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " Europe America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " English\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " North America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 4606], device='cuda:0')\n",
      " Europe\n",
      " North\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 3723, 4273], device='cuda:0')\n",
      " United States\n",
      " the States\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 13936],\n",
      "       device='cuda:0')\n",
      " Asia\n",
      " North\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 8690], device='cuda:0')\n",
      " Russian\n",
      " English\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 9822], device='cuda:0')\n",
      " France\n",
      " France\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " Europe America\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 73537],\n",
      "       device='cuda:0')\n",
      " Kazakhstan\n",
      " Israel\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 8690], device='cuda:0')\n",
      " Russian\n",
      " of\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100, 36074, 22506],\n",
      "       device='cuda:0')\n",
      " Kazakh\n",
      " theakh\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 8494], device='cuda:0')\n",
      " Australia\n",
      " origin\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 35217],\n",
      "       device='cuda:0')\n",
      " Arabic\n",
      " the\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 4606], device='cuda:0')\n",
      " Europe\n",
      " Europe\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " in\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 3723, 4273], device='cuda:0')\n",
      " United States\n",
      " his States\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " English\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " South America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " North America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " English\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 8524], device='cuda:0')\n",
      " Russia\n",
      " the\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 27490],\n",
      "       device='cuda:0')\n",
      " Thai\n",
      " the\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 8690], device='cuda:0')\n",
      " Russian\n",
      " English\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " North America\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 10384],\n",
      "       device='cuda:0')\n",
      " Africa\n",
      " Europe\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " North America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " the\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " North America\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  3723, 15422],\n",
      "       device='cuda:0')\n",
      " United Kingdom\n",
      " England Kingdom\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 4606], device='cuda:0')\n",
      " Europe\n",
      " South\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 4606], device='cuda:0')\n",
      " Europe\n",
      " Europe\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100, 36074, 22506],\n",
      "       device='cuda:0')\n",
      " Kazakh\n",
      " theakh\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 3723, 4273], device='cuda:0')\n",
      " United States\n",
      " the States\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4987, 5270], device='cuda:0')\n",
      " South America\n",
      " Asia America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " English\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 4606], device='cuda:0')\n",
      " Europe\n",
      " Europe\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " Africa America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " the\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " North America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 8524], device='cuda:0')\n",
      " Russia\n",
      " the\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 32777],\n",
      "       device='cuda:0')\n",
      " Cuba\n",
      " Colombia\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 48668],\n",
      "       device='cuda:0')\n",
      " Ghana\n",
      " the\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " English\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 8753], device='cuda:0')\n",
      " French\n",
      " French\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 4606], device='cuda:0')\n",
      " Europe\n",
      " North\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 69894],\n",
      "       device='cuda:0')\n",
      " Tanzania\n",
      " the\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4892, 5270], device='cuda:0')\n",
      " North America\n",
      " Africa America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 4606], device='cuda:0')\n",
      " Europe\n",
      " Europe\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 7008], device='cuda:0')\n",
      " Canada\n",
      " the\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 10384],\n",
      "       device='cuda:0')\n",
      " Africa\n",
      " Africa\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 13936],\n",
      "       device='cuda:0')\n",
      " Asia\n",
      " North\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 10384],\n",
      "       device='cuda:0')\n",
      " Africa\n",
      " Asia\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 8753], device='cuda:0')\n",
      " French\n",
      " their\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 39133],\n",
      "       device='cuda:0')\n",
      " Colombia\n",
      " Japan\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 4606], device='cuda:0')\n",
      " Europe\n",
      " North\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 15506],\n",
      "       device='cuda:0')\n",
      " Spanish\n",
      " Japanese\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " the\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 15506],\n",
      "       device='cuda:0')\n",
      " Spanish\n",
      " Japanese\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 13936],\n",
      "       device='cuda:0')\n",
      " Asia\n",
      " Asia\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " English\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 4987, 5270], device='cuda:0')\n",
      " South America\n",
      " Asia America\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " English\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  3723, 15422],\n",
      "       device='cuda:0')\n",
      " United Kingdom\n",
      " England Kingdom\n",
      "\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100, 48668],\n",
      "       device='cuda:0')\n",
      " Ghana\n",
      " Vietnam\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, 8524], device='cuda:0')\n",
      " Russia\n",
      " the\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 5734], device='cuda:0')\n",
      " China\n",
      " origin\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 7008], device='cuda:0')\n",
      " Canada\n",
      " the\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " the\n",
      "\n",
      "tensor([-100, -100, -100, -100, -100, -100, 6498], device='cuda:0')\n",
      " English\n",
      " English\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llama.eval()\n",
    "test_loss = []\n",
    "correct_idxs = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_id, batch in enumerate(test_data_loader):\n",
    "        \n",
    "        predictions = llama.forward(\n",
    "            input_ids=batch[\"base_input_ids\"].to(\"cuda\"),\n",
    "            attention_mask=batch[\"base_attention_mask\"].to(\"cuda\"),\n",
    "            labels=batch[\"labels\"].to(\"cuda\"),\n",
    "            \n",
    "        )\n",
    "        test_loss.append(predictions[\"loss\"].item())\n",
    "        \n",
    "        batch_pred_ids = torch.argmax(predictions[\"logits\"], dim=-1)\n",
    "        \n",
    "        for i, (label, pred_ids) in enumerate(zip(batch[\"labels\"].to(\"cuda\"), batch_pred_ids)):\n",
    "            \n",
    "            print(label)\n",
    "            label_idx = label != -100\n",
    "            output_idx = torch.zeros_like(label_idx)\n",
    "            output_idx[:-1] = label_idx[1:]\n",
    "            \n",
    "            label = label[label_idx]\n",
    "            pred_ids = pred_ids[output_idx]\n",
    "            print(tokenizer.decode(label))\n",
    "            print(tokenizer.decode(pred_ids))\n",
    "            print()\n",
    "            \n",
    "            is_correct = (torch.sum(label == pred_ids) == torch.numel(label)).item()\n",
    "                \n",
    "            correct += is_correct\n",
    "            if is_correct:\n",
    "                correct_idxs.append(batch_id * len(batch[\"labels\"]) + i)\n",
    "            total += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
