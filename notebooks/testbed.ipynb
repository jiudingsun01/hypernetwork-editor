{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out 3 out of 3552 entities that the model does not know!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2781.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out 3 out of 3552 entities that the model does not know!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3163.57it/s]\n"
     ]
    }
   ],
   "source": [
    "stopping_index = 8  # 4090 or A6000\n",
    "num_editing_heads = 16 \n",
    "max_grad_clip = 4.0\n",
    "chop_layer = 6\n",
    "lr = 3e-5\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from torch import compile\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import contextlib\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/work/frink/models/llama3-8B-HF\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "def generate_ravel_dataset(n_samples, split=\"train\", domains=[\"city\"], domain_excluded_attributes=[[\"Latitude\", \"Longitude\", \"Timezone\"]], filtering_dict_paths=[None],  seed=42):\n",
    "            \n",
    "    # Seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    \n",
    "    sample_per_domain = n_samples // len(domains)\n",
    "    \n",
    "    for domain, excluded_attributes, filtering_dict_path in zip(domains, domain_excluded_attributes, filtering_dict_paths):\n",
    "                        \n",
    "        templates = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_attribute_to_prompts.json\"), \"r\"))\n",
    "        entities = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_entity_attributes.json\"), \"r\"))\n",
    "        entities_split = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_entity_to_split.json\"), \"r\"))\n",
    "        templates_split = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_prompt_to_split.json\"), \"r\"))\n",
    "        \n",
    "        all_attributes = [a for a in list(templates.keys()) if a not in excluded_attributes]\n",
    "\n",
    "        templates_train = {k: [v for v in vs if templates_split[v] == \"train\"] for k, vs in templates.items()}\n",
    "        templates_train_idxs = {k: [templates[k].index(v) for v in vs if templates_split[v] == \"train\"] for k, vs in templates.items()}\n",
    "        \n",
    "        templates_test = {k: [v for v in vs if templates_split[v] != \"train\"] for k, vs in templates.items()}\n",
    "        templates_test_idxs = {k: [templates[k].index(v) for v in vs if templates_split[v] != \"train\"] for k, vs in templates.items()}\n",
    "\n",
    "        entities_train = {k: v for k, v in entities.items() if entities_split[k] == \"train\"}\n",
    "        name_train = list(entities_train.keys())\n",
    "\n",
    "        entities_test = {k: v for k, v in entities.items() if entities_split[k] != \"train\"}\n",
    "        name_test = list(entities_test.keys())\n",
    "        \n",
    "        if split == \"train\":\n",
    "            entity_dict, entity_name, prompt_dict, prompt_idxs_dict = entities_train, name_train, templates_train, templates_train_idxs\n",
    "        elif split == \"test\":\n",
    "            entity_dict, entity_name, prompt_dict, prompt_idxs_dict = entities_test, name_test, templates_test, templates_test_idxs\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train' or 'test'\")\n",
    "        \n",
    "        if filtering_dict_path is not None:\n",
    "            filtering_dict = json.load(open(filtering_dict_path, \"r\"))\n",
    "            filtered_key = []\n",
    "            \n",
    "            for entity in filtering_dict.keys():\n",
    "                model_knows = True\n",
    "                for attribute in all_attributes:                    \n",
    "                    split_template_idx = [list(filtering_dict[entity][attribute].values())[i] for i in prompt_idxs_dict[attribute]]\n",
    "                    if True not in split_template_idx:\n",
    "                        model_knows = False\n",
    "                        break\n",
    "                if not model_knows:\n",
    "                    filtered_key.append(entity)\n",
    "            print(f\"Filtering out {len(filtered_key)} out of {len(filtering_dict)} entities that the model does not know!\")\n",
    "            filtering_dict = {k: v for k, v in filtering_dict.items() if k not in filtered_key}\n",
    "        else:\n",
    "            filtering_dict = None\n",
    "        \n",
    "        for _ in tqdm(range(sample_per_domain)):\n",
    "            \n",
    "            data = {}\n",
    "            \n",
    "            if filtering_dict is None:\n",
    "                source_entity, base_entity = random.sample(entity_name, 2)\n",
    "                attribute = random.choice(all_attributes)\n",
    "                frozen_attributes = [k for k in all_attributes if k != attribute]\n",
    "                source_entity_dict, base_entity_dict = entity_dict[source_entity], entity_dict[base_entity]\n",
    "                source_template, base_template = random.choice(prompt_dict[attribute]), random.choice(prompt_dict[attribute])\n",
    "            else:\n",
    "                source_entity, base_entity = random.sample([k for k in entity_name if k in filtering_dict.keys()], 2)\n",
    "                attribute = random.choice(all_attributes)\n",
    "                frozen_attributes = [k for k in all_attributes if k != attribute]\n",
    "                source_entity_dict, base_entity_dict = entity_dict[source_entity], entity_dict[base_entity]\n",
    "                source_template_idxs = [i for i in range(len(filtering_dict[source_entity][attribute])) if filtering_dict[source_entity][attribute][str(i)] == True]\n",
    "                source_template_idxs = [prompt_idxs_dict[attribute].index(i) for i in source_template_idxs if i in prompt_idxs_dict[attribute]]\n",
    "                source_template = random.choice([prompt_dict[attribute][i] for i in source_template_idxs])\n",
    "                base_template_idxs = [i for i in range(len(filtering_dict[base_entity][attribute])) if filtering_dict[base_entity][attribute][str(i)] == True]\n",
    "                base_template_idxs = [prompt_idxs_dict[attribute].index(i) for i in base_template_idxs if i in prompt_idxs_dict[attribute]]\n",
    "                base_template = random.choice([prompt_dict[attribute][i] for i in base_template_idxs])\n",
    "                \n",
    "            data[\"input_text\"] = base_template % base_entity\n",
    "            data[\"counterfactual_input_text\"] = source_template % source_entity\n",
    "            data[\"edit_instruction\"] = f\"{base_entity} ; {source_entity} - {attribute}\"\n",
    "            data[\"target\"] = base_entity_dict[attribute]\n",
    "            data[\"counterfactual_target\"] = source_entity_dict[attribute]\n",
    "            \n",
    "            data[\"unaffected_attributes\"] = []\n",
    "            \n",
    "            for frozen_attribute in frozen_attributes:\n",
    "                \n",
    "                if filtering_dict is None:\n",
    "                    source_attribute_template, base_attribute_template = random.choice(prompt_dict[frozen_attribute]), random.choice(prompt_dict[frozen_attribute])\n",
    "                else:\n",
    "                    source_attribute_idxs = [i for i in range(len(filtering_dict[source_entity][frozen_attribute])) if filtering_dict[source_entity][frozen_attribute][str(i)] == True]\n",
    "                    source_attribute_idxs = [prompt_idxs_dict[frozen_attribute].index(i) for i in source_attribute_idxs if i in prompt_idxs_dict[frozen_attribute]]\n",
    "                    source_attribute_template = random.choice([prompt_dict[frozen_attribute][i] for i in source_attribute_idxs])\n",
    "                    base_attribute_idxs = [i for i in range(len(filtering_dict[base_entity][frozen_attribute])) if filtering_dict[base_entity][frozen_attribute][str(i)] == True]\n",
    "                    base_attribute_idxs = [prompt_idxs_dict[frozen_attribute].index(i) for i in base_attribute_idxs if i in prompt_idxs_dict[frozen_attribute]]\n",
    "                    base_attribute_template = random.choice([prompt_dict[frozen_attribute][i] for i in base_attribute_idxs])\n",
    "                    \n",
    "                base_prompt = base_attribute_template % base_entity\n",
    "                counterfactual_prompt = source_attribute_template % source_entity\n",
    "                \n",
    "                target = base_entity_dict[frozen_attribute]\n",
    "                counterfactual_target = source_entity_dict[frozen_attribute]\n",
    "                \n",
    "                data[\"unaffected_attributes\"].append(\n",
    "                    {\n",
    "                        \"input_text\": base_prompt,\n",
    "                        \"counterfactual_input_text\": counterfactual_prompt,\n",
    "                        \"edit_instruction\": f\"{base_entity} ; {source_entity} - {attribute}\",\n",
    "                        \"target\": target,\n",
    "                        \"counterfactual_target\": counterfactual_target,\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "            dataset.append(data)\n",
    "                \n",
    "    dataset = Dataset.from_list(dataset)\n",
    "    return dataset\n",
    "\n",
    "city_train_set = generate_ravel_dataset(10000, split=\"train\", filtering_dict_paths=[\"./notebooks/ravel_llama-3-8b_city_prompt_to_output_statistics.json\"])\n",
    "city_test_set =  generate_ravel_dataset(1000, split=\"test\", filtering_dict_paths=[\"./notebooks/ravel_llama-3-8b_city_prompt_to_output_statistics.json\"])\n",
    "\n",
    "def ravel_collate_fn(batch):\n",
    "    \n",
    "    def tokenize_text_inputs(texts, counterfactual_texts, target_texts):\n",
    "        \n",
    "        input_texts = [text + \" \" + target for text, target in zip(texts, target_texts)]\n",
    "        input_texts = [text.replace(\" \\\" \", \" \\\" \") for text in input_texts]\n",
    "        \n",
    "        tokenized = tokenizer(input_texts, return_tensors=\"pt\", padding=True, max_length=50, truncation=True)\n",
    "        tokenized_counterfactual = tokenizer(counterfactual_texts, return_tensors=\"pt\", padding=True, max_length=50, truncation=True)\n",
    "        tokenized_labels = []\n",
    "        \n",
    "        for input_ids, input_text in zip(tokenized[\"input_ids\"], texts):\n",
    "            input_length = tokenizer(input_text, return_tensors=\"pt\", padding=False)[\"input_ids\"].shape[-1]\n",
    "            label = torch.full_like(input_ids, -100)\n",
    "            label[input_length:] = input_ids[input_length:]\n",
    "            label[input_ids == tokenizer.pad_token_id] = -100\n",
    "            tokenized_labels.append(label)\n",
    "        \n",
    "        tokenized_labels = torch.stack(tokenized_labels)\n",
    "        return {\n",
    "            \"base_input_ids\": tokenized[\"input_ids\"],\n",
    "            \"base_attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"source_input_ids\": tokenized_counterfactual[\"input_ids\"],\n",
    "            \"source_attention_mask\": tokenized_counterfactual[\"attention_mask\"],\n",
    "            \"labels\": tokenized_labels\n",
    "        }\n",
    "        \n",
    "    prompts, edit_instructions, targets, unaffected_attributes, counterfactual_prompts = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        prompts.append(b[\"input_text\"])\n",
    "        edit_instructions.append(b[\"edit_instruction\"])\n",
    "        targets.append(b[\"counterfactual_target\"])\n",
    "        unaffected_attributes.append(b[\"unaffected_attributes\"])\n",
    "        counterfactual_prompts.append(b[\"counterfactual_input_text\"])\n",
    "        \n",
    "        \n",
    "    editor_input_ids = tokenizer(edit_instructions, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    \n",
    "    returned_dict = {\n",
    "        \"editor_input_ids\": editor_input_ids,\n",
    "        **tokenize_text_inputs(prompts, counterfactual_prompts, targets),\n",
    "    }\n",
    "    \n",
    "    base_prompts_unaffected, counterfactual_prompts_unaffected, targets_unaffected, edit_instructions_unaffected, instance_indices = [], [], [], [], []\n",
    "    \n",
    "    for i, attribute_list in enumerate(unaffected_attributes):\n",
    "        \n",
    "        for d in attribute_list:\n",
    "            base_prompts_unaffected.append(d[\"input_text\"])\n",
    "            targets_unaffected.append(d[\"target\"])\n",
    "            counterfactual_prompts_unaffected.append(d[\"counterfactual_input_text\"])\n",
    "                    \n",
    "        for _ in range(len(attribute_list)):\n",
    "            edit_instructions_unaffected.append(editor_input_ids[i])\n",
    "            instance_indices.append(i)\n",
    "        \n",
    "    edit_instructions_unaffected = torch.stack(edit_instructions_unaffected)\n",
    "    instance_indices = torch.tensor(instance_indices)\n",
    "    \n",
    "    assert len(base_prompts_unaffected) == len(targets_unaffected)\n",
    "    \n",
    "    tokenized_unaffected = tokenize_text_inputs(base_prompts_unaffected, counterfactual_prompts_unaffected, targets_unaffected)\n",
    "    \n",
    "    returned_dict[\"editor_input_ids_unaffected\"] = edit_instructions_unaffected\n",
    "    returned_dict[\"base_input_ids_unaffected\"] = tokenized_unaffected[\"base_input_ids\"]\n",
    "    returned_dict[\"base_attention_mask_unaffected\"] = tokenized_unaffected[\"base_attention_mask\"]\n",
    "    returned_dict[\"source_input_ids_unaffected\"] = tokenized_unaffected[\"source_input_ids\"]\n",
    "    returned_dict[\"source_attention_mask_unaffected\"] = tokenized_unaffected[\"source_attention_mask\"]\n",
    "    returned_dict[\"labels_unaffected\"] = tokenized_unaffected[\"labels\"]\n",
    "    returned_dict[\"instance_indices\"] = instance_indices\n",
    "    \n",
    "    return returned_dict\n",
    "\n",
    "batch_size = 32  # 50 or so\n",
    "data_loader = DataLoader(\n",
    "    city_train_set, batch_size=batch_size, collate_fn=ravel_collate_fn, shuffle=True\n",
    ")  # batch_size, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(\n",
    "    city_test_set, batch_size=batch_size, collate_fn=ravel_collate_fn, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from models.llama3.model import LlamaInterpretor, LlamaInterpretorConfig\n",
    "from models.utils import EditorModelOutput\n",
    "\n",
    "editor_config = LlamaInterpretorConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v0.1\")\n",
    "\n",
    "editor_config.name_or_path = \"TinyLlama/TinyLlama-1.1B-Chat-v0.1\"\n",
    "editor_config.torch_dtype = torch.float16\n",
    "editor_config.num_editing_heads = 16\n",
    "editor_config.chop_editor_at_layer = 10\n",
    "editor_config.default_intervention_layer = 10\n",
    "editor_config._attn_implementation = 'eager'\n",
    "\n",
    "editor_inner = LlamaInterpretor(editor_config)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(editor_config.name_or_path)\n",
    "editor_inner = editor_inner.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters = []\n",
    "for name, param in editor_inner.named_parameters():\n",
    "    if \"target_model\" not in name:\n",
    "        trainable_parameters.append(param)\n",
    "        \n",
    "optimzer = optim.SGD(trainable_parameters, lr=lr, weight_decay=0.01)  # usually: lr = 5e-5. 1e-3 worked well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "for batch in data_loader:\n",
    "    break\n",
    "\n",
    "editor_input_ids = batch[\"editor_input_ids\"].cuda()\n",
    "base_input_ids = batch[\"base_input_ids\"].cuda()\n",
    "base_attention_mask = batch[\"base_attention_mask\"].cuda()\n",
    "source_input_ids = batch[\"source_input_ids\"].cuda()\n",
    "source_attention_mask = batch[\"source_attention_mask\"].cuda()\n",
    "labels = batch[\"labels\"].cuda()\n",
    "editor_attention_mask=editor_input_ids != editor_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred: EditorModelOutput = editor_inner(\n",
    "    editor_input_ids=editor_input_ids,\n",
    "    editor_attention_mask=editor_attention_mask,\n",
    "    base_input_ids=base_input_ids,\n",
    "    base_attention_mask=base_attention_mask,\n",
    "    source_input_ids=source_input_ids,\n",
    "    source_attention_mask=source_attention_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "    _pred.logits.reshape(-1, _pred.logits.shape[-1]),\n",
    "    dim=1,\n",
    ")\n",
    "\n",
    "labels = labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indices = labels != -100\n",
    "log_prob_predictions = log_prob_predictions[label_indices, :]\n",
    "labels = labels[label_indices]\n",
    "\n",
    "# Compute the cross-entropy loss with masking\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "loss = criterion(log_prob_predictions, labels.long())\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.backward()\n",
    "\"\"\"nn.utils.clip_grad_norm_(\n",
    "    editor_inner.parameters(), max_grad_clip\n",
    ")\"\"\"\n",
    "optimzer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0012, device='cuda:0', dtype=torch.float16,\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor(0., device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor_inner.hypernetwork.model.layers[0].self_attn.q_proj.weight[0, 0], editor_inner.hypernetwork.model.layers[0].self_attn.q_proj.weight.grad[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
