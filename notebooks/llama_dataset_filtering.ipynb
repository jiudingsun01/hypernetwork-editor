{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out 1751 out of 1787 entities that the model does not know!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 171916.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out 1724 out of 1765 entities that the model does not know!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 197872.53it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast, LlamaConfig, LlamaModel\n",
    "import torch\n",
    "from torch import compile\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import contextlib\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/work/frink/models/llama3-8B-HF\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def generate_ravel_dataset(n_samples, split=\"train\", domains=[\"city\"], domain_excluded_attributes=[[\"Latitude\", \"Longitude\", \"Timezone\"]], filtering_dict_paths=[None],  seed=42):\n",
    "            \n",
    "    # Seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    \n",
    "    sample_per_domain = n_samples // len(domains)\n",
    "    \n",
    "    for domain, excluded_attributes in zip(domains, domain_excluded_attributes):\n",
    "                        \n",
    "        templates = {\n",
    "            \"Language\": [\"People in %s usually speak\"],\n",
    "            \"Country\": [\"%s is in the country of\"],\n",
    "            \"Continent\": [\"%s is in the continent of\"]\n",
    "        }\n",
    "        \n",
    "        prompt_idxs_dict = {\n",
    "            \"Language\": [3],\n",
    "            \"Country\": [5],\n",
    "            \"Continent\": [3]\n",
    "        }\n",
    "        \n",
    "        entities = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_entity_attributes.json\"), \"r\"))\n",
    "        entities_split = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_entity_to_split.json\"), \"r\"))\n",
    "        \n",
    "        all_attributes = [a for a in list(templates.keys()) if a not in excluded_attributes]\n",
    "\n",
    "        entities_train = {k: v for k, v in entities.items() if entities_split[k] == \"train\"}\n",
    "        name_train = list(entities_train.keys())\n",
    "\n",
    "        entities_test = {k: v for k, v in entities.items() if entities_split[k] != \"train\"}\n",
    "        name_test = list(entities_test.keys())\n",
    "        \n",
    "        if split == \"train\":\n",
    "            entity_dict, entity_name = entities_train, name_train\n",
    "        elif split == \"test\":\n",
    "            entity_dict, entity_name = entities_test, name_test\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train' or 'test'\")\n",
    "        \n",
    "        \n",
    "        filtered_key = []\n",
    "        for entity in entity_dict.keys():\n",
    "            model_knows = True\n",
    "            \n",
    "            entity_token_len = len(tokenizer(entity)[\"input_ids\"])\n",
    "            \n",
    "            if entity_token_len > 1:\n",
    "                filtered_key.append(entity)\n",
    "            \n",
    "            if not model_knows:\n",
    "                filtered_key.append(entity)\n",
    "        \n",
    "        print(f\"Filtering out {len(filtered_key)} out of {len(entity_dict)} entities that the model does not know!\")\n",
    "        filtering_dict = {k: v for k, v in entity_dict.items() if k not in filtered_key}\n",
    "        entity_name = [n for n in entity_name if n not in filtered_key]\n",
    "        \n",
    "        for _ in tqdm(range(sample_per_domain)):\n",
    "            \n",
    "            data = {}\n",
    "            \n",
    "            source_entity, base_entity = random.sample(entity_name, 2)\n",
    "            attribute = random.choice(all_attributes)\n",
    "            another_attribute = random.choice(all_attributes)\n",
    "            source_entity_dict, base_entity_dict = entity_dict[source_entity], entity_dict[base_entity]\n",
    "            source_template, base_template = random.choice(templates[another_attribute]), random.choice(templates[attribute])\n",
    "        \n",
    "                \n",
    "            data[\"input_text\"] = base_template % base_entity\n",
    "            data[\"counterfactual_input_text\"] = source_template % source_entity\n",
    "            data[\"entity\"] = base_entity\n",
    "            data[\"edit_instruction\"] = f\"{base_entity} ; {source_entity} - {attribute}\"\n",
    "            data[\"counterfactual_entity\"] = source_entity\n",
    "            data[\"target\"] = base_entity_dict[attribute]\n",
    "            data[\"counterfactual_target\"] = source_entity_dict[attribute]  \n",
    "            data[\"counterfactual_input_text_with_same_template\"] = base_template % source_entity\n",
    "                \n",
    "            dataset.append(data)\n",
    "                \n",
    "    dataset = Dataset.from_list(dataset)\n",
    "    return dataset\n",
    "\n",
    "city_train_set = generate_ravel_dataset(10000, split=\"train\", filtering_dict_paths=None)\n",
    "city_test_set =  generate_ravel_dataset(1000, split=\"test\", filtering_dict_paths=None)\n",
    "\n",
    "\n",
    "# city_train_set = generate_ravel_dataset(1000, split=\"train\", filtering_dict_paths=None)\n",
    "# city_test_set =  generate_ravel_dataset(100, split=\"test\", filtering_dict_paths=None)\n",
    "# city_train_set = load_from_disk(\"./experiment_models_new/city_train_set\")\n",
    "# city_test_set = load_from_disk(\"./experiment_models_new/city_test_set\")\n",
    "\n",
    "def ravel_collate_fn(batch):\n",
    "    \n",
    "    def tokenize_text_inputs(texts, counterfactual_texts, target_texts):\n",
    "        \n",
    "        input_texts = [text + \" \" + target for text, target in zip(texts, target_texts)]\n",
    "        input_texts = [text.replace(\" \\\" \", \" \\\" \") for text in input_texts]\n",
    "        \n",
    "        tokenized = tokenizer(input_texts, return_tensors=\"pt\", padding=True, max_length=50, truncation=True)\n",
    "        tokenized_counterfactual = tokenizer(counterfactual_texts, return_tensors=\"pt\", padding=True, max_length=50, truncation=True)\n",
    "        tokenized_labels = []\n",
    "        \n",
    "        for input_ids, input_text in zip(tokenized[\"input_ids\"], texts):\n",
    "            input_length = tokenizer(input_text, return_tensors=\"pt\", padding=False)[\"input_ids\"].shape[-1]\n",
    "            input_length += torch.sum(input_ids == tokenizer.pad_token_id)\n",
    "            \n",
    "            label = torch.full_like(input_ids, -100)\n",
    "            label[input_length:] = input_ids[input_length:]\n",
    "            tokenized_labels.append(label)\n",
    "        \n",
    "        tokenized_labels = torch.stack(tokenized_labels)\n",
    "        return {\n",
    "            \"base_input_ids\": tokenized[\"input_ids\"],\n",
    "            \"base_attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"source_input_ids\": tokenized_counterfactual[\"input_ids\"],\n",
    "            \"source_attention_mask\": tokenized_counterfactual[\"attention_mask\"],\n",
    "            \"labels\": tokenized_labels\n",
    "        }\n",
    "        \n",
    "    prompts, edit_instructions, targets, counterfactual_prompts = [], [], [], []\n",
    "    for b in batch:\n",
    "        prompts.append(b[\"counterfactual_input_text_with_same_template\"])\n",
    "        edit_instructions.append(b[\"edit_instruction\"])\n",
    "        targets.append(b[\"counterfactual_target\"])\n",
    "        counterfactual_prompts.append(b[\"counterfactual_input_text\"])\n",
    "        \n",
    "    editor_input_ids = tokenizer(edit_instructions, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    \n",
    "    returned_dict = {\n",
    "        \"editor_input_ids\": editor_input_ids,\n",
    "        **tokenize_text_inputs(prompts, counterfactual_prompts, targets),\n",
    "    }    \n",
    "    \n",
    "    return returned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289fd93ef27f4e80821f4e6b4a665437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# @torch.compile #Apparently this fails when used inside jupyter notebooks but is fine if i make dedicated scripts\n",
    "from models.llama3.model import LlamaInterpretor, LlamaInterpretorConfig, RavelInterpretorHypernetwork\n",
    "from models.utils import InterpretorModelOutput\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "llama = LlamaForCausalLM.from_pretrained(\"/work/frink/models/llama3-8B-HF\")\n",
    "llama = llama.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(model, dataset):\n",
    "\n",
    "    batch_size = 16  # 50 or so\n",
    "    data_loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, collate_fn=ravel_collate_fn, shuffle=False\n",
    "    )  # batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    model.eval()\n",
    "    correct_idxs = set()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch in enumerate(data_loader):\n",
    "            \n",
    "            predictions = model.forward(\n",
    "                input_ids=batch[\"base_input_ids\"].to(\"cuda\"),\n",
    "                attention_mask=batch[\"base_attention_mask\"].to(\"cuda\"),\n",
    "                labels=batch[\"labels\"].to(\"cuda\"),\n",
    "            )\n",
    "            \n",
    "            batch_pred_ids = torch.argmax(predictions[\"logits\"], dim=-1)\n",
    "            \n",
    "            for i, (label, pred_ids) in enumerate(zip(batch[\"labels\"].to(\"cuda\"), batch_pred_ids)):\n",
    "                            \n",
    "                label_idx = label != -100\n",
    "                output_idx = torch.zeros_like(label_idx)\n",
    "                output_idx[:-1] = label_idx[1:]\n",
    "                \n",
    "                label = label[label_idx]\n",
    "                pred_ids = pred_ids[output_idx]\n",
    "                is_correct = (torch.sum(label == pred_ids) == torch.numel(label)).item()\n",
    "                    \n",
    "                correct += is_correct\n",
    "                if is_correct:\n",
    "                    correct_idxs.add(batch_id * len(batch[\"labels\"]) + i)\n",
    "                total += 1\n",
    "                \n",
    "    print(f\"Accuracy: {correct / total}\")\n",
    "    new_dataset = dataset.select(correct_idxs)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3742\n",
      "Accuracy: 0.331\n"
     ]
    }
   ],
   "source": [
    "city_train_set = filter_dataset(llama, city_train_set)\n",
    "city_test_set = filter_dataset(llama, city_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 3742)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(city_test_set), len(city_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37975e3ac8ab4296aa1dec0948abf200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b943bf33d548f5a64616828bc68c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/330 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "city_train_set.save_to_disk(\"./data/city_train_set\")\n",
    "city_test_set.save_to_disk(\"./data/city_test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
