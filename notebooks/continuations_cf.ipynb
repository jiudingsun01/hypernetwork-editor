{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../assets/data/counterfact.json\"\n",
    "# load the json file\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'case_id': 0,\n",
       " 'pararel_idx': 2796,\n",
       " 'requested_rewrite': {'prompt': 'The mother tongue of {} is',\n",
       "  'relation_id': 'P103',\n",
       "  'target_new': {'str': 'English', 'id': 'Q1860'},\n",
       "  'target_true': {'str': 'French', 'id': 'Q150'},\n",
       "  'subject': 'Danielle Darrieux'},\n",
       " 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native',\n",
       "  'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'],\n",
       " 'neighborhood_prompts': ['The mother tongue of Léon Blum is',\n",
       "  'The native language of Montesquieu is',\n",
       "  'François Bayrou, a native',\n",
       "  'The native language of Raymond Barre is',\n",
       "  'Michel Rocard is a native speaker of',\n",
       "  'Jacques Chaban-Delmas is a native speaker of',\n",
       "  'The native language of François Bayrou is',\n",
       "  'Maurice Genevoix, speaker of',\n",
       "  'The mother tongue of François Bayrou is',\n",
       "  'Melchior de Vogüé, speaker of'],\n",
       " 'attribute_prompts': ['J.\\xa0R.\\xa0R. Tolkien is a native speaker of',\n",
       "  'The mother tongue of Douglas Adams is',\n",
       "  'The mother tongue of Paul McCartney is',\n",
       "  'Elvis Presley is a native speaker of',\n",
       "  'Barack Obama, speaker of',\n",
       "  'Douglas Adams, speaker of',\n",
       "  'Meryl Streep, a native',\n",
       "  'George Orwell spoke the language',\n",
       "  'George Washington, a native',\n",
       "  'Michael Jackson, a native'],\n",
       " 'generation_prompts': [\"Danielle Darrieux's mother tongue is\",\n",
       "  'Where Danielle Darrieux is from, people speak the language of',\n",
       "  \"Danielle Darrieux's mother tongue is\",\n",
       "  'Danielle Darrieux was born in',\n",
       "  \"Danielle Darrieux's mother tongue is\",\n",
       "  \"Danielle Darrieux's mother tongue is\",\n",
       "  'Danielle Darrieux was born in',\n",
       "  'Where Danielle Darrieux is from, people speak the language of',\n",
       "  'Danielle Darrieux was born in',\n",
       "  'Danielle Darrieux was born in']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['case_id', 'pararel_idx', 'requested_rewrite', 'paraphrase_prompts', 'neighborhood_prompts', 'attribute_prompts', 'generation_prompts'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = datasets.Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['case_id', 'pararel_idx', 'requested_rewrite', 'paraphrase_prompts', 'neighborhood_prompts', 'attribute_prompts', 'generation_prompts'],\n",
       "    num_rows: 21919\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 16:18:11 config.py:1130] Casting torch.float32 to torch.float16.\n",
      "INFO 06-17 16:18:11 config.py:1151] Downcasting torch.float32 to torch.float16.\n",
      "INFO 06-17 16:18:11 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='gpt2', speculative_config=None, tokenizer='gpt2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=gpt2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sid/miniconda3/envs/editor/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 16:18:12 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "INFO 06-17 16:18:12 weight_utils.py:250] No model.safetensors.index.json found in remote.\n",
      "INFO 06-17 16:18:13 model_runner.py:146] Loading model weights took 0.2378 GB\n",
      "INFO 06-17 16:18:14 gpu_executor.py:83] # GPU blocks: 127995, # CPU blocks: 7281\n",
      "INFO 06-17 16:18:15 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-17 16:18:15 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-17 16:18:20 model_runner.py:924] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "model = LLM(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  8.89it/s, Generation Speed: 447.44 toks/s]\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(\"hello\", sampling_params=SamplingParams(max_tokens=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(batch):\n",
    "    continuations = []\n",
    "    # flatten generation prompts\n",
    "    generation_prompts = []\n",
    "    for gen_list in batch[\"generation_prompts\"]:\n",
    "        generation_prompts.extend(gen_list)\n",
    "    continuations = model.generate(\n",
    "        generation_prompts,\n",
    "        sampling_params=SamplingParams(max_tokens=50, temperature=0.7, top_k=50),\n",
    "    )\n",
    "    idx = 0\n",
    "    reshaped_continuations = []\n",
    "    for gen_list in batch[\"generation_prompts\"]:\n",
    "        chunk = [\n",
    "            prompt + out.outputs[0].text\n",
    "            for prompt, out in zip(\n",
    "                generation_prompts[idx : idx + len(gen_list)],\n",
    "                continuations[idx : idx + len(gen_list)],\n",
    "            )\n",
    "        ]\n",
    "        reshaped_continuations.append(chunk)\n",
    "        idx += len(gen_list)\n",
    "    batch[\"generation_continuations\"] = reshaped_continuations\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9e4e5ca9274b32ba859e6b33299ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 10000/10000 [01:05<00:00, 151.72it/s, Generation Speed: 7410.65 toks/s]\n"
     ]
    }
   ],
   "source": [
    "processed_subset = hf_dataset.select(range(1000)).map(process_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['case_id', 'pararel_idx', 'requested_rewrite', 'paraphrase_prompts', 'neighborhood_prompts', 'attribute_prompts', 'generation_prompts', 'generation_continuations'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf09130bc6b400bb53dc4bd526839ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_subset.save_to_disk(\"../assets/data/processed_counterfact_1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "editor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
