{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run\n",
    "download_wikipedia.ipynb\n",
    "to get the file\n",
    "wikipedia_three_sentences.csv\n",
    "\n",
    "Then, we do our processing here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_grad_clip = 4.0\n",
    "lr = 3e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes from Sid: SAE vs principal components\n",
    "-you could try to work on the SAE's???\n",
    "-get discrete targets\n",
    "[I wouldn't be surprised at all if this helped a lot]\n",
    "\n",
    "-Next deliverables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gx58g3dp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7e5e4474384aecba48d61026782586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁▁▁▁▁▁</td></tr><tr><td>test_average</td><td>▇█▂█▁▃</td></tr><tr><td>train_batch_gradient_norm</td><td>▆▃▄▄▄▃▅▅▅▃▃▄▃▃▅▃▂▁▃▇▄▁▇▅▂▄▂█▄▄▃▄▁▂▂▃▁▁▂▁</td></tr><tr><td>train_batch_prediction_loss</td><td>▄▅▅▇▅▇▅▅▆▄▄▅▆▅▆▆▃▆▃▂▅▆▁▆▂▂▅▂▄▅▄▅▇▄▄█▃▁▆▃</td></tr><tr><td>train_batch_total_loss</td><td>▄▅▅▇▅▇▅▅▆▄▄▅▆▅▆▆▃▆▃▂▅▆▁▆▂▂▅▂▄▅▄▅▇▄▄█▃▁▆▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>274</td></tr><tr><td>test_accuracy</td><td>0.0</td></tr><tr><td>test_average</td><td>11.29454</td></tr><tr><td>train_batch_gradient_norm</td><td>0.1615</td></tr><tr><td>train_batch_prediction_loss</td><td>11.12634</td></tr><tr><td>train_batch_total_loss</td><td>11.12634</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-tree-58</strong> at: <a href='https://wandb.ai/jiudingsun/hypernetworks-interp/runs/gx58g3dp' target=\"_blank\">https://wandb.ai/jiudingsun/hypernetworks-interp/runs/gx58g3dp</a><br/> View project at: <a href='https://wandb.ai/jiudingsun/hypernetworks-interp' target=\"_blank\">https://wandb.ai/jiudingsun/hypernetworks-interp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240708_174338-gx58g3dp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gx58g3dp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c73e2e2378f4c30b97cd7d9f4fb0379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112243878758615, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/frink/sun.jiu/hypernetwork-editor/wandb/run-20240708_175317-zje15uyz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiudingsun/hypernetworks-interp/runs/zje15uyz' target=\"_blank\">youthful-terrain-59</a></strong> to <a href='https://wandb.ai/jiudingsun/hypernetworks-interp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiudingsun/hypernetworks-interp' target=\"_blank\">https://wandb.ai/jiudingsun/hypernetworks-interp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiudingsun/hypernetworks-interp/runs/zje15uyz' target=\"_blank\">https://wandb.ai/jiudingsun/hypernetworks-interp/runs/zje15uyz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jiudingsun/hypernetworks-interp/runs/zje15uyz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa455e72ec0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Trying out the editor hypernetwork on the dune dataset\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"hypernetworks-interp\",\n",
    "    config={\"targetmodel\": \"llama3-8b\", \"editormodel\": \"llama3-8b\", \"dataset\": \"ravel\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f708c1316b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# torch.set_default_device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import compile\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we stick on the \"reverse attention\" module at the end!\n",
    "This is a customized attention head that reads from the editor model, and writes to the target model's activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/work/frink/models/llama3-8B-HF\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out 3 out of 3552 entities that the model does not know!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 3171.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out 3 out of 3552 entities that the model does not know!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3224.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "def generate_ravel_dataset(n_samples, split=\"train\", domains=[\"city\"], domain_excluded_attributes=[[\"Latitude\", \"Longitude\", \"Timezone\"]], filtering_dict_paths=[None],  seed=42):\n",
    "            \n",
    "    # Seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    \n",
    "    sample_per_domain = n_samples // len(domains)\n",
    "    \n",
    "    for domain, excluded_attributes, filtering_dict_path in zip(domains, domain_excluded_attributes, filtering_dict_paths):\n",
    "                        \n",
    "        templates = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_attribute_to_prompts.json\"), \"r\"))\n",
    "        entities = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_entity_attributes.json\"), \"r\"))\n",
    "        entities_split = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_entity_to_split.json\"), \"r\"))\n",
    "        templates_split = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_prompt_to_split.json\"), \"r\"))\n",
    "        \n",
    "        all_attributes = [a for a in list(templates.keys()) if a not in excluded_attributes]\n",
    "\n",
    "        templates_train = {k: [v for v in vs if templates_split[v] == \"train\"] for k, vs in templates.items()}\n",
    "        templates_train_idxs = {k: [templates[k].index(v) for v in vs if templates_split[v] == \"train\"] for k, vs in templates.items()}\n",
    "        \n",
    "        templates_test = {k: [v for v in vs if templates_split[v] != \"train\"] for k, vs in templates.items()}\n",
    "        templates_test_idxs = {k: [templates[k].index(v) for v in vs if templates_split[v] != \"train\"] for k, vs in templates.items()}\n",
    "\n",
    "        entities_train = {k: v for k, v in entities.items() if entities_split[k] == \"train\"}\n",
    "        name_train = list(entities_train.keys())\n",
    "\n",
    "        entities_test = {k: v for k, v in entities.items() if entities_split[k] != \"train\"}\n",
    "        name_test = list(entities_test.keys())\n",
    "        \n",
    "        if split == \"train\":\n",
    "            entity_dict, entity_name, prompt_dict, prompt_idxs_dict = entities_train, name_train, templates_train, templates_train_idxs\n",
    "        elif split == \"test\":\n",
    "            entity_dict, entity_name, prompt_dict, prompt_idxs_dict = entities_test, name_test, templates_test, templates_test_idxs\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train' or 'test'\")\n",
    "        \n",
    "        if filtering_dict_path is not None:\n",
    "            filtering_dict = json.load(open(filtering_dict_path, \"r\"))\n",
    "            filtered_key = []\n",
    "            \n",
    "            for entity in filtering_dict.keys():\n",
    "                model_knows = True\n",
    "                for attribute in all_attributes:                    \n",
    "                    split_template_idx = [list(filtering_dict[entity][attribute].values())[i] for i in prompt_idxs_dict[attribute]]\n",
    "                    if True not in split_template_idx:\n",
    "                        model_knows = False\n",
    "                        break\n",
    "                if not model_knows:\n",
    "                    filtered_key.append(entity)\n",
    "            print(f\"Filtering out {len(filtered_key)} out of {len(filtering_dict)} entities that the model does not know!\")\n",
    "            filtering_dict = {k: v for k, v in filtering_dict.items() if k not in filtered_key}\n",
    "        else:\n",
    "            filtering_dict = None\n",
    "        \n",
    "        for _ in tqdm(range(sample_per_domain)):\n",
    "            \n",
    "            data = {}\n",
    "            \n",
    "            if filtering_dict is None:\n",
    "                source_entity, base_entity = random.sample(entity_name, 2)\n",
    "                attribute = random.choice(all_attributes)\n",
    "                frozen_attributes = [k for k in all_attributes if k != attribute]\n",
    "                source_entity_dict, base_entity_dict = entity_dict[source_entity], entity_dict[base_entity]\n",
    "                source_template, base_template = random.choice(prompt_dict[attribute]), random.choice(prompt_dict[attribute])\n",
    "            else:\n",
    "                source_entity, base_entity = random.sample([k for k in entity_name if k in filtering_dict.keys()], 2)\n",
    "                attribute = random.choice(all_attributes)\n",
    "                frozen_attributes = [k for k in all_attributes if k != attribute]\n",
    "                source_entity_dict, base_entity_dict = entity_dict[source_entity], entity_dict[base_entity]\n",
    "                source_template_idxs = [i for i in range(len(filtering_dict[source_entity][attribute])) if filtering_dict[source_entity][attribute][str(i)] == True]\n",
    "                source_template_idxs = [prompt_idxs_dict[attribute].index(i) for i in source_template_idxs if i in prompt_idxs_dict[attribute]]\n",
    "                source_template = random.choice([prompt_dict[attribute][i] for i in source_template_idxs])\n",
    "                base_template_idxs = [i for i in range(len(filtering_dict[base_entity][attribute])) if filtering_dict[base_entity][attribute][str(i)] == True]\n",
    "                base_template_idxs = [prompt_idxs_dict[attribute].index(i) for i in base_template_idxs if i in prompt_idxs_dict[attribute]]\n",
    "                base_template = random.choice([prompt_dict[attribute][i] for i in base_template_idxs])\n",
    "                \n",
    "            data[\"input_text\"] = base_template % base_entity\n",
    "            data[\"counterfactual_input_text\"] = source_template % source_entity\n",
    "            data[\"edit_instruction\"] = f\"{base_entity} ; {source_entity} - {attribute}\"\n",
    "            data[\"target\"] = base_entity_dict[attribute]\n",
    "            data[\"counterfactual_target\"] = source_entity_dict[attribute]\n",
    "            \n",
    "            data[\"unaffected_attributes\"] = []\n",
    "            \n",
    "            for frozen_attribute in frozen_attributes:\n",
    "                \n",
    "                if filtering_dict is None:\n",
    "                    source_attribute_template, base_attribute_template = random.choice(prompt_dict[frozen_attribute]), random.choice(prompt_dict[frozen_attribute])\n",
    "                else:\n",
    "                    source_attribute_idxs = [i for i in range(len(filtering_dict[source_entity][frozen_attribute])) if filtering_dict[source_entity][frozen_attribute][str(i)] == True]\n",
    "                    source_attribute_idxs = [prompt_idxs_dict[frozen_attribute].index(i) for i in source_attribute_idxs if i in prompt_idxs_dict[frozen_attribute]]\n",
    "                    source_attribute_template = random.choice([prompt_dict[frozen_attribute][i] for i in source_attribute_idxs])\n",
    "                    base_attribute_idxs = [i for i in range(len(filtering_dict[base_entity][frozen_attribute])) if filtering_dict[base_entity][frozen_attribute][str(i)] == True]\n",
    "                    base_attribute_idxs = [prompt_idxs_dict[frozen_attribute].index(i) for i in base_attribute_idxs if i in prompt_idxs_dict[frozen_attribute]]\n",
    "                    base_attribute_template = random.choice([prompt_dict[frozen_attribute][i] for i in base_attribute_idxs])\n",
    "                    \n",
    "                base_prompt = base_attribute_template % base_entity\n",
    "                counterfactual_prompt = source_attribute_template % source_entity\n",
    "                \n",
    "                target = base_entity_dict[frozen_attribute]\n",
    "                counterfactual_target = source_entity_dict[frozen_attribute]\n",
    "                \n",
    "                data[\"unaffected_attributes\"].append(\n",
    "                    {\n",
    "                        \"input_text\": base_prompt,\n",
    "                        \"counterfactual_input_text\": counterfactual_prompt,\n",
    "                        \"edit_instruction\": f\"{base_entity} ; {source_entity} - {attribute}\",\n",
    "                        \"target\": target,\n",
    "                        \"counterfactual_target\": counterfactual_target,\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "            dataset.append(data)\n",
    "                \n",
    "    dataset = Dataset.from_list(dataset)\n",
    "    return dataset\n",
    "\n",
    "city_train_set = generate_ravel_dataset(10000, split=\"train\", filtering_dict_paths=[\"./notebooks/ravel_llama-3-8b_city_prompt_to_output_statistics.json\"])\n",
    "city_test_set =  generate_ravel_dataset(1000, split=\"test\", filtering_dict_paths=[\"./notebooks/ravel_llama-3-8b_city_prompt_to_output_statistics.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ravel_collate_fn(batch):\n",
    "    \n",
    "    def tokenize_text_inputs(texts, counterfactual_texts, target_texts):\n",
    "        \n",
    "        input_texts = [text + \" \" + target for text, target in zip(texts, target_texts)]\n",
    "        input_texts = [text.replace(\" \\\" \", \" \\\" \") for text in input_texts]\n",
    "        \n",
    "        tokenized = tokenizer(input_texts, return_tensors=\"pt\", padding=True, max_length=50, truncation=True)\n",
    "        tokenized_counterfactual = tokenizer(counterfactual_texts, return_tensors=\"pt\", padding=True, max_length=50, truncation=True)\n",
    "        tokenized_labels = []\n",
    "        \n",
    "        for input_ids, input_text in zip(tokenized[\"input_ids\"], texts):\n",
    "            input_length = tokenizer(input_text, return_tensors=\"pt\", padding=False)[\"input_ids\"].shape[-1]\n",
    "            label = torch.full_like(input_ids, -100)\n",
    "            label[input_length:] = input_ids[input_length:]\n",
    "            label[input_ids == tokenizer.pad_token_id] = -100\n",
    "            tokenized_labels.append(label)\n",
    "        \n",
    "        tokenized_labels = torch.stack(tokenized_labels)\n",
    "        return {\n",
    "            \"base_input_ids\": tokenized[\"input_ids\"],\n",
    "            \"base_attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"source_input_ids\": tokenized_counterfactual[\"input_ids\"],\n",
    "            \"source_attention_mask\": tokenized_counterfactual[\"attention_mask\"],\n",
    "            \"labels\": tokenized_labels\n",
    "        }\n",
    "        \n",
    "    prompts, edit_instructions, targets, unaffected_attributes, counterfactual_prompts = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        prompts.append(b[\"input_text\"])\n",
    "        edit_instructions.append(b[\"edit_instruction\"])\n",
    "        targets.append(b[\"counterfactual_target\"])\n",
    "        unaffected_attributes.append(b[\"unaffected_attributes\"])\n",
    "        counterfactual_prompts.append(b[\"counterfactual_input_text\"])\n",
    "        \n",
    "        \n",
    "    editor_input_ids = tokenizer(edit_instructions, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    \n",
    "    returned_dict = {\n",
    "        \"editor_input_ids\": editor_input_ids,\n",
    "        **tokenize_text_inputs(prompts, counterfactual_prompts, targets),\n",
    "    }\n",
    "    \n",
    "    base_prompts_unaffected, counterfactual_prompts_unaffected, targets_unaffected, edit_instructions_unaffected, instance_indices = [], [], [], [], []\n",
    "    \n",
    "    for i, attribute_list in enumerate(unaffected_attributes):\n",
    "        \n",
    "        for d in attribute_list:\n",
    "            base_prompts_unaffected.append(d[\"input_text\"])\n",
    "            targets_unaffected.append(d[\"target\"])\n",
    "            counterfactual_prompts_unaffected.append(d[\"counterfactual_input_text\"])\n",
    "                    \n",
    "        for _ in range(len(attribute_list)):\n",
    "            edit_instructions_unaffected.append(editor_input_ids[i])\n",
    "            instance_indices.append(i)\n",
    "        \n",
    "    edit_instructions_unaffected = torch.stack(edit_instructions_unaffected)\n",
    "    instance_indices = torch.tensor(instance_indices)\n",
    "    \n",
    "    assert len(base_prompts_unaffected) == len(targets_unaffected)\n",
    "    \n",
    "    tokenized_unaffected = tokenize_text_inputs(base_prompts_unaffected, counterfactual_prompts_unaffected, targets_unaffected)\n",
    "    \n",
    "    returned_dict[\"editor_input_ids_unaffected\"] = edit_instructions_unaffected\n",
    "    returned_dict[\"base_input_ids_unaffected\"] = tokenized_unaffected[\"base_input_ids\"]\n",
    "    returned_dict[\"base_attention_mask_unaffected\"] = tokenized_unaffected[\"base_attention_mask\"]\n",
    "    returned_dict[\"source_input_ids_unaffected\"] = tokenized_unaffected[\"source_input_ids\"]\n",
    "    returned_dict[\"source_attention_mask_unaffected\"] = tokenized_unaffected[\"source_attention_mask\"]\n",
    "    returned_dict[\"labels_unaffected\"] = tokenized_unaffected[\"labels\"]\n",
    "    returned_dict[\"instance_indices\"] = instance_indices\n",
    "    \n",
    "    return returned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # 50 or so\n",
    "data_loader = DataLoader(\n",
    "    city_train_set, batch_size=batch_size, collate_fn=ravel_collate_fn, shuffle=True\n",
    ")  # batch_size, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(\n",
    "    city_test_set, batch_size=batch_size, collate_fn=ravel_collate_fn, shuffle=True\n",
    ")\n",
    "\n",
    "for batch in data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.compile #Apparently this fails when used inside jupyter notebooks but is fine if i make dedicated scripts\n",
    "from models.llama3.model import LlamaInterpretor, LlamaInterpretorConfig\n",
    "from models.utils import EditorModelOutput\n",
    "\n",
    "class RavelInterpretorHypernetwork(nn.Module):\n",
    "    # Separating the editor config file, from its base model's configurations\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path=\"/work/frink/models/llama3-8B-HF\",\n",
    "        num_editing_heads=32,\n",
    "        chop_editor_at_layer=8,\n",
    "        intervention_layer=10,\n",
    "        torch_dtype=torch.float16\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.editor_config = LlamaInterpretorConfig.from_pretrained(model_name_or_path)\n",
    "        self.editor_config.name_or_path = model_name_or_path\n",
    "        self.editor_config.torch_dtype = torch_dtype\n",
    "        self.editor_config.num_editing_heads = num_editing_heads\n",
    "        self.editor_config.chop_editor_at_layer = chop_editor_at_layer\n",
    "        self.editor_config.default_intervention_layer = intervention_layer\n",
    "        self.editor_config._attn_implementation = 'eager'\n",
    "                \n",
    "        self.editor_inner = LlamaInterpretor(self.editor_config)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.residual_cache = None\n",
    "        self.opt = None\n",
    "        self.training_loss = None\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        editor_input_ids: torch.Tensor = None,\n",
    "        base_input_ids: torch.Tensor = None,\n",
    "        base_attention_mask: torch.Tensor = None,\n",
    "        source_input_ids: torch.Tensor = None,\n",
    "        source_attention_mask: torch.Tensor = None,\n",
    "        labels: torch.Tensor = None,\n",
    "    ):\n",
    "        _pred: EditorModelOutput = self.editor_inner(\n",
    "            editor_input_ids=editor_input_ids,\n",
    "            editor_attention_mask=editor_input_ids != self.editor_config.eos_token_id,\n",
    "            base_input_ids=base_input_ids,\n",
    "            base_attention_mask=base_attention_mask,\n",
    "            source_input_ids=source_input_ids,\n",
    "            source_attention_mask=source_attention_mask,\n",
    "            # output_target_hidden_states=True,\n",
    "        )\n",
    "        \n",
    "        if labels is None:\n",
    "            return {\n",
    "                \"logits\": _pred.logits,\n",
    "                \"target_hidden_states\": _pred.target_hidden_states,\n",
    "            }\n",
    "        else:\n",
    "            log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                _pred.logits.reshape(-1, _pred.logits.shape[-1]),\n",
    "                dim=1,\n",
    "            )\n",
    "            \n",
    "            labels = labels.reshape(-1)\n",
    "            assert labels.shape == log_prob_predictions.shape[:-1]\n",
    "            \n",
    "            # Only consider the tokens that are not -100 in target_labels\n",
    "            label_indices = labels != -100\n",
    "            \n",
    "            log_prob_predictions = log_prob_predictions[label_indices, :]\n",
    "            labels = labels[label_indices]\n",
    "            \n",
    "            # Compute the cross-entropy loss with masking\n",
    "            criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "            loss = criterion(log_prob_predictions, labels.long())\n",
    "            \n",
    "            return {\n",
    "                \"logits\": _pred.logits,\n",
    "                \"target_hidden_states\": _pred.target_hidden_states,\n",
    "                \"loss\": loss,\n",
    "            }\n",
    "        \n",
    "    \n",
    "    # Generate text using the target model, with a new edit application at every step.\n",
    "    # This is a very slow way to generate text.\n",
    "    # If you only want to edit first k tokens, use the forward pass instead with stop_editing_index = k\n",
    "    def inspect_batch_prediction_ouptuts(self, batch):\n",
    "        self.editor_inner.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            predictions = self.forward(\n",
    "                editor_input_ids=batch[\"editor_input_ids\"].to(\"cuda\"),\n",
    "                base_input_ids=batch[\"base_input_ids\"].to(\"cuda\"),\n",
    "                base_attention_mask=batch[\"base_attention_mask\"].to(\"cuda\"),\n",
    "                source_input_ids=batch[\"source_input_ids\"].to(\"cuda\"),\n",
    "                source_attention_mask=batch[\"source_attention_mask\"].to(\"cuda\"),\n",
    "            )\n",
    "            \n",
    "            batch_pred_ids = torch.argmax(predictions[\"logits\"], dim=-1)\n",
    "            batch_full_output = self.tokenizer.batch_decode(batch_pred_ids, skip_special_tokens=True)\n",
    "            \n",
    "            batch_output = []\n",
    "            correct = 0\n",
    "            \n",
    "            for label, pred_ids in zip(batch[\"labels\"].to(\"cuda\"), batch_pred_ids):\n",
    "                \n",
    "                output_idx = label != -100\n",
    "                label = label[output_idx]\n",
    "                pred_ids = pred_ids[output_idx]\n",
    "                batch_output.append(\n",
    "                    self.tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "                )             \n",
    "                \n",
    "                correct += torch.sum(label == pred_ids) == torch.numel(label)\n",
    "            \n",
    "        return {\n",
    "            \"batch_output\": batch_output,\n",
    "            \"batch_full_output\": batch_full_output,\n",
    "            \"n_correct\": correct,\n",
    "        }\n",
    "        \n",
    "    def evaluate_intervention(self, editor_input_text, base_prompt, source_prompt, gen_len=1):\n",
    "        self.editor_inner.eval()\n",
    "        \n",
    "        editor_input_ids = self.tokenizer(editor_input_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        source_input_ids = self.tokenizer(source_prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        base_input_ids = self.tokenizer(base_prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = self.forward(\n",
    "                editor_input_ids=editor_input_ids.to(\"cuda\"),\n",
    "                base_input_ids=base_input_ids.to(\"cuda\"),\n",
    "                source_input_ids=source_input_ids.to(\"cuda\"),\n",
    "            )\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def eval_accuracy(self, test_loader, use_unaffected=False):\n",
    "        \n",
    "        self.editor_inner.eval()\n",
    "        test_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                predictions = self.forward(\n",
    "                    editor_input_ids=batch[\"editor_input_ids\"].to(\"cuda\"),\n",
    "                    base_input_ids=batch[\"base_input_ids\"].to(\"cuda\"),\n",
    "                    base_attention_mask=batch[\"base_attention_mask\"].to(\"cuda\"),\n",
    "                    source_input_ids=batch[\"source_input_ids\"].to(\"cuda\"),\n",
    "                    source_attention_mask=batch[\"source_attention_mask\"].to(\"cuda\"),\n",
    "                    labels=batch[\"labels\"].to(\"cuda\"),\n",
    "                )\n",
    "                test_loss.append(predictions[\"loss\"].item())\n",
    "                \n",
    "                batch_pred_ids = torch.argmax(predictions[\"logits\"], dim=-1)\n",
    "                \n",
    "                if use_unaffected:\n",
    "                    unaffected_prediction = self.forward(\n",
    "                        editor_input_ids=batch[\"editor_input_ids_unaffected\"].to(\"cuda\"),\n",
    "                        base_input_ids=batch[\"base_input_ids_unaffected\"].to(\"cuda\"),\n",
    "                        base_attention_mask=batch[\"base_attention_mask_unaffected\"].to(\"cuda\"),\n",
    "                        source_input_ids=batch[\"source_input_ids_unaffected\".to(\"cuda\")],\n",
    "                        source_attention_mask=batch[\"source_attention_mask_unaffected\"].to(\"cuda\"),\n",
    "                        labels=batch[\"labels_unaffected\"].to(\"cuda\"),\n",
    "                    )\n",
    "                    batch_pred_ids_unaffected = torch.argmax(unaffected_prediction[\"logits\"], dim=-1)\n",
    "                    \n",
    "                    correct_unaffected = []\n",
    "                    \n",
    "                    for label, pred_ids in zip(batch[\"labels_unaffected\"].to(\"cuda\"), batch_pred_ids_unaffected):\n",
    "                        output_idx = label != -100\n",
    "                        label = label[output_idx]\n",
    "                        pred_ids = pred_ids[output_idx]\n",
    "                        correct_unaffected.append((torch.sum(label == pred_ids) == torch.numel(label)))\n",
    "                    \n",
    "                    correct_unaffected = torch.stack(correct_unaffected)\n",
    "                    \n",
    "                    test_loss[-1] += unaffected_prediction[\"loss\"].item()\n",
    "                \n",
    "                for i, (label, pred_ids) in enumerate(zip(batch[\"labels\"].to(\"cuda\"), batch_pred_ids)):\n",
    "                    output_idx = label != -100\n",
    "                    label = label[output_idx]\n",
    "                    pred_ids = pred_ids[output_idx]\n",
    "                    \n",
    "                    is_correct = (torch.sum(label == pred_ids) == torch.numel(label)).item()\n",
    "                    if use_unaffected:\n",
    "                        # indices of the position which value is i\n",
    "                        instance_idx = torch.nonzero(batch[\"instance_indices\"].to(\"cuda\") == i).squeeze()\n",
    "                        # check if all the unaffected positions were correct\n",
    "                        # if not, set is_correct to False\n",
    "                        is_correct = is_correct and all(correct_unaffected[instance_idx])\n",
    "                        \n",
    "                    correct += is_correct\n",
    "                    total += 1\n",
    "                    \n",
    "        return correct / total, sum(test_loss) / len(test_loss)\n",
    "             \n",
    "\n",
    "    def run_train(\n",
    "        self,\n",
    "        train_loader,\n",
    "        test_loader=None,\n",
    "        epochs=1,\n",
    "        eval_per_steps: int = None,\n",
    "        use_unaffected = False\n",
    "    ):\n",
    "        trainable_parameters = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"target_model\" not in name:\n",
    "                trainable_parameters.append(param)\n",
    "                \n",
    "        self.opt = optim.SGD(trainable_parameters, lr=lr, weight_decay=0.01)  # usually: lr = 5e-5. 1e-3 worked well!\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Create a tqdm progress bar\n",
    "            with tqdm(\n",
    "                total=len(train_loader),\n",
    "                desc=f\"Epoch {epoch + 1}/{epochs}\",\n",
    "                unit=\"batch\",\n",
    "                disable=True,\n",
    "            ) as pbar:\n",
    "                num_datapoints_in_epoch = 0\n",
    "                epoch_train_loss = 0\n",
    "                epoch_gradient_norm = 0\n",
    "                # Train loop\n",
    "                batch_index = -1  # index of first batch will be 0\n",
    "\n",
    "                for step, batch in enumerate(\n",
    "                    train_loader\n",
    "                ):  \n",
    "                    if step % eval_per_steps == 0:\n",
    "                        # Evaluate the model\n",
    "                        accuracy, test_loss = self.eval_accuracy(\n",
    "                            test_loader, use_unaffected=use_unaffected\n",
    "                        )\n",
    "                        \n",
    "                        if wandb.run:\n",
    "                            wandb.log(\n",
    "                                {\n",
    "                                    \"test_average\": test_loss,\n",
    "                                    \"test_accuracy\": accuracy,\n",
    "                                }\n",
    "                            )\n",
    "                        \n",
    "                    batch_index += 1\n",
    "                    self.batch = batch\n",
    "                    current_batch_size = len(batch[\"editor_input_ids\"])\n",
    "                    num_datapoints_in_epoch += current_batch_size\n",
    "                    self.opt.zero_grad()\n",
    "\n",
    "                    self.prediction = self.forward(\n",
    "                        editor_input_ids=batch[\"editor_input_ids\"].to(\"cuda\"),\n",
    "                        base_input_ids=batch[\"base_input_ids\"].to(\"cuda\"),\n",
    "                        base_attention_mask=batch[\"base_attention_mask\"].to(\"cuda\"),\n",
    "                        source_input_ids=batch[\"source_input_ids\"].to(\"cuda\"),\n",
    "                        source_attention_mask=batch[\"source_attention_mask\"].to(\"cuda\"),\n",
    "                        labels=batch[\"labels\"].to(\"cuda\"),\n",
    "                    )\n",
    "\n",
    "                    self.prediction_loss = self.prediction[\"loss\"]\n",
    "                    \n",
    "                    if use_unaffected:\n",
    "                        self.prediction_loss += self.forward(\n",
    "                            editor_input_ids=batch[\"editor_input_ids_unaffected\"].to(\"cuda\"),\n",
    "                            input_ids=batch[\"input_ids_unaffected\"].to(\"cuda\"),\n",
    "                            attention_mask=batch[\"attention_mask_unaffected\"].to(\"cuda\"),\n",
    "                            labels=batch[\"labels_unaffected\"].to(\"cuda\"),\n",
    "                        )[\"loss\"]\n",
    "\n",
    "                    # Compute the total loss and backpropagate\n",
    "                    self.training_loss = self.prediction_loss\n",
    "                    self.training_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(\n",
    "                        self.parameters(), max_grad_clip\n",
    "                    )  # just implemented this! dunno if a cap of 1 to large, so I'm messing with reducing it\n",
    "\n",
    "                    # Check for nan gradients\n",
    "                    # if check_nan_gradients(self):\n",
    "                    #     break\n",
    "\n",
    "                    # Backwards step\n",
    "                    self.opt.step()\n",
    "\n",
    "                    # metrics\n",
    "                    epoch_train_loss += self.training_loss.item() * current_batch_size\n",
    "                    gradients = [\n",
    "                        p.grad.view(-1) for p in self.parameters() if p.grad is not None\n",
    "                    ]\n",
    "                    all_gradients = torch.cat(gradients)\n",
    "                    gradient_norm = torch.norm(all_gradients).item()\n",
    "                    epoch_gradient_norm += gradient_norm * current_batch_size\n",
    "\n",
    "                    metrics = {\n",
    "                        \"step\": step * (epoch + 1),\n",
    "                        \"train_batch_total_loss\": self.training_loss.item(),\n",
    "                        \"train_batch_prediction_loss\": self.prediction_loss.item(),\n",
    "                        \"train_batch_gradient_norm\": gradient_norm,\n",
    "                    }\n",
    "\n",
    "                    if wandb.run:\n",
    "                        wandb.log(metrics)\n",
    "                    if step % 5 == 0:\n",
    "                        print(metrics)\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)  # note: this was incorrectly displaying before!\n",
    "\n",
    "                    # Check if it's time to save a checkpoint\n",
    "                    current_time = time.time()\n",
    "                    # first loop initialization\n",
    "                    if batch_index == 0 and epoch == 0:\n",
    "                        last_checkpoint_time = -100000\n",
    "\n",
    "                if wandb.run:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"epoch_train_total_loss\": epoch_train_loss\n",
    "                            / num_datapoints_in_epoch,\n",
    "                            \"gradient_norm\": epoch_gradient_norm\n",
    "                            / num_datapoints_in_epoch,\n",
    "                        }\n",
    "                    )\n",
    "            # Save the final model\n",
    "            torch.save(self.state_dict(), \"final_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "hypernetwork = RavelInterpretorHypernetwork()\n",
    "hypernetwork = hypernetwork.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step': 0, 'train_batch_total_loss': 10.989103317260742, 'train_batch_prediction_loss': 10.989103317260742, 'train_batch_gradient_norm': 0.10186767578125}\n",
      "{'step': 5, 'train_batch_total_loss': 11.157174110412598, 'train_batch_prediction_loss': 11.157174110412598, 'train_batch_gradient_norm': 0.1614990234375}\n",
      "{'step': 10, 'train_batch_total_loss': 11.133227348327637, 'train_batch_prediction_loss': 11.133227348327637, 'train_batch_gradient_norm': 0.11865234375}\n",
      "{'step': 15, 'train_batch_total_loss': 11.147483825683594, 'train_batch_prediction_loss': 11.147483825683594, 'train_batch_gradient_norm': 0.13525390625}\n",
      "{'step': 20, 'train_batch_total_loss': 11.47998046875, 'train_batch_prediction_loss': 11.47998046875, 'train_batch_gradient_norm': 0.146484375}\n",
      "{'step': 25, 'train_batch_total_loss': 11.171156883239746, 'train_batch_prediction_loss': 11.171156883239746, 'train_batch_gradient_norm': 0.11248779296875}\n",
      "{'step': 30, 'train_batch_total_loss': 11.014498710632324, 'train_batch_prediction_loss': 11.014498710632324, 'train_batch_gradient_norm': 0.12420654296875}\n",
      "{'step': 35, 'train_batch_total_loss': 11.34976863861084, 'train_batch_prediction_loss': 11.34976863861084, 'train_batch_gradient_norm': 0.1251220703125}\n",
      "{'step': 40, 'train_batch_total_loss': 11.121350288391113, 'train_batch_prediction_loss': 11.121350288391113, 'train_batch_gradient_norm': 0.12469482421875}\n",
      "{'step': 45, 'train_batch_total_loss': 11.198533058166504, 'train_batch_prediction_loss': 11.198533058166504, 'train_batch_gradient_norm': 0.11102294921875}\n",
      "{'step': 50, 'train_batch_total_loss': 11.467340469360352, 'train_batch_prediction_loss': 11.467340469360352, 'train_batch_gradient_norm': 0.09832763671875}\n",
      "{'step': 55, 'train_batch_total_loss': 11.295417785644531, 'train_batch_prediction_loss': 11.295417785644531, 'train_batch_gradient_norm': 0.151123046875}\n",
      "{'step': 60, 'train_batch_total_loss': 11.1211576461792, 'train_batch_prediction_loss': 11.1211576461792, 'train_batch_gradient_norm': 0.09136962890625}\n",
      "{'step': 65, 'train_batch_total_loss': 10.911584854125977, 'train_batch_prediction_loss': 10.911584854125977, 'train_batch_gradient_norm': 0.10736083984375}\n",
      "{'step': 70, 'train_batch_total_loss': 10.943690299987793, 'train_batch_prediction_loss': 10.943690299987793, 'train_batch_gradient_norm': 0.11700439453125}\n",
      "{'step': 75, 'train_batch_total_loss': 11.21096134185791, 'train_batch_prediction_loss': 11.21096134185791, 'train_batch_gradient_norm': 0.134521484375}\n",
      "{'step': 80, 'train_batch_total_loss': 11.015945434570312, 'train_batch_prediction_loss': 11.015945434570312, 'train_batch_gradient_norm': 0.1163330078125}\n",
      "{'step': 85, 'train_batch_total_loss': 11.095580101013184, 'train_batch_prediction_loss': 11.095580101013184, 'train_batch_gradient_norm': 0.1470947265625}\n",
      "{'step': 90, 'train_batch_total_loss': 11.145216941833496, 'train_batch_prediction_loss': 11.145216941833496, 'train_batch_gradient_norm': 0.1273193359375}\n",
      "{'step': 95, 'train_batch_total_loss': 11.164227485656738, 'train_batch_prediction_loss': 11.164227485656738, 'train_batch_gradient_norm': 0.147216796875}\n",
      "{'step': 100, 'train_batch_total_loss': 11.14062213897705, 'train_batch_prediction_loss': 11.14062213897705, 'train_batch_gradient_norm': 0.1339111328125}\n",
      "{'step': 105, 'train_batch_total_loss': 11.079290390014648, 'train_batch_prediction_loss': 11.079290390014648, 'train_batch_gradient_norm': 0.1202392578125}\n",
      "{'step': 110, 'train_batch_total_loss': 10.906840324401855, 'train_batch_prediction_loss': 10.906840324401855, 'train_batch_gradient_norm': 0.1087646484375}\n",
      "{'step': 115, 'train_batch_total_loss': 11.275473594665527, 'train_batch_prediction_loss': 11.275473594665527, 'train_batch_gradient_norm': 0.11376953125}\n",
      "{'step': 120, 'train_batch_total_loss': 11.022037506103516, 'train_batch_prediction_loss': 11.022037506103516, 'train_batch_gradient_norm': 0.146240234375}\n",
      "{'step': 125, 'train_batch_total_loss': 11.296619415283203, 'train_batch_prediction_loss': 11.296619415283203, 'train_batch_gradient_norm': 0.1099853515625}\n",
      "{'step': 130, 'train_batch_total_loss': 11.130300521850586, 'train_batch_prediction_loss': 11.130300521850586, 'train_batch_gradient_norm': 0.0985107421875}\n",
      "{'step': 135, 'train_batch_total_loss': 11.222155570983887, 'train_batch_prediction_loss': 11.222155570983887, 'train_batch_gradient_norm': 0.0987548828125}\n",
      "{'step': 140, 'train_batch_total_loss': 11.23336410522461, 'train_batch_prediction_loss': 11.23336410522461, 'train_batch_gradient_norm': 0.1475830078125}\n",
      "{'step': 145, 'train_batch_total_loss': 11.330658912658691, 'train_batch_prediction_loss': 11.330658912658691, 'train_batch_gradient_norm': 0.1307373046875}\n",
      "{'step': 150, 'train_batch_total_loss': 11.090581893920898, 'train_batch_prediction_loss': 11.090581893920898, 'train_batch_gradient_norm': 0.09344482421875}\n",
      "{'step': 155, 'train_batch_total_loss': 11.417908668518066, 'train_batch_prediction_loss': 11.417908668518066, 'train_batch_gradient_norm': 0.11688232421875}\n",
      "{'step': 160, 'train_batch_total_loss': 11.132506370544434, 'train_batch_prediction_loss': 11.132506370544434, 'train_batch_gradient_norm': 0.11761474609375}\n",
      "{'step': 165, 'train_batch_total_loss': 11.078526496887207, 'train_batch_prediction_loss': 11.078526496887207, 'train_batch_gradient_norm': 0.10821533203125}\n",
      "{'step': 170, 'train_batch_total_loss': 11.431888580322266, 'train_batch_prediction_loss': 11.431888580322266, 'train_batch_gradient_norm': 0.1304931640625}\n",
      "{'step': 175, 'train_batch_total_loss': 11.1049222946167, 'train_batch_prediction_loss': 11.1049222946167, 'train_batch_gradient_norm': 0.0814208984375}\n",
      "{'step': 180, 'train_batch_total_loss': 10.624937057495117, 'train_batch_prediction_loss': 10.624937057495117, 'train_batch_gradient_norm': 0.08831787109375}\n",
      "{'step': 185, 'train_batch_total_loss': 11.06648063659668, 'train_batch_prediction_loss': 11.06648063659668, 'train_batch_gradient_norm': 0.10894775390625}\n",
      "{'step': 190, 'train_batch_total_loss': 11.284860610961914, 'train_batch_prediction_loss': 11.284860610961914, 'train_batch_gradient_norm': 0.158935546875}\n",
      "{'step': 195, 'train_batch_total_loss': 11.072608947753906, 'train_batch_prediction_loss': 11.072608947753906, 'train_batch_gradient_norm': 0.1297607421875}\n",
      "{'step': 200, 'train_batch_total_loss': 11.267630577087402, 'train_batch_prediction_loss': 11.267630577087402, 'train_batch_gradient_norm': 0.1163330078125}\n",
      "{'step': 205, 'train_batch_total_loss': 10.788492202758789, 'train_batch_prediction_loss': 10.788492202758789, 'train_batch_gradient_norm': 0.116943359375}\n",
      "{'step': 210, 'train_batch_total_loss': 11.318770408630371, 'train_batch_prediction_loss': 11.318770408630371, 'train_batch_gradient_norm': 0.10968017578125}\n",
      "{'step': 215, 'train_batch_total_loss': 11.14027214050293, 'train_batch_prediction_loss': 11.14027214050293, 'train_batch_gradient_norm': 0.1337890625}\n",
      "{'step': 220, 'train_batch_total_loss': 11.10389518737793, 'train_batch_prediction_loss': 11.10389518737793, 'train_batch_gradient_norm': 0.10076904296875}\n",
      "{'step': 225, 'train_batch_total_loss': 11.205479621887207, 'train_batch_prediction_loss': 11.205479621887207, 'train_batch_gradient_norm': 0.09405517578125}\n",
      "{'step': 230, 'train_batch_total_loss': 11.20136547088623, 'train_batch_prediction_loss': 11.20136547088623, 'train_batch_gradient_norm': 0.08441162109375}\n",
      "{'step': 235, 'train_batch_total_loss': 11.114782333374023, 'train_batch_prediction_loss': 11.114782333374023, 'train_batch_gradient_norm': 0.11053466796875}\n",
      "{'step': 240, 'train_batch_total_loss': 10.74853515625, 'train_batch_prediction_loss': 10.74853515625, 'train_batch_gradient_norm': 0.11181640625}\n",
      "{'step': 245, 'train_batch_total_loss': 10.888134956359863, 'train_batch_prediction_loss': 10.888134956359863, 'train_batch_gradient_norm': 0.09783935546875}\n",
      "{'step': 250, 'train_batch_total_loss': 11.269372940063477, 'train_batch_prediction_loss': 11.269372940063477, 'train_batch_gradient_norm': 0.10919189453125}\n",
      "{'step': 255, 'train_batch_total_loss': 10.758340835571289, 'train_batch_prediction_loss': 10.758340835571289, 'train_batch_gradient_norm': 0.09796142578125}\n",
      "{'step': 260, 'train_batch_total_loss': 11.403977394104004, 'train_batch_prediction_loss': 11.403977394104004, 'train_batch_gradient_norm': 0.1112060546875}\n",
      "{'step': 265, 'train_batch_total_loss': 11.187914848327637, 'train_batch_prediction_loss': 11.187914848327637, 'train_batch_gradient_norm': 0.1136474609375}\n",
      "{'step': 270, 'train_batch_total_loss': 11.2586030960083, 'train_batch_prediction_loss': 11.2586030960083, 'train_batch_gradient_norm': 0.1304931640625}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# current problem: 1728 / 30864\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mhypernetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_per_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_unaffected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[10], line 236\u001b[0m, in \u001b[0;36mRavelInterpretorHypernetwork.run_train\u001b[0;34m(self, train_loader, test_loader, epochs, eval_per_steps, use_unaffected)\u001b[0m\n\u001b[1;32m    233\u001b[0m num_datapoints_in_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current_batch_size\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43meditor_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meditor_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_attention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_attention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_unaffected:\n",
      "Cell \u001b[0;32mIn[10], line 41\u001b[0m, in \u001b[0;36mRavelInterpretorHypernetwork.forward\u001b[0;34m(self, editor_input_ids, base_input_ids, base_attention_mask, source_input_ids, source_attention_mask, labels)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     34\u001b[0m     editor_input_ids: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     labels: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m ):\n\u001b[0;32m---> 41\u001b[0m     _pred: EditorModelOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meditor_inner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43meditor_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meditor_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43meditor_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meditor_input_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meditor_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# output_target_hidden_states=True,\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: _pred\u001b[38;5;241m.\u001b[39mlogits,\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: _pred\u001b[38;5;241m.\u001b[39mtarget_hidden_states,\n\u001b[1;32m     55\u001b[0m         }\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/hypernetwork-editor/models/llama3/model.py:433\u001b[0m, in \u001b[0;36mLlamaInterpretor.forward\u001b[0;34m(self, editor_input_ids, editor_attention_mask, base_input_ids, base_attention_mask, source_input_ids, source_attention_mask, base_hidden_states, base_position_ids, source_hidden_states, source_position_ids, intervention_layer, output_edited_hidden_states, output_intervention_ratio, batch_intervention_ratio)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Run target model for encoded hidden states\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     base_hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m--> 433\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_target_model_for_encoded_hidden_states\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_position_ids\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# seems to break while we are passing thru batch_size=1; the last (12th =) has different dimensions\u001b[39;00m\n\u001b[1;32m    436\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    437\u001b[0m     )\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     source_hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_target_model_for_encoded_hidden_states(\n\u001b[1;32m    442\u001b[0m             source_input_ids, source_attention_mask, source_position_ids\n\u001b[1;32m    443\u001b[0m         ),\n\u001b[1;32m    444\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    445\u001b[0m     )\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/hypernetwork-editor/models/llama3/model.py:401\u001b[0m, in \u001b[0;36mLlamaInterpretor._run_target_model_for_encoded_hidden_states\u001b[0;34m(self, target_input_ids, target_attention_mask, position_ids)\u001b[0m\n\u001b[1;32m    393\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model(\n\u001b[1;32m    394\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39mtarget_input_ids,\n\u001b[1;32m    395\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mtarget_attention_mask,\n\u001b[1;32m    396\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    397\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m     )\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mhidden_states\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1210\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1207\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1210\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1223\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1017\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1007\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1008\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1014\u001b[0m         cache_position,\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1017\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:740\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    752\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:664\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# Reference: https://github.com/pytorch/pytorch/issues/112577.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query_states\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m causal_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 664\u001b[0m     query_states \u001b[38;5;241m=\u001b[39m \u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    666\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# current problem: 1728 / 30864\n",
    "hypernetwork.run_train(\n",
    "    train_loader=data_loader,\n",
    "    test_loader=test_data_loader,\n",
    "    epochs=3,\n",
    "    eval_per_steps = 50,\n",
    "    use_unaffected=False\n",
    ")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
