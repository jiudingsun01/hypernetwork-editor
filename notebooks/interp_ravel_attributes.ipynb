{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run\n",
    "download_wikipedia.ipynb\n",
    "to get the file\n",
    "wikipedia_three_sentences.csv\n",
    "\n",
    "Then, we do our processing here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_index = 8  # 4090 or A6000\n",
    "num_editing_heads = (\n",
    "    1\n",
    ")  # more seems to be better for this #per sid's suggestion: can add more heads in every layer. This is probably a really great suggestion\n",
    "max_grad_clip = 4.0\n",
    "chop_layer = 6\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes from Sid: SAE vs principal components\n",
    "-you could try to work on the SAE's???\n",
    "-get discrete targets\n",
    "[I wouldn't be surprised at all if this helped a lot]\n",
    "\n",
    "-Next deliverables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "# # Trying out the editor hypernetwork on the dune dataset\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"hypernetworks-interp\",\n",
    "    config={\"targetmodel\": \"llama3-8b\", \"editormodel\": \"llama3-8b\", \"dataset\": \"ravel\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast, LlamaConfig, LlamaModel\n",
    "import torch\n",
    "from torch import compile\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import contextlib\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we stick on the \"reverse attention\" module at the end!\n",
    "This is a customized attention head that reads from the editor model, and writes to the target model's activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GPT2Editor' from 'models.gpt2.model' (/work/frink/sun.jiu/hypernetwork-editor/models/gpt2/model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# @torch.compile #Apparently this fails when used inside jupyter notebooks but is fine if i make dedicated scripts\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Interpretor, GPT2InterpretorConfig, GPT2InterpretorHypernetwork\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditorModelOutput\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRavelEditorHypernetwork\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Separating the editor config file, from its base model's configurations\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/hypernetwork-editor/models/gpt2/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Editor, GPT2EditorConfig, GPT2EditorHypernetwork\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Editor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2EditorConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2EditorHypernetwork\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GPT2Editor' from 'models.gpt2.model' (/work/frink/sun.jiu/hypernetwork-editor/models/gpt2/model.py)"
     ]
    }
   ],
   "source": [
    "# @torch.compile #Apparently this fails when used inside jupyter notebooks but is fine if i make dedicated scripts\n",
    "from models.gpt2.model import GPT2Interpretor, GPT2InterpretorConfig, GPT2InterpretorHypernetwork\n",
    "from models.utils import EditorModelOutput\n",
    "\n",
    "\n",
    "class RavelEditorHypernetwork(nn.Module):\n",
    "    # Separating the editor config file, from its base model's configurations\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_editing_heads=32,\n",
    "        use_layerwise_embeddings=True,\n",
    "        chop_editor_at_layer=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.editor_config = GPT2InterpretorConfig(\n",
    "            num_editing_heads=num_editing_heads,\n",
    "            chop_editor_at_layer=chop_editor_at_layer,\n",
    "        )\n",
    "        \n",
    "        self.editor_inner = GPT2Interpretor(self.editor_config)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.editor_config.name_or_path)\n",
    "\n",
    "        self.residual_cache = None\n",
    "        self.opt = None\n",
    "        self.training_loss = None\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        editor_input_ids: torch.Tensor = None,\n",
    "        base_input_ids: torch.Tensor = None,\n",
    "        base_attention_mask: torch.Tensor = None,\n",
    "        source_input_ids: torch.Tensor = None,\n",
    "        source_attention_mask: torch.Tensor = None,\n",
    "        labels: torch.Tensor = None,\n",
    "    ):\n",
    "        _pred: EditorModelOutput = self.editor_inner(\n",
    "            editor_input_ids=editor_input_ids,\n",
    "            editor_attention_mask=editor_input_ids != self.editor_config.eos_token_id,\n",
    "            base_input_ids=base_input_ids,\n",
    "            base_attention_mask=base_attention_mask,\n",
    "            source_input_ids=source_input_ids,\n",
    "            source_attention_mask=source_attention_mask,\n",
    "            output_target_hidden_states=True,\n",
    "        )\n",
    "        \n",
    "        if labels is None:\n",
    "            return {\n",
    "                \"logits\": _pred.logits,\n",
    "                \"edit_vectors\": _pred.edit_vectors,\n",
    "                \"target_hidden_states\": _pred.target_hidden_states,\n",
    "            }\n",
    "        else:\n",
    "            log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                _pred.logits.reshape(-1, _pred.logits.shape[-1]),\n",
    "                dim=1,\n",
    "            )\n",
    "            \n",
    "            labels = labels.reshape(-1)\n",
    "            assert labels.shape == log_prob_predictions.shape[:-1]\n",
    "            \n",
    "            # Only consider the tokens that are not -100 in target_labels\n",
    "            label_indices = labels != -100\n",
    "            \n",
    "            log_prob_predictions = log_prob_predictions[label_indices, :]\n",
    "            labels = labels[label_indices]\n",
    "            \n",
    "            # Compute the cross-entropy loss with masking\n",
    "            criterion = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "            loss = criterion(log_prob_predictions, labels.long())\n",
    "            \n",
    "            return {\n",
    "                \"logits\": _pred.logits,\n",
    "                \"edit_vectors\": _pred.edit_vectors,\n",
    "                \"target_hidden_states\": _pred.target_hidden_states,\n",
    "                \"loss\": loss,\n",
    "            }\n",
    "        \n",
    "    \n",
    "    # Generate text using the target model, with a new edit application at every step.\n",
    "    # This is a very slow way to generate text.\n",
    "    # If you only want to edit first k tokens, use the forward pass instead with stop_editing_index = k\n",
    "    def inspect_batch_prediction_ouptuts(self, batch):\n",
    "        self.editor_inner.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            predictions = self.forward(\n",
    "                editor_input_ids=batch[\"editor_input_ids\"],\n",
    "                base_input_ids=batch[\"base_input_ids\"],\n",
    "                base_attention_mask=batch[\"base_attention_mask\"],\n",
    "                source_input_ids=batch[\"source_input_ids\"],\n",
    "                source_attention_mask=batch[\"source_attention_mask\"],\n",
    "            )\n",
    "            \n",
    "            batch_pred_ids = torch.argmax(predictions[\"logits\"], dim=-1)\n",
    "            batch_full_output = self.tokenizer.batch_decode(batch_pred_ids, skip_special_tokens=True)\n",
    "            \n",
    "            batch_output = []\n",
    "            correct = 0\n",
    "            \n",
    "            for label, pred_ids in zip(batch[\"labels\"], batch_pred_ids):\n",
    "                \n",
    "                output_idx = label != -100\n",
    "                label = label[output_idx]\n",
    "                pred_ids = pred_ids[output_idx]\n",
    "                batch_output.append(\n",
    "                    self.tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "                )             \n",
    "                \n",
    "                correct += torch.sum(label == pred_ids) == torch.numel(label)\n",
    "            \n",
    "        return {\n",
    "            \"batch_output\": batch_output,\n",
    "            \"batch_full_output\": batch_full_output,\n",
    "            \"n_correct\": correct,\n",
    "        }\n",
    "    \n",
    "    def eval_accuracy(self, test_loader, use_unaffected=False):\n",
    "        \n",
    "        self.editor_inner.eval()\n",
    "        test_loss = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                predictions = self.forward(\n",
    "                    editor_input_ids=batch[\"editor_input_ids\"],\n",
    "                    base_input_ids=batch[\"base_input_ids\"],\n",
    "                    base_attention_mask=batch[\"base_attention_mask\"],\n",
    "                    source_input_ids=batch[\"source_input_ids\"],\n",
    "                    source_attention_mask=batch[\"source_attention_mask\"],\n",
    "                    labels=batch[\"labels\"],\n",
    "                )\n",
    "                test_loss.append(predictions[\"loss\"].item())\n",
    "                \n",
    "                batch_pred_ids = torch.argmax(predictions[\"logits\"], dim=-1)\n",
    "                \n",
    "                if use_unaffected:\n",
    "                    unaffected_prediction = self.forward(\n",
    "                        editor_input_ids=batch[\"editor_input_ids_unaffected\"],\n",
    "                        base_input_ids=batch[\"base_input_ids_unaffected\"],\n",
    "                        base_attention_mask=batch[\"base_attention_mask_unaffected\"],\n",
    "                        source_input_ids=batch[\"source_input_ids_unaffected\"],\n",
    "                        source_attention_mask=batch[\"source_attention_mask_unaffected\"],\n",
    "                        labels=batch[\"labels_unaffected\"],\n",
    "                    )\n",
    "                    batch_pred_ids_unaffected = torch.argmax(unaffected_prediction[\"logits\"], dim=-1)\n",
    "                    \n",
    "                    correct_unaffected = []\n",
    "                    \n",
    "                    for label, pred_ids in zip(batch[\"labels_unaffected\"], batch_pred_ids_unaffected):\n",
    "                        output_idx = label != -100\n",
    "                        label = label[output_idx]\n",
    "                        pred_ids = pred_ids[output_idx]\n",
    "                        correct_unaffected.append((torch.sum(label == pred_ids) == torch.numel(label)))\n",
    "                    \n",
    "                    correct_unaffected = torch.stack(correct_unaffected)\n",
    "                    \n",
    "                    test_loss[-1] += unaffected_prediction[\"loss\"].item()\n",
    "                \n",
    "                for i, (label, pred_ids) in enumerate(zip(batch[\"labels\"], batch_pred_ids)):\n",
    "                    output_idx = label != -100\n",
    "                    label = label[output_idx]\n",
    "                    pred_ids = pred_ids[output_idx]\n",
    "                    \n",
    "                    is_correct = (torch.sum(label == pred_ids) == torch.numel(label)).item()\n",
    "                    if use_unaffected:\n",
    "                        # indices of the position which value is i\n",
    "                        instance_idx = torch.nonzero(batch[\"instance_indices\"] == i).squeeze()\n",
    "                        # check if all the unaffected positions were correct\n",
    "                        # if not, set is_correct to False\n",
    "                        is_correct = is_correct and all(correct_unaffected[instance_idx])\n",
    "                        \n",
    "                    correct += is_correct\n",
    "                    total += 1\n",
    "                    \n",
    "        return correct / total, sum(test_loss) / len(test_loss)\n",
    "             \n",
    "\n",
    "    def run_train(\n",
    "        self,\n",
    "        train_loader,\n",
    "        test_loader=None,\n",
    "        stop_editing_index=8,\n",
    "        epochs=1,\n",
    "        eval_per_steps: int = None,\n",
    "        use_unaffected = False\n",
    "    ):\n",
    "        trainable_parameters = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"target_model\" not in name:\n",
    "                trainable_parameters.append(param)\n",
    "                \n",
    "        self.opt = optim.AdamW(trainable_parameters, lr=lr, weight_decay=0.01)  # usually: lr = 5e-5. 1e-3 worked well!\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Create a tqdm progress bar\n",
    "            with tqdm(\n",
    "                total=len(train_loader),\n",
    "                desc=f\"Epoch {epoch + 1}/{epochs}\",\n",
    "                unit=\"batch\",\n",
    "                disable=True,\n",
    "            ) as pbar:\n",
    "                num_datapoints_in_epoch = 0\n",
    "                epoch_train_loss = 0\n",
    "                epoch_gradient_norm = 0\n",
    "                # Train loop\n",
    "                batch_index = -1  # index of first batch will be 0\n",
    "\n",
    "                for step, batch in enumerate(\n",
    "                    train_loader\n",
    "                ):  \n",
    "                    if step % eval_per_steps == 0:\n",
    "                        # Evaluate the model\n",
    "                        accuracy, test_loss = self.eval_accuracy(\n",
    "                            test_loader, stop_editing_index, use_unaffected=use_unaffected\n",
    "                        )\n",
    "                        \n",
    "                        if wandb.run:\n",
    "                            wandb.log(\n",
    "                                {\n",
    "                                    \"test_average\": test_loss,\n",
    "                                    \"test_accuracy\": accuracy,\n",
    "                                }\n",
    "                            )\n",
    "                        \n",
    "                    batch_index += 1\n",
    "                    self.batch = batch\n",
    "                    current_batch_size = len(batch[\"editor_input_ids\"])\n",
    "                    num_datapoints_in_epoch += current_batch_size\n",
    "                    self.opt.zero_grad()\n",
    "\n",
    "                    self.prediction = self.forward(\n",
    "                        editor_input_ids=batch[\"editor_input_ids\"],\n",
    "                        base_input_ids=batch[\"base_input_ids\"],\n",
    "                        base_attention_mask=batch[\"base_attention_mask\"],\n",
    "                        source_input_ids=batch[\"source_input_ids\"],\n",
    "                        source_attention_mask=batch[\"source_attention_mask\"],\n",
    "                        labels=batch[\"labels\"],\n",
    "                    )\n",
    "\n",
    "                    self.prediction_loss = self.prediction[\"loss\"]\n",
    "                    \n",
    "                    if use_unaffected:\n",
    "                        self.prediction_loss += self.forward(\n",
    "                            editor_input_ids=batch[\"editor_input_ids_unaffected\"],\n",
    "                            input_ids=batch[\"input_ids_unaffected\"],\n",
    "                            attention_mask=batch[\"attention_mask_unaffected\"],\n",
    "                            labels=batch[\"labels_unaffected\"],\n",
    "                            stop_editing_idx=stop_editing_index\n",
    "                        )[\"loss\"]\n",
    "\n",
    "                    # Compute the total loss and backpropagate\n",
    "                    self.training_loss = self.prediction_loss\n",
    "                    self.training_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(\n",
    "                        self.parameters(), max_grad_clip\n",
    "                    )  # just implemented this! dunno if a cap of 1 to large, so I'm messing with reducing it\n",
    "\n",
    "                    # Check for nan gradients\n",
    "                    # if check_nan_gradients(self):\n",
    "                    #     break\n",
    "\n",
    "                    # Backwards step\n",
    "                    self.opt.step()\n",
    "\n",
    "                    # metrics\n",
    "                    epoch_train_loss += self.training_loss.item() * current_batch_size\n",
    "                    gradients = [\n",
    "                        p.grad.view(-1) for p in self.parameters() if p.grad is not None\n",
    "                    ]\n",
    "                    all_gradients = torch.cat(gradients)\n",
    "                    gradient_norm = torch.norm(all_gradients).item()\n",
    "                    epoch_gradient_norm += gradient_norm * current_batch_size\n",
    "\n",
    "                    metrics = {\n",
    "                        \"step\": step * (epoch + 1),\n",
    "                        \"train_batch_total_loss\": self.training_loss.item(),\n",
    "                        \"train_batch_prediction_loss\": self.prediction_loss.item(),\n",
    "                        \"train_batch_gradient_norm\": gradient_norm,\n",
    "                    }\n",
    "\n",
    "                    if wandb.run:\n",
    "                        wandb.log(metrics)\n",
    "                    if step % 5 == 0:\n",
    "                        print(metrics)\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)  # note: this was incorrectly displaying before!\n",
    "\n",
    "                    # Check if it's time to save a checkpoint\n",
    "                    current_time = time.time()\n",
    "                    # first loop initialization\n",
    "                    if batch_index == 0 and epoch == 0:\n",
    "                        last_checkpoint_time = -100000\n",
    "\n",
    "                if wandb.run:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"epoch_train_total_loss\": epoch_train_loss\n",
    "                            / num_datapoints_in_epoch,\n",
    "                            \"gradient_norm\": epoch_gradient_norm\n",
    "                            / num_datapoints_in_epoch,\n",
    "                        }\n",
    "                    )\n",
    "            # Save the final model\n",
    "            torch.save(self.state_dict(), \"final_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 44.48 GiB of which 67.25 MiB is free. Including non-PyTorch memory, this process has 44.41 GiB memory in use. Of the allocated memory 44.21 GiB is allocated by PyTorch, and 20.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m PreTrainedTokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/work/frink/models/llama3-8B-HF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/work/frink/models/llama3-8B-HF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m      6\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/transformers/modeling_utils.py:3550\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3544\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3545\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3546\u001b[0m )\n\u001b[1;32m   3548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3549\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3550\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3552\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3553\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1138\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:927\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 927\u001b[0m     [LlamaDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    928\u001b[0m )\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:927\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 927\u001b[0m     [\u001b[43mLlamaDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    928\u001b[0m )\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:702\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m LLAMA_ATTENTION_CLASSES[config\u001b[38;5;241m.\u001b[39m_attn_implementation](config\u001b[38;5;241m=\u001b[39mconfig, layer_idx\u001b[38;5;241m=\u001b[39mlayer_idx)\n\u001b[0;32m--> 702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:219\u001b[0m, in \u001b[0;36mLlamaMLP.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn \u001b[38;5;241m=\u001b[39m ACT2FN[config\u001b[38;5;241m.\u001b[39mhidden_act]\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 44.48 GiB of which 67.25 MiB is free. Including non-PyTorch memory, this process has 44.41 GiB memory in use. Of the allocated memory 44.21 GiB is allocated by PyTorch, and 20.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/work/frink/models/llama3-8B-HF\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = LlamaForCausalLM.from_pretrained(\"/work/frink/models/llama3-8B-HF\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 72338.41it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 82409.31it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "def generate_ravel_dataset(n_samples, split=\"train\", domains=[\"city\"], seed=42):\n",
    "            \n",
    "    # Seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    \n",
    "    sample_per_domain = n_samples // len(domains)\n",
    "    \n",
    "    for domain in domains:\n",
    "        \n",
    "        templates = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_attribute_to_prompts.json\"), \"r\"))\n",
    "        entities = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_entity_attributes.json\"), \"r\"))\n",
    "        entities_split = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_entity_to_split.json\"), \"r\"))\n",
    "        templates_split = json.load(open(os.path.join(\"./data/ravel/\", f\"ravel_{domain}_prompt_to_split.json\"), \"r\"))\n",
    "\n",
    "        templates_train = {k: [v for v in vs if templates_split[v] == \"train\"] for k, vs in templates.items()}\n",
    "        templates_test = {k: [v for v in vs if templates_split[v] != \"train\"] for k, vs in templates.items()}\n",
    "\n",
    "        entities_train = {k: v for k, v in entities.items() if entities_split[k] == \"train\"}\n",
    "        name_train = list(entities_train.keys())\n",
    "\n",
    "        entities_test = {k: v for k, v in entities.items() if entities_split[k] != \"train\"}\n",
    "        name_test = list(entities_test.keys())\n",
    "        \n",
    "        if split == \"train\":\n",
    "            entity_dict, entity_name, prompt_dict = entities_train, name_train, templates_train\n",
    "        elif split == \"test\":\n",
    "            entity_dict, entity_name, prompt_dict = entities_test, name_test, templates_test\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train' or 'test'\")\n",
    "        \n",
    "        all_attributes = list(prompt_dict.keys())\n",
    "        \n",
    "        for _ in tqdm(range(sample_per_domain)):\n",
    "            \n",
    "            attribute = random.choice(all_attributes)\n",
    "            frozen_attributes = [k for k in all_attributes if k != attribute]\n",
    "            \n",
    "            data = {}\n",
    "            \n",
    "            source_entity, base_entity = random.sample(entity_name, 2)\n",
    "            source_entity_dict, base_entity_dict = entity_dict[source_entity], entity_dict[base_entity]\n",
    "            \n",
    "            template = random.choice(prompt_dict[attribute])\n",
    "\n",
    "            data[\"input_text\"] = template % base_entity\n",
    "            data[\"counterfactual_input_text\"] = template % source_entity\n",
    "            data[\"edit_instruction\"] = f\"{base_entity} ; {source_entity} - {attribute}\"\n",
    "            data[\"target\"] = base_entity_dict[attribute]\n",
    "            data[\"counterfactual_target\"] = source_entity_dict[attribute]\n",
    "            \n",
    "            data[\"unaffected_attributes\"] = []\n",
    "            \n",
    "            for frozen_attribute in frozen_attributes:\n",
    "                \n",
    "                attribute_template = random.choice(prompt_dict[frozen_attribute])\n",
    "                base_prompt = attribute_template % base_entity\n",
    "                counterfactual_prompt = attribute_template % source_entity\n",
    "                \n",
    "                target = base_entity_dict[frozen_attribute]\n",
    "                counterfactual_target = source_entity_dict[frozen_attribute]\n",
    "                \n",
    "                data[\"unaffected_attributes\"].append(\n",
    "                    {\n",
    "                        \"input_text\": base_prompt,\n",
    "                        \"counterfactual_input_text\": counterfactual_prompt,\n",
    "                        \"edit_instruction\": f\"{base_entity} ; {source_entity} - {attribute}\",\n",
    "                        \"target\": target,\n",
    "                        \"counterfactual_target\": counterfactual_target,\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "            dataset.append(data)\n",
    "                \n",
    "    dataset = Dataset.from_list(dataset)\n",
    "    return dataset\n",
    "\n",
    "city_train_set = generate_ravel_dataset(10000, split=\"train\")\n",
    "city_test_set =  generate_ravel_dataset(1000, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ravel_collate_fn(batch):\n",
    "    \n",
    "    def tokenize_text_inputs(texts, counterfactual_texts, target_texts):\n",
    "        \n",
    "        input_texts = [text + \" \" + target for text, target in zip(texts, target_texts)]\n",
    "        input_texts = [text.replace(\" \\\" \", \" \\\" \") for text in input_texts]\n",
    "        \n",
    "        tokenized = tokenizer(input_texts, return_tensors=\"pt\", padding=True, max_length=50, truncation=True)\n",
    "        tokenized_counterfactual = tokenizer(counterfactual_texts, return_tensors=\"pt\", padding=True, max_length=50, truncation=True)\n",
    "        tokenized_labels = []\n",
    "        \n",
    "        for input_ids, input_text in zip(tokenized[\"input_ids\"], texts):\n",
    "            input_length = tokenizer(input_text, return_tensors=\"pt\", padding=False)[\"input_ids\"].shape[-1]\n",
    "            label = torch.full_like(input_ids, -100)\n",
    "            label[input_length:] = input_ids[input_length:]\n",
    "            label[input_ids == tokenizer.pad_token_id] = -100\n",
    "            tokenized_labels.append(label)\n",
    "        \n",
    "        tokenized_labels = torch.stack(tokenized_labels)\n",
    "        return {\n",
    "            \"base_input_ids\": tokenized[\"input_ids\"],\n",
    "            \"base_attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"source_input_ids\": tokenized_counterfactual[\"input_ids\"],\n",
    "            \"source_attention_mask\": tokenized_counterfactual[\"attention_mask\"],\n",
    "            \"labels\": tokenized_labels\n",
    "        }\n",
    "        \n",
    "    prompts, edit_instructions, targets, unaffected_attributes, counterfactual_prompts = [], [], [], [], []\n",
    "    for b in batch:\n",
    "        prompts.append(b[\"input_text\"])\n",
    "        edit_instructions.append(b[\"edit_instruction\"])\n",
    "        targets.append(b[\"counterfactual_target\"])\n",
    "        unaffected_attributes.append(b[\"unaffected_attributes\"])\n",
    "        counterfactual_prompts.append(b[\"counterfactual_input_text\"])\n",
    "        \n",
    "        \n",
    "    editor_input_ids = tokenizer(edit_instructions, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    \n",
    "    returned_dict = {\n",
    "        \"editor_input_ids\": editor_input_ids,\n",
    "        **tokenize_text_inputs(prompts, counterfactual_prompts, targets),\n",
    "    }\n",
    "    \n",
    "    base_prompts_unaffected, counterfactual_prompts_unaffected, targets_unaffected, edit_instructions_unaffected, instance_indices = [], [], [], [], []\n",
    "    \n",
    "    for i, attribute_list in enumerate(unaffected_attributes):\n",
    "        \n",
    "        for d in attribute_list:\n",
    "            base_prompts_unaffected.append(d[\"input_text\"])\n",
    "            targets_unaffected.append(d[\"target\"])\n",
    "            counterfactual_prompts_unaffected.append(d[\"counterfactual_input_text\"])\n",
    "                    \n",
    "        for _ in range(len(attribute_list)):\n",
    "            edit_instructions_unaffected.append(editor_input_ids[i])\n",
    "            instance_indices.append(i)\n",
    "        \n",
    "    edit_instructions_unaffected = torch.stack(edit_instructions_unaffected)\n",
    "    instance_indices = torch.tensor(instance_indices)\n",
    "    \n",
    "    assert len(base_prompts_unaffected) == len(targets_unaffected)\n",
    "    \n",
    "    tokenized_unaffected = tokenize_text_inputs(base_prompts_unaffected, counterfactual_prompts_unaffected, targets_unaffected)\n",
    "    \n",
    "    returned_dict[\"editor_input_ids_unaffected\"] = edit_instructions_unaffected\n",
    "    returned_dict[\"base_input_ids_unaffected\"] = tokenized_unaffected[\"base_input_ids\"]\n",
    "    returned_dict[\"base_attention_mask_unaffected\"] = tokenized_unaffected[\"base_attention_mask\"]\n",
    "    returned_dict[\"source_input_ids_unaffected\"] = tokenized_unaffected[\"source_input_ids\"]\n",
    "    returned_dict[\"source_attention_mask_unaffected\"] = tokenized_unaffected[\"source_attention_mask\"]\n",
    "    returned_dict[\"labels_unaffected\"] = tokenized_unaffected[\"labels\"]\n",
    "    returned_dict[\"instance_indices\"] = instance_indices\n",
    "    \n",
    "    return returned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "32it [00:00, 1167.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "input_prompt:  St. Petersburg: Europe/Moscow. Bongor:\n",
      "gt_label:  Africa/Ndjamena\n",
      "prediction:  . 1. 2. 3. 4. 5. 6.\n",
      "-----------\n",
      "input_prompt:  Hong Kong: Asia/Hong_Kong. Surat:\n",
      "gt_label:  Asia/Kolkata\n",
      "prediction:  _REF: 2019-12-05T00:00:00Z.\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"San Francisco\", \"continent\": \"North America\"}, {\"city\": \"Esperance\", \"continent\": \"\n",
      "gt_label:  Oceania\n",
      "prediction:  \", \"country\": \"Australia\"}]\n",
      "    \"\"\"\n",
      "    # Your code here\n",
      "    return [city\n",
      "-----------\n",
      "input_prompt:  Sydney is a city in the continent of Oceania. Luzern is a city in the continent of\n",
      "gt_label:  Europe\n",
      "prediction:  ��urope. We are using latitude and longitude to display the list below. Luzern is 16\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"New York City\", \"lat\": \"40.7\"}, {\"city\": \"Chamdo\", \"lat\": \"\n",
      "gt_label:  31\n",
      "prediction:  31.2\"}, {\"city\": \"Kunming\", \"lat\": \" 25.0\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Pecos\", \"lat\": \"\n",
      "gt_label:  31\n",
      "prediction:  \", \"lon\": \"  \", \"state\": \"TX\", \"country\": \"US\", \"\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Beijing\", \"lang\": \"Chinese\"}, {\"city\": \"Haora\", \"lang\": \"\n",
      "gt_label:  Bengali\n",
      "prediction:  \", \"country\": \"India\"}, {\"city\": \"Shanghai\", \"lang\": \"Chinese\"},\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Kuala Lumpur\", \"lat\": \"3.1\"}, {\"city\": \"Bathurst\", \"lat\": \"\n",
      "gt_label:  -33\n",
      "prediction:  33.4\"}]\n",
      "    >>> find_closest_city(cities, 33.4)\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Rio de Janeiro\", \"lat\": \"23\"}, {\"city\": \"Chicago\", \"lat\": \"\n",
      "gt_label:  42\n",
      "prediction:  \", \"lon\": \"  \"}, {\"city\": \"New York\", \"lat\": \"\n",
      "-----------\n",
      "input_prompt:  San Francisco: America/Los_Angeles. Chukai:\n",
      "gt_label:  Asia/Kuala_Lumpur\n",
      "prediction:  _REF: 1.0.0.0.0.0.0.0.\n",
      "-----------\n",
      "input_prompt:  Cape Town is a city in the continent of Africa. Sarh is a city in the continent of\n",
      "gt_label:  Africa\n",
      "prediction:  africa. We are using latitude and longitude to display the list below. Cape Town is 1\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Tokyo\", \"country\": \"Japan\"}, {\"city\": \"Dhule\", \"country\": \"\n",
      "gt_label:  India\n",
      "prediction:  \", \"state\": \"Maharashtra\"}, {\"city\": \"Dhule\", \"country\":\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Sydney\", \"lat\": \"34\"}, {\"city\": \"Callao\", \"lat\": \"\n",
      "gt_label:  -12\n",
      "prediction:  \", \"lon\": \"  \"}, {\"city\": \"Buenos Aires\", \"lat\":\n",
      "-----------\n",
      "input_prompt:  People in Rome speak Italian. People in Kulunda speak\n",
      "gt_label:  Russian\n",
      "prediction:  ; they speak the Russian Language. Which language do people speak in Kulunda? In Kulunda,\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"New York City\", \"country\": \"United States\"}, {\"city\": \"Celeken\", \"country\": \"\n",
      "gt_label:  Turkmenistan\n",
      "prediction:  \", \"state\": \"  \"}, {\"city\": \"Kazan\", \"country\": \"\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Rome\", \"long\": \"12.5\"}, {\"city\": \"Paraguari\", \"long\": \"\n",
      "gt_label:  -57\n",
      "prediction:  \", \"lat\": \"  \"}, {\"city\": \"Buenos Aires\", \"long\":\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"New York City\", \"lat\": \"40.7\"}, {\"city\": \"Timmins\", \"lat\": \"\n",
      "gt_label:  48\n",
      "prediction:  \", \"lon\": \" \"}, {\"city\": \"Toronto\", \"lat\": \"43.7\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Rome\", \"country\": \"Italy\"}, {\"city\": \"Sokoto\", \"country\": \"\n",
      "gt_label:  Nigeria\n",
      "prediction:  \", \"state\": \"Sokoto\"}, {\"city\": \"Sao Paulo\", \"country\":\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"St. Petersburg\", \"lat\": \"60\"}, {\"city\": \"Xangongo\", \"lat\": \"\n",
      "gt_label:  -17\n",
      "prediction:  \", \"lon\": \"  \"}, {\"city\": \"Kabinda\", \"lat\": \"\n",
      "-----------\n",
      "input_prompt:  in Topki, people usually speak\n",
      "gt_label:  Russian\n",
      "prediction:  ://www.rosstat.gov.ru/\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"New Delhi\", \"lat\": \"29\"}, {\"city\": \"Wete\", \"lat\": \"\n",
      "gt_label:  -5\n",
      "prediction:  \", \"lon\": \"  \"}, {\"city\": \"Kuala Lumpur\", \"lat\": \"\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Cape Town\", \"country\": \"South Africa\"}, {\"city\": \"Stettler\", \"country\": \"\n",
      "gt_label:  Canada\n",
      "prediction:  \", \"state\": \"Alberta\"}, {\"city\": \"Stettler\", \"country\": \"\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"San Francisco\", \"continent\": \"North America\"}, {\"city\": \"Gillette\", \"continent\": \"\n",
      "gt_label:  North America\n",
      "prediction:  \", \"country\": \"United States\"}, {\"city\": \"Baltimore\", \"continent\": \"North\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Mexico City\", \"long\": \"99.1\"}, {\"city\": \"Salatiga\", \"long\": \"\n",
      "gt_label:  110\n",
      "prediction:  \", \"lat\": \" 0.0\"}, {\"city\": \"Bogota\", \"long\n",
      "-----------\n",
      "input_prompt:  Beijing is a city in the continent of Asia. Balakhna is a city in the continent of\n",
      "gt_label:  Europe\n",
      "prediction:  africa.\n",
      "-----------\n",
      "input_prompt:  city to country: Toronto is in Canada. Baoding is in\n",
      "gt_label:  China\n",
      "prediction:  the People's Republic of China.  The  city of Baoding is located in the province of\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Rome\", \"country\": \"Italy\"}, {\"city\": \"Montpelier\", \"country\": \"\n",
      "gt_label:  United States\n",
      "prediction:  \", \"state\": \"Vermont\"}]\n",
      "    \"\"\"\n",
      "    # Your code here\n",
      "    return\n",
      "-----------\n",
      "input_prompt:  Time zone in Los Angeles is America/Santiago; Time zone in Omaha is\n",
      "gt_label:  America/Chicago\n",
      "prediction:  America/Chicago; Time zone in San Francisco is America/Los_Angeles; Time zone\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Toronto\", \"country\": \"Canada\"}, {\"city\": \"Roatan\", \"country\": \"\n",
      "gt_label:  Honduras\n",
      "prediction:  \", \"continent\": \"North America\"}]\n",
      "    \"\"\"\n",
      "    # Your code here\n",
      "    return [\n",
      "-----------\n",
      "input_prompt:  Time zone in Beijing is Asia/Shanghai; Time zone in Biloela is\n",
      "gt_label:  Australia/Brisbane\n",
      "prediction:  is Australia/Brisbane\n",
      "Biloela is 7 hours ahead of Beijing\n",
      "Biloela\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Hong Kong\", \"lang\": \"Chinese, English\"}, {\"city\": \"Luwuk\", \"lang\": \"\n",
      "gt_label:  Indonesian\n",
      "prediction:  \", \"country\": \"Indonesia\"}, {\"city\": \"Luwuk\", \"lang\": \"\n",
      "-----------\n",
      "input_prompt:  [{\"city\": \"Rome\", \"long\": \"12.5\"}, {\"city\": \"Uyuni\", \"long\": \"\n",
      "gt_label:  -67\n",
      "prediction:  \", \"lat\": \"-20.1\"}]\n",
      "    \"\"\"\n",
      "    cities = []\n",
      "    for city in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 85\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mselect(filtered_idx)\n\u001b[0;32m---> 85\u001b[0m filtered_training_set \u001b[38;5;241m=\u001b[39m \u001b[43mfiltering_ravel_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcity_train_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m filtered_test_set \u001b[38;5;241m=\u001b[39m filtering_ravel_dataset(city_test_set, model, tokenizer)\n",
      "Cell \u001b[0;32mIn[16], line 80\u001b[0m, in \u001b[0;36mfiltering_ravel_dataset\u001b[0;34m(dataset, model, tokenizer)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_correct:\n\u001b[1;32m     78\u001b[0m             filtered_idx\u001b[38;5;241m.\u001b[39mappend(i \u001b[38;5;241m+\u001b[39m step \u001b[38;5;241m*\u001b[39m batch_size)\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mselect(filtered_idx)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "def filtering_ravel_dataset(dataset, model, tokenizer):\n",
    "    \n",
    "    def tokenize_text_inputs(texts, target_texts):\n",
    "        input_texts = [text + \" \" for text in texts]\n",
    "        input_texts = [text.replace(\" \\\" \", \" \\\" \") for text in input_texts]\n",
    "        \n",
    "        tokenized = tokenizer(input_texts, return_tensors=\"pt\", padding=True)\n",
    "        tokenized_labels = tokenizer(target_texts, return_tensors=\"pt\", padding=True)[\"input_ids\"]\n",
    "        tokenized_labels[tokenized_labels == tokenizer.pad_token_id] = -100\n",
    "        tokenized[\"labels\"] = tokenized_labels\n",
    "        return tokenized\n",
    "    \n",
    "    def filtering_collate_fn(batch):\n",
    "        prompts, targets, unaffected_attributes = [], [], []\n",
    "        for b in batch:\n",
    "            prompts.append(b[\"counterfactual_input_text\"])\n",
    "            targets.append(b[\"counterfactual_target\"])\n",
    "            unaffected_attributes.append(b[\"unaffected_attributes\"])\n",
    "        \n",
    "        returned_dict = {\n",
    "            **tokenize_text_inputs(prompts, targets),\n",
    "        }\n",
    "        \n",
    "        prompts_unaffected, targets_unaffected, instance_indices = [], [], []\n",
    "        \n",
    "        for i, attribute_list in enumerate(unaffected_attributes):\n",
    "    \n",
    "            for d in attribute_list:\n",
    "                prompts_unaffected.append(d[\"input_text\"])\n",
    "                targets_unaffected.append(d[\"target\"])\n",
    "                        \n",
    "            for _ in range(len(attribute_list)):\n",
    "                instance_indices.append(i)\n",
    "            \n",
    "        instance_indices = torch.tensor(instance_indices)\n",
    "        \n",
    "        assert len(prompts_unaffected) == len(targets_unaffected)\n",
    "        \n",
    "        tokenized_unaffected = tokenize_text_inputs(prompts_unaffected, targets_unaffected)\n",
    "        \n",
    "        returned_dict[\"input_ids_unaffected\"] = tokenized_unaffected[\"input_ids\"]\n",
    "        returned_dict[\"attention_mask_unaffected\"] = tokenized_unaffected[\"attention_mask\"]\n",
    "        returned_dict[\"labels_unaffected\"] = tokenized_unaffected[\"labels\"]\n",
    "        returned_dict[\"instance_indices\"] = instance_indices\n",
    "        \n",
    "        return returned_dict\n",
    "        \n",
    "    filtered_idx = []\n",
    "    \n",
    "    batch_size = 32  # 50 or so\n",
    "    data_loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, collate_fn=filtering_collate_fn, shuffle=False, generator=torch.Generator(device='cuda')\n",
    "    )  # batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    \n",
    "    for step, batch in enumerate(data_loader):\n",
    "            \n",
    "        batch_pred_ids = model.generate(\n",
    "            input_ids=batch[\"input_ids\"].to(\"cuda\"),\n",
    "            attention_mask=batch[\"attention_mask\"].to(\"cuda\"),\n",
    "            max_new_tokens=20\n",
    "        )\n",
    "                 \n",
    "        for i, (label, pred_ids) in enumerate(tqdm(zip(batch[\"labels\"], batch_pred_ids))):\n",
    "            \n",
    "            output_idx = label != -100\n",
    "            label = label[output_idx]\n",
    "            pred_ids = pred_ids[len(batch[\"input_ids\"][i]):]\n",
    "            print(\"-----------\")\n",
    "            print(\"input_prompt: \", tokenizer.decode(batch[\"input_ids\"][i], skip_special_tokens=True).strip())\n",
    "            print(\"gt_label: \", tokenizer.decode(label, skip_special_tokens=True).strip())\n",
    "            print(\"prediction: \", tokenizer.decode(pred_ids, skip_special_tokens=True).strip())\n",
    "            is_correct = tokenizer.decode(label, skip_special_tokens=True).strip() in tokenizer.decode(pred_ids, skip_special_tokens=True).strip()\n",
    "            \n",
    "            if is_correct:\n",
    "                filtered_idx.append(i + step * batch_size)\n",
    "                \n",
    "        raise\n",
    "    \n",
    "    return dataset.select(filtered_idx)\n",
    "        \n",
    "    \n",
    "filtered_training_set = filtering_ravel_dataset(city_train_set, model, tokenizer)\n",
    "filtered_test_set = filtering_ravel_dataset(city_test_set, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # 50 or so\n",
    "data_loader = DataLoader(\n",
    "    city_train_set, batch_size=batch_size, collate_fn=ravel_collate_fn, shuffle=True, generator=torch.Generator(device='cuda')\n",
    ")  # batch_size, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(\n",
    "    city_test_set, batch_size=batch_size, collate_fn=ravel_collate_fn, shuffle=True, generator=torch.Generator(device='cuda')\n",
    ")  \n",
    "\n",
    "for batch in data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fd745f725f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step': 0, 'train_batch_total_loss': 9.80630111694336, 'train_batch_prediction_loss': 9.80630111694336, 'train_batch_gradient_norm': 0.13300803303718567}\n",
      "{'step': 5, 'train_batch_total_loss': 10.047240257263184, 'train_batch_prediction_loss': 10.047240257263184, 'train_batch_gradient_norm': 0.04190468788146973}\n",
      "{'step': 10, 'train_batch_total_loss': 9.785999298095703, 'train_batch_prediction_loss': 9.785999298095703, 'train_batch_gradient_norm': 0.05512644350528717}\n",
      "{'step': 15, 'train_batch_total_loss': 10.005376815795898, 'train_batch_prediction_loss': 10.005376815795898, 'train_batch_gradient_norm': 0.31540969014167786}\n",
      "{'step': 20, 'train_batch_total_loss': 9.516730308532715, 'train_batch_prediction_loss': 9.516730308532715, 'train_batch_gradient_norm': 0.21790249645709991}\n",
      "{'step': 25, 'train_batch_total_loss': 9.168835639953613, 'train_batch_prediction_loss': 9.168835639953613, 'train_batch_gradient_norm': 0.7928840517997742}\n",
      "{'step': 30, 'train_batch_total_loss': 10.571182250976562, 'train_batch_prediction_loss': 10.571182250976562, 'train_batch_gradient_norm': 0.46857672929763794}\n",
      "{'step': 35, 'train_batch_total_loss': 9.791640281677246, 'train_batch_prediction_loss': 9.791640281677246, 'train_batch_gradient_norm': 0.49616357684135437}\n",
      "{'step': 40, 'train_batch_total_loss': 8.875999450683594, 'train_batch_prediction_loss': 8.875999450683594, 'train_batch_gradient_norm': 0.6185921430587769}\n",
      "{'step': 45, 'train_batch_total_loss': 9.45417594909668, 'train_batch_prediction_loss': 9.45417594909668, 'train_batch_gradient_norm': 1.1520929336547852}\n",
      "{'step': 50, 'train_batch_total_loss': 8.810523986816406, 'train_batch_prediction_loss': 8.810523986816406, 'train_batch_gradient_norm': 0.8834603428840637}\n",
      "{'step': 55, 'train_batch_total_loss': 9.986294746398926, 'train_batch_prediction_loss': 9.986294746398926, 'train_batch_gradient_norm': 0.7781723141670227}\n",
      "{'step': 60, 'train_batch_total_loss': 8.964590072631836, 'train_batch_prediction_loss': 8.964590072631836, 'train_batch_gradient_norm': 0.8756664395332336}\n",
      "{'step': 65, 'train_batch_total_loss': 9.48682975769043, 'train_batch_prediction_loss': 9.48682975769043, 'train_batch_gradient_norm': 0.5710510015487671}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m hypernetwork \u001b[38;5;241m=\u001b[39m RavelEditorHypernetwork(\n\u001b[1;32m      3\u001b[0m     edit_dampening_factor\u001b[38;5;241m=\u001b[39medit_dampening_factor,  \u001b[38;5;66;03m# 1/10000,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     use_layerwise_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     chop_editor_at_layer\u001b[38;5;241m=\u001b[39mchop_layer,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# current problem: 1728 / 30864\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mhypernetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_editing_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_editing_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 20000\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlam_testing_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_per_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_unaffected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[5], line 248\u001b[0m, in \u001b[0;36mRavelEditorHypernetwork.run_train\u001b[0;34m(self, train_loader, test_loader, stop_editing_index, epochs, KL_divergence_loss, lam, lam_testing_penalty, f_data_to_soft_labels, checkpoint_interval, eval_per_steps, use_unaffected)\u001b[0m\n\u001b[1;32m    245\u001b[0m num_datapoints_in_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current_batch_size\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43meditor_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meditor_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_attention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_attention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_unaffected:\n",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m, in \u001b[0;36mRavelEditorHypernetwork.forward\u001b[0;34m(self, editor_input_ids, base_input_ids, base_attention_mask, source_input_ids, source_attention_mask, labels)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     40\u001b[0m     editor_input_ids: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     labels: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m ):\n\u001b[0;32m---> 47\u001b[0m     _pred: EditorModelOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meditor_inner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43meditor_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meditor_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43meditor_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meditor_input_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meditor_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_target_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     59\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: _pred\u001b[38;5;241m.\u001b[39mlogits,\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit_vectors\u001b[39m\u001b[38;5;124m\"\u001b[39m: _pred\u001b[38;5;241m.\u001b[39medit_vectors,\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: _pred\u001b[38;5;241m.\u001b[39mtarget_hidden_states,\n\u001b[1;32m     62\u001b[0m         }\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/hypernetwork-editor/models/gpt2/interp_model.py:356\u001b[0m, in \u001b[0;36mGPT2Interpretor.forward\u001b[0;34m(self, editor_input_ids, editor_attention_mask, base_input_ids, base_attention_mask, source_input_ids, source_attention_mask, base_hidden_states, base_position_ids, source_hidden_states, source_position_ids, intervention_layer, output_target_hidden_states, output_edited_hidden_states, output_intervention_ratio, batch_intervention_ratio)\u001b[0m\n\u001b[1;32m    352\u001b[0m else:\n\u001b[1;32m    353\u001b[0m     hooks = [(self.target_model.transformer.h[intervention_layer - 1], representation_swap)]\n\u001b[1;32m    355\u001b[0m with add_fwd_hooks(hooks):\n\u001b[0;32m--> 356\u001b[0m     # THIS IS THE LINE WHERE THE MODEL IS CALLED (AND THE EDITOR IS CALLED AT\n\u001b[1;32m    357\u001b[0m     # THE END OF `layer` AS A SIDE EFFECT)\n\u001b[1;32m    358\u001b[0m     target_result = self.target_model(\n\u001b[1;32m    359\u001b[0m         input_ids=base_input_ids,\n\u001b[1;32m    360\u001b[0m         attention_mask=base_attention_mask,\n\u001b[1;32m    361\u001b[0m         position_ids=base_position_ids,\n\u001b[1;32m    362\u001b[0m         output_hidden_states=output_edited_hidden_states,\n\u001b[1;32m    363\u001b[0m     )\n\u001b[1;32m    365\u001b[0m logits = target_result.logits\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1327\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1325\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1327\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/frink/sun.jiu/miniconda3/envs/subspace/lib/python3.10/site-packages/torch/fx/traceback.py:62\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         current_meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_fn_seq_nr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m current_meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprev_grad_fn_seq_nr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     59\u001b[0m         current_meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_grad_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m current_meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprev_in_grad_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_stack\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_preserve_node_meta:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stop_editing_index = 8\n",
    "hypernetwork = RavelEditorHypernetwork(\n",
    "    edit_dampening_factor=edit_dampening_factor,  # 1/10000,\n",
    "    use_layerwise_embeddings=False,\n",
    "    num_editing_heads=num_editing_heads,\n",
    "    edit_channel_width=editor_channel_width,\n",
    "    chop_editor_at_layer=chop_layer,\n",
    ")\n",
    "\n",
    "# current problem: 1728 / 30864\n",
    "hypernetwork.run_train(\n",
    "    train_loader=data_loader,\n",
    "    test_loader=test_data_loader,\n",
    "    stop_editing_index=stop_editing_index,\n",
    "    epochs=3,\n",
    "    lam=0,  # 20000\n",
    "    lam_testing_penalty=0,\n",
    "    eval_per_steps = 50,\n",
    "    use_unaffected=False\n",
    ")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_batch in test_data_loader:\n",
    "    break\n",
    "\n",
    "result = hypernetwork.inspect_batch_prediction_ouptuts(test_batch, stop_editing_index=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at some outputs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9940, device='cuda:0'), 0.015099854703294113)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernetwork.eval_accuracy(test_data_loader, stop_editing_index=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokens = []\n",
    "for labels in test_batch[\"labels\"]:\n",
    "    labels = labels[labels != -100]\n",
    "    label_tokens.append(tokenizer.decode(labels, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' soft',\n",
       " ' male',\n",
       " ' library',\n",
       " ' male',\n",
       " ' Literature',\n",
       " ' forest',\n",
       " ' Physics',\n",
       " ' broadcast',\n",
       " ' medicine',\n",
       " ' /dɪˈskɝɪdʒ/',\n",
       " ' Africa/Blantyre',\n",
       " ' studio',\n",
       " ' shapes',\n",
       " ' purple',\n",
       " ' office',\n",
       " ' non-living things',\n",
       " ' male',\n",
       " ' blessed',\n",
       " ' Asia',\n",
       " ' 1953',\n",
       " ' 8',\n",
       " ' /ˈsut/',\n",
       " ' m',\n",
       " ' non-living thing',\n",
       " ' hospital',\n",
       " ' Arabic',\n",
       " ' probation',\n",
       " ' resolved',\n",
       " ' hard',\n",
       " '',\n",
       " ' dispatch power',\n",
       " ' Uzbekistan']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
